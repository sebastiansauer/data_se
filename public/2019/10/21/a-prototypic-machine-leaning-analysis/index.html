<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.67.0" />


<title>A prototypic machine leaning analysis - Reanalyis of &#34;Mindful Machine Learning&#34; - Data Se</title>
<meta property="og:title" content="A prototypic machine leaning analysis - Reanalyis of &#34;Mindful Machine Learning&#34; - Data Se">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo-data-se.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/">Blog</a></li>
    
    <li><a href="/privacy/">Data privacy</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">A prototypic machine leaning analysis - Reanalyis of &#34;Mindful Machine Learning&#34;</h1>

    
    <span class="article-date">2019/10/21</span>
    

    <div class="article-content">
      


<div id="scope" class="section level1">
<h1>Scope</h1>
<p>This is the re-analysis of this paper</p>
<p>Sauer, S., Buettner, R., Heidenreich, T., Lemke, J., Berg, C., &amp; Kurz, C. (2018).
Mindful machine learning: Using machine learning algorithms to predict the practice of mindfulness.
European Journal of Psychological Assessment, 34(1), 6-13.
<a href="http://dx.doi.org/10.1027/1015-5759/a000312" class="uri">http://dx.doi.org/10.1027/1015-5759/a000312</a></p>
<p>A colleague (<a href="http://www.psy.lmu.de/pm/personen/lehrstuhlmitarbeiter/pargent/index.html">Dr. Florian Pargent</a>) informed me about an potential error in my analysis. Having checked it, I confirm that there is an error in the initial analyses.</p>
<p>Specifically, he informed me that the performance measures appear to be based on on the <em>train</em> sample,
where the <em>test</em> sample should have been used.</p>
<p>After having checked the original code, I agree with Dr. Pargent notion. Specifically, in <em>each model</em>, the <em>training</em> data has been presented, but not the <em>testing</em> data as it should have been.</p>
<p>The following code documents the re-analyses that now employs the <em>testing</em> data for gauging model performance.</p>
</div>
<div id="cautions" class="section level1">
<h1>Cautions</h1>
<p>The following aspects could not been controlled:</p>
<ul>
<li>Package versions</li>
<li>R version</li>
</ul>
<p>It seems unlikely though not completely impossible that these factors possibly influence the results. However, we deem the possible influence of those factor negligible.</p>
</div>
<div id="aim-of-this-post" class="section level1">
<h1>Aim of this post</h1>
<p>Correcting the literature is taking place elsewhere; he I would like to show a somewhat typical flow of analysis in machine learning for prediction. I think the code is quite straight forward and should be easy to generalize to similar problems.</p>
</div>
<div id="accessibility" class="section level1">
<h1>Accessibility</h1>
<p>This code, alongside with the data, can be openly accessed from this repository: <a href="https://github.com/sebastiansauer/reanalysis-mindful-machine-learning" class="uri">https://github.com/sebastiansauer/reanalysis-mindful-machine-learning</a></p>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<div id="init" class="section level2">
<h2>Init</h2>
<pre class="r"><code>knitr::opts_chunk$set(
  comment = &quot;#&gt;&quot;,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  results = &quot;hide&quot;,
  out.width = &quot;70%&quot;,
  fig.align = &#39;center&#39;,
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = &quot;hold&quot;,
  size = &quot;tiny&quot;
)</code></pre>
<p>Load libs:</p>
<pre class="r"><code>library(conflicted)
library(caret)
library(here)
library(tictoc)
library(testthat)
library(tidyverse)
library(drlib)</code></pre>
</div>
<div id="define-constants" class="section level2">
<h2>Define constants</h2>
<pre class="r"><code>output_filename &lt;- &quot;model_performance.Rds&quot;</code></pre>
</div>
<div id="load-original-data" class="section level2">
<h2>Load original data</h2>
<pre class="r"><code>dat_url &lt;- &quot;https://raw.github.com/sebastiansauer/reanalysis-mindful-machine-learning/master/original-data/1015-5759_a000312_esm5.csv&quot;

dat &lt;- read_csv(dat_url)</code></pre>
</div>
<div id="prepare-data" class="section level2">
<h2>Prepare data</h2>
<div id="set-dv-to-type-factor" class="section level3">
<h3>Set DV to type <code>factor</code></h3>
<p>As <code>caret</code> will only do classificiton if the outcome is of type factor, we need to convert the outcome variable <code>practice</code> to factor.</p>
<pre class="r"><code>dat2 &lt;- dat %&gt;% 
  mutate(practice = factor(practice),
         practice = case_when(
           practice == &quot;1&quot; ~ &quot;yes&quot;,
           practice == &quot;0&quot; ~ &quot;no&quot;
         ),
         practice = factor(practice)) %&gt;% 
  dplyr::select(practice, everything()) %&gt;% 
  dplyr::select(-X1)

glimpse(dat2)
#&gt; Observations: 276
#&gt; Variables: 15
#&gt; $ practice &lt;fct&gt; no, no, yes, no, no, no, yes, no, yes, no, yes, yes, no…
#&gt; $ fmi1     &lt;dbl&gt; 0.6, 0.8, 0.8, 1.0, 0.6, 0.4, 1.0, 0.4, 1.0, 1.0, 0.6, …
#&gt; $ fmi2     &lt;dbl&gt; 0.8, 0.6, 0.4, 0.0, 0.6, 0.2, 0.8, 0.0, 0.8, 0.6, 0.2, …
#&gt; $ fmi3     &lt;dbl&gt; 0.4, 1.0, 0.6, 0.2, 0.2, 0.4, 0.8, 0.0, 0.8, 1.0, 0.6, …
#&gt; $ fmi4     &lt;dbl&gt; 0.0, 1.0, 1.0, 0.2, 0.2, 0.8, 0.8, 0.6, 0.8, 1.0, 0.6, …
#&gt; $ fmi5     &lt;dbl&gt; 0.2, 0.8, 0.8, 0.2, 1.0, 0.8, 0.6, 0.4, 0.8, 1.0, 0.8, …
#&gt; $ fmi6     &lt;dbl&gt; 0.4, 0.0, 1.0, 0.0, 0.2, 0.8, 0.6, 0.4, 0.6, 1.0, 0.8, …
#&gt; $ fmi7     &lt;dbl&gt; 0.6, 0.6, 0.6, 0.2, 0.8, 0.4, 0.8, 0.2, 1.0, 1.0, 0.8, …
#&gt; $ fmi8     &lt;dbl&gt; 0.8, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.2, 0.8, 1.0, 0.8, …
#&gt; $ fmi9     &lt;dbl&gt; 1.0, 0.6, 1.0, 0.2, 0.4, 0.6, 0.6, 0.2, 0.2, 1.0, 0.8, …
#&gt; $ fmi10    &lt;dbl&gt; 0.0, 0.4, 0.8, 0.2, 0.4, 0.4, 0.8, 0.2, 0.8, 1.0, 0.6, …
#&gt; $ fmi11    &lt;dbl&gt; 0.4, 1.0, 0.4, 0.4, 0.6, 0.2, 0.8, 0.2, 0.6, 1.0, 0.6, …
#&gt; $ fmi12    &lt;dbl&gt; 0.4, 0.4, 1.0, 0.0, 0.4, 0.2, 0.8, 0.0, 0.0, 1.0, 0.8, …
#&gt; $ fmi13    &lt;dbl&gt; 0.8, 0.8, 0.4, 0.8, 1.0, 0.8, 0.4, 0.4, 0.6, 0.0, 0.8, …
#&gt; $ fmi14    &lt;dbl&gt; 0.6, 0.8, 1.0, 0.8, 1.0, 0.6, 1.0, 0.6, 0.8, 1.0, 0.8, …</code></pre>
</div>
<div id="ensure-valide-factor-levels" class="section level3">
<h3>Ensure valide factor levels</h3>
<p>The factor levels should be proper R names for some caret models, accoring to <a href="https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015/discussion/13964">this source</a>, see also <a href="https://stackoverflow.com/questions/18402016/error-when-i-try-to-predict-class-probabilities-in-r-caret">this SO post</a>.</p>
<p>The dependent variable should be of type <code>factor</code> and needs valid factor levels <a href="https://stackoverflow.com/questions/18402016/error-when-i-try-to-predict-class-probabilities-in-r-caret">as stated in this SO post</a>.
Let’s check that.</p>
<pre class="r"><code>levels(dat2$practice) &lt;- make.names(levels(factor(dat2$practice)))
levels(dat2$practice)
#&gt; [1] &quot;no&quot;  &quot;yes&quot;</code></pre>
<p>Check that all predictors are metric, and that the DV is of type factor.</p>
<p>Define vector of predictor names:</p>
<pre class="r"><code>predictor_names &lt;- c(paste0(&quot;fmi&quot;, 1:14))
predictor_names</code></pre>
<p>Test for correct length:</p>
<pre class="r"><code>test_dat2 &lt;- dat2 %&gt;% 
  map( ~class(.))

expect_length(test_dat2, n = (length(predictor_names) + 1))</code></pre>
</div>
</div>
</div>
<div id="training-vs-test-sample" class="section level1">
<h1>Training vs test sample</h1>
<pre class="r"><code>inTraining &lt;- createDataPartition(dat2$practice, p = 0.8, list = FALSE)
training &lt;- dat2[inTraining, ]
testing &lt;- dat2[-inTraining, ]</code></pre>
<div id="define-model-list" class="section level2">
<h2>Define model list</h2>
<pre class="r"><code>model_list &lt;- c(&quot;glm&quot;, &quot;gbm&quot;, &quot;qda&quot;, &quot;svmLinear&quot;, &quot;svmPoly&quot;, &quot;rf&quot;, &quot;nnet&quot;, &quot;ada&quot;, &quot;knn&quot;, &quot;elm&quot;)

n_models &lt;- length(model_list)
n_models</code></pre>
<p>There are 10 being tested.</p>
</div>
<div id="define-standard-cross-validation-scheme" class="section level2">
<h2>Define standard cross-validation scheme</h2>
<pre class="r"><code>fitControl &lt;- trainControl(## 10-fold CV
    method = &quot;repeatedcv&quot;,
    number = 10,
    ## repeated ten times
    repeats = 10)</code></pre>
</div>
<div id="define-data-objects-for-results" class="section level2">
<h2>Define data objects for results</h2>
<pre class="r"><code>conf.all &lt;- data.frame(matrix(nrow = 10, ncol = 4))
names(conf.all) &lt;- c(&quot;model&quot;, &quot;sensitivity&quot;, &quot;specificity&quot;, &quot;kappa&quot;)


model_output &lt;- list()

model_confmatrix &lt;- list()</code></pre>
</div>
</div>
<div id="funs" class="section level1">
<h1>funs</h1>
<p>This function stores the model output in some lists:</p>
<pre class="r"><code># delete-me
save_model_output &lt;- function(name, modelOut) {
  
  model_confmatrix[[name]] &lt;&lt;- confusionMatrix(predict(modelOut, testing), 
                                             testing$practice)
  
  conf.all[name, ] &lt;&lt;- c(name, model_confmatrix[[name]][c(1,2)], model_confmatrix[[name]]$overall[2])
  
}</code></pre>
<pre class="r"><code>run_model &lt;- function(model_name, ...){
  
  set.seed(42)
  
  start_time = Sys.time()
  
  modelOut &lt;- train(practice ~ ., 
                    data = training, 
                    method = model_name, 
                    trControl = fitControl,
                    ...)
  
  modelPred &lt;- predict(modelOut, testing)
  
  modelConfMatrix &lt;- confusionMatrix(modelPred, testing$practice)
  
  end_time = Sys.time()
  
  coefs &lt;- list(name = model_name, 
                sensitivity = modelConfMatrix$byClass[1], 
                specificity = modelConfMatrix$byClass[2], 
                accuracy = modelConfMatrix$overall[1],
                kappa = modelConfMatrix$overall[2],
                time_taken = end_time - start_time)
}</code></pre>
</div>
<div id="now-run-the-models" class="section level1">
<h1>Now run the models</h1>
<div id="check" class="section level2">
<h2>Check</h2>
<pre class="r"><code>modelOut &lt;- train(practice ~ ., 
                  data = training, 
                  method = &quot;glm&quot;, 
                  trControl = fitControl,
                  family = &quot;binomial&quot;)

modelPred &lt;- predict(modelOut, testing)
  
modelConfMatrix &lt;- confusionMatrix(modelPred, testing$practice)
modelConfMatrix
#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction no yes
#&gt;        no  29   7
#&gt;        yes  4  15
#&gt;                                           
#&gt;                Accuracy : 0.8             
#&gt;                  95% CI : (0.6703, 0.8957)
#&gt;     No Information Rate : 0.6             
#&gt;     P-Value [Acc &gt; NIR] : 0.001331        
#&gt;                                           
#&gt;                   Kappa : 0.5736          
#&gt;                                           
#&gt;  Mcnemar&#39;s Test P-Value : 0.546494        
#&gt;                                           
#&gt;             Sensitivity : 0.8788          
#&gt;             Specificity : 0.6818          
#&gt;          Pos Pred Value : 0.8056          
#&gt;          Neg Pred Value : 0.7895          
#&gt;              Prevalence : 0.6000          
#&gt;          Detection Rate : 0.5273          
#&gt;    Detection Prevalence : 0.6545          
#&gt;       Balanced Accuracy : 0.7803          
#&gt;                                           
#&gt;        &#39;Positive&#39; Class : no              
#&gt; 


modelConfMatrix$byClass[c(1,2)]
#&gt; Sensitivity Specificity 
#&gt;   0.8787879   0.6818182

modelConfMatrix$overall[c(1,2)]
#&gt;  Accuracy     Kappa 
#&gt; 0.8000000 0.5736434</code></pre>
</div>
<div id="check---glm" class="section level2">
<h2>Check - GLM</h2>
<pre class="r"><code>model_output[[&quot;glm&quot;]] &lt;- run_model(&quot;glm&quot;, family = &quot;binomial&quot;)

model_output[[&quot;glm&quot;]]</code></pre>
</div>
<div id="loop-over-models" class="section level2">
<h2>Loop over models</h2>
<pre class="r"><code>if (!file.exists(paste0(here(),&quot;/MML/reanalysis-objects/&quot;, output_filename))) {
  time &lt;- Sys.time()
  model_performance &lt;- model_list %&gt;% 
    map_dfr(run_model)
  
  Sys.time() - time
} else {cat(&quot;Output file already exists. Skipping re-computation. Loading output file.\n&quot;)
  model_performance &lt;- read_rds(paste0(here(),&quot;/reanalysis-objects/&quot;, output_filename) )}</code></pre>
</div>
<div id="check-results" class="section level2">
<h2>Check results</h2>
<pre class="r"><code>model_performance</code></pre>
</div>
<div id="own-music-for-boosting" class="section level2">
<h2>Own music for Boosting</h2>
<p>Define different grid to check hyper parameter:</p>
<pre class="r"><code>gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = 1:30*50,
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

performance_gbm &lt;- run_model(&quot;gbm&quot;, tuneGrid = gbmGrid)


performance_gbm_df &lt;- flatten(performance_gbm) %&gt;% 
  as_tibble()</code></pre>
<p>Now add the gbm results to the main performance data frame, that is, replace the old values by the new one:</p>
<pre class="r"><code>model_performance2 &lt;- model_performance


model_performance2[performance_gbm_df$name == model_performance2$name, ] &lt;- performance_gbm_df


model_performance2</code></pre>
</div>
</div>
<div id="visualize-model-performance" class="section level1">
<h1>Visualize model performance</h1>
<div id="reformat-data" class="section level2">
<h2>Reformat data</h2>
<p>Now spread it into a long format.</p>
<pre class="r"><code>model_performance_long &lt;- model_performance2 %&gt;% 
  pivot_longer(cols = sensitivity:kappa, names_to = &quot;coefficient&quot;, values_to = &quot;value&quot;)
model_performance_long</code></pre>
</div>
<div id="check-1" class="section level2">
<h2>Check</h2>
<pre class="r"><code>model_performance_long %&gt;% 
  count(name)</code></pre>
<p>Looks good, 4 coefficients per model (name).</p>
<p>Similarly,</p>
<pre class="r"><code>model_performance_long %&gt;% 
  count(coefficient)</code></pre>
</div>
<div id="figure-2-original-paper" class="section level2">
<h2>Figure 2 (original paper)</h2>
<p>Now plot:</p>
<pre class="r"><code>model_performance_long %&gt;%
  #dplyr::filter(coefficient != &quot;accuracy&quot;) %&gt;% 
  ggplot(aes(x = reorder_within(name, value, coefficient), y = value, fill = coefficient)) +
  geom_col(position = &quot;dodge&quot;) +
  coord_flip() +
  scale_fill_viridis_d() +
  scale_x_reordered() +
  theme_light() +
  labs(x = &quot;model&quot;, y = &quot;value&quot;) +
  facet_wrap(~ coefficient, scales = &quot;free&quot;)</code></pre>
<p><img src="/post/2019-10-21-a-prototypic-machine-leaning-analysis_files/figure-html/figure2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the original mode, <code>rf</code> scores best. In this re-analysis, <code>rf</code> scored third-best (measured by Cohen’s Kappa in both analyses).</p>
</div>
<div id="figure-3" class="section level2">
<h2>Figure 3</h2>
<p>Figure 3 in the original paper computes the average per coefficient across all models, and compares “classical” linear model results with the “machine learning” models.</p>
<p>First compute the summary stats.</p>
<pre class="r"><code>model_output_df_sum &lt;- model_performance_long %&gt;% 
  mutate(type = ifelse(name == &quot;glm&quot;, &quot;classical&quot;, &quot;machine learning&quot;)) %&gt;% 
  group_by(type, coefficient) %&gt;% 
  summarise(cofficient_mean = mean(value))

model_output_df_sum</code></pre>
<p>Now plot.</p>
<pre class="r"><code>model_output_df_sum %&gt;% 
  ggplot(aes(x = reorder(coefficient, -cofficient_mean), y = cofficient_mean)) +
  geom_col(position = &quot;dodge&quot;, aes(fill = coefficient)) +
  facet_wrap(~ type) +
  scale_fill_viridis_d() +
  theme_light() +
  labs(y = &quot;value&quot;, x = &quot;coefficient&quot;) +
  geom_label(aes(label = round(cofficient_mean, 2)))</code></pre>
<p><img src="/post/2019-10-21-a-prototypic-machine-leaning-analysis_files/figure-html/figure3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>There is no strong difference between classical and machine learning models, seen from an summary perspective. However, a slight advantage may be inferred in favor of the ML models. In particular, the best scoring model came from the ML fraction.</p>
<p>As concluded in the original paper, algorithms do differ in their performance given a particular data set. Analysts should be aware of the variation in the results induced by picking one ore more certain models.</p>
</div>
</div>
<div id="computation-time" class="section level1">
<h1>Computation time</h1>
<pre class="r"><code>model_performance2 %&gt;%
  mutate(time = as.numeric(time_taken)) %&gt;% 
  ggplot(aes(x = reorder(name, time), y = time)) +
  geom_point(size = 4) +
  coord_flip() +
  theme_light() +
  scale_y_log10() +
  labs(y = &quot;time in log10 [sec]&quot;, x = &quot;model&quot;)</code></pre>
<p><img src="/post/2019-10-21-a-prototypic-machine-leaning-analysis_files/figure-html/unnamed-chunk-11-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="write-output-to-disk" class="section level1">
<h1>Write output to disk</h1>
<pre class="r"><code>#model_output &lt;- read_rds(&quot;reanalysis-objects/model_output.rds&quot;)
#write_rds(model_performance, paste0(here(),&quot;/MML/reanalysis-objects/&quot;, output_filename))
</code></pre>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-data-se-netlify-com.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

