<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Intuition on Data Se</title>
    <link>/tags/intuition/</link>
    <description>Recent content in Intuition on Data Se</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 24 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/intuition/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Why &#34;n-1&#34; in empirical variance? A simulation.</title>
      <link>/2018/03/24/why-n-1-in-empirical-variance-a-simulation/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/24/why-n-1-in-empirical-variance-a-simulation/</guid>
      <description>It is well-known that the empirical variance underestimates the population variance. Specifically, the empirical variance is defined as: \(var_{emp} = \frac{\sum_i x_i - \bar{x}}{n-1}\). But why \(n-1\), why not just \(n\), as intuition (of some) dictates? Put shortly, as the variance of a sample tends to underestimates the population variance we have to inflate it artificially, to enlarge it, that’s why we do put a smaller number (the “n-1”) in the denominator, resulting in a larger value of the whole fraction.</description>
    </item>
    
    <item>
      <title>Effect sizes for the Mann-Whitney U Test: an intuition</title>
      <link>/2017/07/04/effsize_utest/</link>
      <pubDate>Tue, 04 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/04/effsize_utest/</guid>
      <description>The Mann-Whitney U-Test is a test with a wide applicability, wider than the t-Test. Why that? Because the U-Test is applicable for ordinal data, and it can be argued that confining the metric level of a psychological variable to ordinal niveau is a reasonable bet. Second, it is robust, more robust than the t-test, because it only considers ranks, not raw values. In addition, some say that the efficiency of the U-Test is very close to the t-Test (.</description>
    </item>
    
    <item>
      <title>mean and sd of z-values</title>
      <link>/2017/05/26/z-values/</link>
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/26/z-values/</guid>
      <description>Edit: This post was updated, including two errors fixed - thanks to (private) comments from Norman Markgraf.
z-values, aka values coming from an z-transformation are a frequent creature in statistics land. Among their properties are the following:
 mean is zero variance is one (and hence sd is one)  But why is that? How come that this two properties are true? The goal of this post is to shed light on these two properties of z-values.</description>
    </item>
    
    <item>
      <title>Squares maximize area - a visualization</title>
      <link>/2017/05/19/maximize_area/</link>
      <pubDate>Fri, 19 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/19/maximize_area/</guid>
      <description>An old story is that one of the farmer with a fence of some given length, say 20m. Now this farmer wants to put up his fence so that he claims the largest piece of land possible. What width (w) and height (h) should we pick?
Instead of a formal proof, let&amp;rsquo;s start with a visualization.
First, we need some packages.
library(tidyverse) library(gganimate) library(RColorBrewer) library(scales) library(knitr)  Now, let&amp;rsquo;s make up serveral ways to split up a rectengular piece of land.</description>
    </item>
    
    <item>
      <title>A predictor&#39;s unique contribution - (visual) demonstration</title>
      <link>/2017/05/17/storks/</link>
      <pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/17/storks/</guid>
      <description>A well-known property of regression models is that they capture the unique contribution of a predictor. By &amp;ldquo;unique&amp;rdquo; we mean the effect of the predictor (on the criterion) if the other predictor(s) is/are held constant. A typical classroom example goes along the following lines.
All about storks  There&amp;rsquo;s a correlation between babies and storks. Counties with lots of storks enjoy large number of babies and v.v.
 However, I have children, I know the storks are not overly involved in that business, so says the teacher (polite laughters in the audience).</description>
    </item>
    
    <item>
      <title>Variance explained vs. variance blurred</title>
      <link>/2017/05/05/explained_variance/</link>
      <pubDate>Fri, 05 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/05/explained_variance/</guid>
      <description>Frequently, someones says that some indicator variable X &amp;ldquo;explains&amp;rdquo; some proportion of some target variable, Y. What does this actually mean? By &amp;ldquo;mean&amp;rdquo; I am trying to find some intuition that &amp;ldquo;clicks&amp;rdquo; rather than citing the (well-known) formualas.
To start with, let&amp;rsquo;s load some packages and make up some random data.
library(tidyverse)  n_rows &amp;lt;- 100 set.seed(271828) df &amp;lt;- data_frame( exp_clean = rnorm(n = n_rows, mean = 2, sd = 1), cntrl_clean = rnorm(n = n_rows, mean = 0, sd = 1), exp_noisy = exp_clean + rnorm(n = n_rows, mean = 0, sd = 3), cntrl_noisy = cntrl_clean + rnorm(n = n_rows, mean = 0, sd = 3), ID = 1:n_rows)  Here, we drew 100 cases from the population of the &amp;ldquo;experimental group&amp;rdquo; (mue = 2) and 100 cases from the control group (mue = 0).</description>
    </item>
    
    <item>
      <title>Covariance as correlation</title>
      <link>/2017/04/25/cor_as_cov/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/25/cor_as_cov/</guid>
      <description>Correlation is one of the most widely used and a well-known measure of the assocation (linear association, that is) of two variables.
Perhaps less well-known is that the correlation is in principle analoguous to the covariation.
To see this, consider the a formula of the covariance of two empirical datasets, $X$ and $Y$:
$$COV(X,Y) = \frac{1}{n} \cdot \big( \sum (X_i -\bar{X}) \cdot (Y_i - \bar{Y}) \big) $$
In other words, the covariance of $X$ and $Y$ $COV(X,Y)$ is the average of difference of some value to its mean.</description>
    </item>
    
    <item>
      <title>Some reflections on stochastic independence</title>
      <link>/2016/11/08/stochastic_independence/</link>
      <pubDate>Tue, 08 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/08/stochastic_independence/</guid>
      <description>We are often interested in the question whether two variables are &amp;ldquo;associated&amp;rdquo;, &amp;ldquo;correlated&amp;rdquo; (I mean the normal English term) or &amp;ldquo;dependent&amp;rdquo;. What exactly, or rather in normal words, does that mean? Let&amp;rsquo;s look at some easy case.
NOTE: The example has been updated to reflect a more tangible and sensible scenario (find the old one in the previous commit at Github).
Titanic data For example, let&amp;rsquo;s look at survival rates of the Titanic disaster, to see whether the probability of survival (event A) depends on the whether you embarked for 1st class (event B).</description>
    </item>
    
    <item>
      <title>Why is SD(X) unequal to MAD(X)?</title>
      <link>/2016/08/31/why-sd-is-unequal-to-mad/</link>
      <pubDate>Wed, 31 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/31/why-sd-is-unequal-to-mad/</guid>
      <description>MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]} });   It may seem bewildering that the standard deviation (sd) of a vector X is (generally) unequal to the mean absolute deviation from the mean (MAD) of X, ie.
$$sd(X) \ne MAD(X)$$.
One could now argue this way: well, sd(X) involves computing the mean of the squared $$x_i$$, then taking the square root of this mean, thereby &amp;ldquo;coming back&amp;rdquo; to the initial size or dimension of x (i.</description>
    </item>
    
    <item>
      <title>Why absolute correlation value (r) cannot exceed 1. An intuition.</title>
      <link>/2016/08/28/why-abs-correlation-is-max-1/</link>
      <pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/28/why-abs-correlation-is-max-1/</guid>
      <description>Pearson&amp;rsquo;s correlation is a well-known and widely used instrument to gauge the degree of linear association of two variables (see this post for an intuition on correlation).
There a many formulas for correlation, but a short and easy one is this one:
$$r = \varnothing(z_x z_y)$$.
In words, $$r$$ can be seen as the average product of z-scores.
In &amp;ldquo;raw values&amp;rdquo;, r is given by
$$ r = \frac{\frac{1}{n}\sum{\Delta X \Delta Y}}{\sqrt{\frac{1}{n}\sum{\Delta X^2}} \sqrt{\frac{1}{n}\sum{\Delta Y^2}}} $$.</description>
    </item>
    
    <item>
      <title>Intuition on correlation</title>
      <link>/2016/07/25/correlation-intuition/</link>
      <pubDate>Mon, 25 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/25/correlation-intuition/</guid>
      <description>reading time: 10 min.
Pearson’s correlation (short: correlation) is one of statistics’ all time classics. With an age of about a century, it is some kind of grand dad of analytic tools – but an oldie who is still very busy!
Formula, interpretation and application of correlation is well known.
In some non-technical lay terms, correlation captures the (linear) degree of co-variation of two linear variables. For example: if tall people have large feet (and small people small feet), on average, we say that height and foot size are correlated.</description>
    </item>
    
    <item>
      <title>Intuition on Cohen&#39;s d</title>
      <link>/2016/07/15/cohens-d-intuition/</link>
      <pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/15/cohens-d-intuition/</guid>
      <description>reading time: 5-10 min.
Cohen&amp;rsquo;s d is a widely known and extensively used measure of effect size. That is, d is used to gauge how strong an effect is (given the fact that the effect exists). For example, one way to estimate d is as follows:
data(tips, package = &amp;quot;reshape2&amp;quot;) library(compute.es) t1 &amp;lt;- t.test(tip ~ sex, data = tips) t1$statistic  ## t ## -1.489536  table(tips$sex)  ## ## Female Male ## 87 157  tes(t1$statistic, 87, 157)  ## Mean Differences ES: ## ## d [ 95 %CI] = -0.</description>
    </item>
    
    <item>
      <title>Why have z-transformed values a mean of zero and a sd of 1?</title>
      <link>/2016/07/02/z-value-intuition/</link>
      <pubDate>Sat, 02 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/02/z-value-intuition/</guid>
      <description>z-transformation is an ubiquitous operation in data analysis. It is often quite practical.
Example: Assume Dr Zack scored 42 points on a test (say, IQ). Average score is 40 in the relevant population, and SD is 1, let’s say. So Zack’s score is 2 points above average. 2 points equals to SDs in this example. We can thus safely infer that Zack is about 2 SDs above average (leaving measurement precision and other issues at side).</description>
    </item>
    
  </channel>
</rss>