<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stats on Data Se</title>
    <link>/tags/stats/</link>
    <description>Recent content in stats on Data Se</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/stats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Correspondance of residual and outcome variable distribution in regression</title>
      <link>/2019/10/11/correspondance-of-residual-and-outcome-variable-distribution-in-regression/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/11/correspondance-of-residual-and-outcome-variable-distribution-in-regression/</guid>
      <description>Load packages library(tidyverse) library(mosaic) library(broom)  Motivation A well known assumption of the linear model is that the error terms (residuals) should be normally distributed. Noteworthy, statisticians such as Gelman do not consider this assumption te be central (in their 2007 book, he and Hill say that they do not even test for it). However, for p-values this distribution plays a role. In this post, we will not discuss the relevance of this assumption but rather explore a related but different question: Is there a correspondance between the distribution of the error terms and the outcome variable?</description>
    </item>
    
    <item>
      <title>P-values are uniformly distributed under the H0, a simulation</title>
      <link>/2019/10/11/p-values-are-equally-distributed-under-the-h0/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/11/p-values-are-equally-distributed-under-the-h0/</guid>
      <description>Load packages library(tidyverse) library(mosaic)  Motivation The p-value is a ubiquituous tool for gauging the plausibility of a Null hypothesis. More specifically, the p-values indicates the probability of obtaining a test statistic at least as extreme as in the present data if the Null hypothesis was true and the experiment would be repeated an infinite number of times (under the same conditions except the data generating process).
The distribution of the p-values depends on the strength of some effect (among other things).</description>
    </item>
    
    <item>
      <title>Some algebraic properties of z-scores</title>
      <link>/2019/10/07/some-algebraic-properties-of-z-scores/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/07/some-algebraic-properties-of-z-scores/</guid>
      <description>Load packages library(tidyverse)  Motivation Z-scores (z-values) are a useful and widely employed tool to gauge and compare measurements. For instance, z-scores help to compare the relative position of some measurements with respect to their distributions. In this post, we will prove some basic (algebraic) properties of z-values. There’s nothing new to that, it’s just I’d like to have it neat and concise somewhere to quickly find it. I’ll add some explanation for the ease of reception.</description>
    </item>
    
    <item>
      <title>Different ways to present summaries in ggplot2</title>
      <link>/2017/09/08/ggplot-summaries/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/08/ggplot-summaries/</guid>
      <description>A convenient and well applicable visualization for comparing groups with respect to a metric variable is the boxplot. However, often, comparing means is accompanied by t-tests, ANOVAs, and friends. Such tests test the mean, not the median, and hence the boxplot is presenting the tested statistic. It would be better to align test and diagram. How can that be achieved using ggplot2? This posts demonstrates some possibilities.
First, let&#39;s plot a boxplot.</description>
    </item>
    
    <item>
      <title>Effect sizes for the Mann-Whitney U Test: an intuition</title>
      <link>/2017/07/04/effsize_utest/</link>
      <pubDate>Tue, 04 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/04/effsize_utest/</guid>
      <description>The Mann-Whitney U-Test is a test with a wide applicability, wider than the t-Test. Why that? Because the U-Test is applicable for ordinal data, and it can be argued that confining the metric level of a psychological variable to ordinal niveau is a reasonable bet. Second, it is robust, more robust than the t-test, because it only considers ranks, not raw values. In addition, some say that the efficiency of the U-Test is very close to the t-Test (.</description>
    </item>
    
    <item>
      <title>mean and sd of z-values</title>
      <link>/2017/05/26/z-values/</link>
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/26/z-values/</guid>
      <description>Edit: This post was updated, including two errors fixed - thanks to (private) comments from Norman Markgraf.
z-values, aka values coming from an z-transformation are a frequent creature in statistics land. Among their properties are the following:
 mean is zero variance is one (and hence sd is one)  But why is that? How come that this two properties are true? The goal of this post is to shed light on these two properties of z-values.</description>
    </item>
    
    <item>
      <title>Simple way of plotting normal/logistic/etc. curve</title>
      <link>/2017/05/24/plotting_s-curve/</link>
      <pubDate>Wed, 24 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/24/plotting_s-curve/</guid>
      <description>Plotting a function is often helpful to better understand what&#39;s going on. Plotting curves in R base is simple by virtue of function curve. But how to draw curves using ggplot2?
That&#39;s a little bit more complicated by can still be accomplished by 1-2 lines.
library(ggplot2) Normal curve p &amp;lt;- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) p + stat_function(fun = dnorm, n = 101) stat_function is some kind of parallel function to curve.</description>
    </item>
    
    <item>
      <title>Deriving the logits for logistic regression</title>
      <link>/2017/05/06/deriving-the-logits-for-logistic-regression/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/06/deriving-the-logits-for-logistic-regression/</guid>
      <description>The logistic regression is an incredible useful tool, partly because binary outcomes are so frequent in live (“she loves me - she doesn’t love me”). In parts because we can make use of well-known “normal” regression instruments.
But the formula of logistic regression appears opaque to many (beginners or those with not so much math background).
Let’s try to shed some light on the formula by discussing some accessible explanation on how to derive the formula.</description>
    </item>
    
    <item>
      <title>Variance explained vs. variance blurred</title>
      <link>/2017/05/05/explained_variance/</link>
      <pubDate>Fri, 05 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/05/explained_variance/</guid>
      <description>Frequently, someones says that some indicator variable X &amp;ldquo;explains&amp;rdquo; some proportion of some target variable, Y. What does this actually mean? By &amp;ldquo;mean&amp;rdquo; I am trying to find some intuition that &amp;ldquo;clicks&amp;rdquo; rather than citing the (well-known) formualas.
To start with, let&#39;s load some packages and make up some random data.
library(tidyverse) n_rows &amp;lt;- 100 set.seed(271828) df &amp;lt;- data_frame( exp_clean = rnorm(n = n_rows, mean = 2, sd = 1), cntrl_clean = rnorm(n = n_rows, mean = 0, sd = 1), exp_noisy = exp_clean + rnorm(n = n_rows, mean = 0, sd = 3), cntrl_noisy = cntrl_clean + rnorm(n = n_rows, mean = 0, sd = 3), ID = 1:n_rows) Here, we drew 100 cases from the population of the &amp;ldquo;experimental group&amp;rdquo; (mue = 2) and 100 cases from the control group (mue = 0).</description>
    </item>
    
    <item>
      <title>Einführung in die Datenanalyse mit R-Paket &#39;dplyr&#39; - R User Group Nürnberg</title>
      <link>/2017/04/27/datenanalyse_mit_dplyr/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/27/datenanalyse_mit_dplyr/</guid>
      <description>Datenjudo mit dplyr Einleitung Innerhalb der R-Landschaft hat sich das Paket dplyr binnen kurzer Zeit zu einem der verbreitesten Pakete entwickelt; es stellt ein innovatives Konzept der Datenanalyse zur Verfügung. dplyr zeichnet sich durch zwei Ideen aus. Die erste Idee ist, dass nur Tabellen (&amp;ldquo;dataframes&amp;rdquo; oder &amp;ldquo;tibbles&amp;rdquo;) verarbeitet werden, keine anderen Datenstrukturen. Diese Tabellen werden von Funktion zu Funktion durchgereicht. Der Fokus auf Tabellen vereinfacht die Analyse, da Spalten nicht einzeln oder mittels Schleifen werden müssen.</description>
    </item>
    
    <item>
      <title>Covariance as correlation</title>
      <link>/2017/04/25/cor_as_cov/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/25/cor_as_cov/</guid>
      <description>Correlation is one of the most widely used and a well-known measure of the assocation (linear association, that is) of two variables.
Perhaps less well-known is that the correlation is in principle analoguous to the covariation.
To see this, consider the a formula of the covariance of two empirical datasets, $X$ and $Y$:
$$COV(X,Y) = \frac{1}{n} \cdot \big( \sum (X_i -\bar{X}) \cdot (Y_i - \bar{Y}) \big) $$
In other words, the covariance of $X$ and $Y$ $COV(X,Y)$ is the average of difference of some value to its mean.</description>
    </item>
    
    <item>
      <title>Plotting skewed distributions</title>
      <link>/2017/04/19/skewed-distribs/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/19/skewed-distribs/</guid>
      <description>Let&#39;s plot some skewed stuff, aehm, distributions!
Actually, the point I - initially - wanted to make is that in skewed distribution, don&#39;t use means. Or at least, be very aware that (arithmetic) means can be grossly misleading. But for today, let&#39;s focus on drawing skewed distributions.
Some packages:
library(tidyverse) library(fGarch) # for snorm Some skewed distribution include:
 &amp;ldquo;polluted&amp;rdquo; normal distributions, ie., mixtures of two normals Exponential distributions Gamma distributions Beta distributions  One way to visualize them is to draw their curve, ie.</description>
    </item>
    
    <item>
      <title>The effect of sample on p-values. A simulation.</title>
      <link>/2017/04/13/pvalue_sample_size/</link>
      <pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/13/pvalue_sample_size/</guid>
      <description>It is well-known that the notorious p-values is sensitive to sample size: The larger the sample, the more bound the p-value is to fall below the magic number of .05.
Of course, the p-value is also a function of the effect size, eg., the distance between two means and the respective variances. But still, the p-values tends to become significant in the face of larges samples, and non-significant otherwise.
Theoretically, quite simple and well understood.</description>
    </item>
    
    <item>
      <title>Checklist for Data Cleansing</title>
      <link>/2017/02/13/data_cleansing/</link>
      <pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/13/data_cleansing/</guid>
      <description>What this post is about: Data cleansing in practice with R Data analysis, in practice, consists typically of some different steps which can be subsumed as &amp;ldquo;preparing data&amp;rdquo; and &amp;ldquo;model data&amp;rdquo; (not considering communication here):
(Inspired by this)
Often, the first major part &amp;ndash; &amp;ldquo;prepare&amp;rdquo; &amp;ndash; is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!). In practice, one rather has to get his (her) hands dirt&amp;hellip;</description>
    </item>
    
    <item>
      <title>Dataset &#39;performance in stats test&#39;</title>
      <link>/2017/01/27/data_test_inference/</link>
      <pubDate>Fri, 27 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/27/data_test_inference/</guid>
      <description>This posts shows data cleaning and preparation for a data set on a statistics test (NHST inference). Data is published under a CC-licence, see here.
Data was collected 2015 to 2017 in statistics courses at the FOM university in different places in Germany. Several colleagues helped to collect the data. Thanks a lot! Now let&#39;s enjoy the outcome (and make it freely available to all).
Raw N is 743. The test consists of 40 items which are framed as propositions; students are asked to respond with either &amp;ldquo;true&amp;rdquo; or &amp;ldquo;false&amp;rdquo; to each item.</description>
    </item>
    
    <item>
      <title>Convert logit to probability</title>
      <link>/2017/01/24/convert_logit2prob/</link>
      <pubDate>Tue, 24 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/24/convert_logit2prob/</guid>
      <description>Logistic regression may give a headache initially. While the structure and idea is the same as &amp;ldquo;normal&amp;rdquo; regression, the interpretation of the b&#39;s (ie., the regression coefficients) can be more challenging.
This post provides a convenience function for converting the output of the glm function to a probability. Or more generally, to convert logits (that&#39;s what spit out by glm) to a probabilty.
Note1: The objective of this post is to explain the mechanics of logits.</description>
    </item>
    
    <item>
      <title>Gentle intro to &#39;R-squared equals squared r&#39;</title>
      <link>/2017/01/20/rsquared/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/20/rsquared/</guid>
      <description>It comes as no surprise that $$R^2$$ (&amp;ldquo;coefficient of determination&amp;rdquo;) equals $$r^2$$ in simple regression (predictor X, criterion Y), where $$r(X,Y)$$ is Pearson&#39;s correlation coefficient. $$R^2$$ equals the fraction of explained variance in a simple regression. However, the statistical (mathematical) background is often less clear or buried in less-intuitive formula.
The goal of this post is to offer a gentle explanantion why
$$R^2 = r^2$$,
where $$r$$ is $$r(Y,\hat{Y})$$ and $$\hat{Y}$$ are the predicted values.</description>
    </item>
    
    <item>
      <title>Why is the variance additive? An intuition.</title>
      <link>/2017/01/04/additivity_variance/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/04/additivity_variance/</guid>
      <description>The variance of some data can be defined in rough terms as the mean of the squared deviations from the mean.
Let&#39;s repeat that because it is important:
 Variance: Mean of squared deviations from the mean.
 An example helps to illustrate. Assume some class of students are forced to write an exam in a statistics class (OMG). Let&#39;s say the grades range fom 1 to 6, 1 being the best and 6 the worst.</description>
    </item>
    
    <item>
      <title>Müncher Mietpreis: Übung zum p-Wert</title>
      <link>/2016/12/21/mietpreis_p-wert/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/21/mietpreis_p-wert/</guid>
      <description>Sie möchten die Hypothese (H0) testen, dass der mittlere Mietpreis in München 16,28€ beträgt (wie der Münchner Merkur einmal behauptet hat). Dafür ziehen Sie eine Stichprobe der Größe n = 36. Gehen Sie von einer SD von 3€ in der Population aus (Menge aller Mietwohnungen in München). Alpha sei 5%. Der Mittelwert Ihrer Stichprobe ist 16,79€. Nehmen Sie als H1 die Hypothese, dass der wahre mittlere Mietpreis höher ist.
Gesucht  Was ist der z-Wert des Stichprobenergebnisses?</description>
    </item>
    
    <item>
      <title>Some thoughts on &#39;Dear stats curriculum developers&#39;</title>
      <link>/2016/12/08/stats_curriculum/</link>
      <pubDate>Thu, 08 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/08/stats_curriculum/</guid>
      <description>Recently, Andrew Gelman (@StatModeling at Twitter) published a post with this title - &amp;ldquo;“Dear Major Textbook Publisher”: A Rant&amp;rdquo;.
In essence, he discussed how a good stats intro text book should be like. And complained about the low quality of some many textbooks out there.
As I am also in the business guilty of coming up with stats curriculum for my students (applied courses for business type students mostly), I discuss some thoughts for &amp;ldquo;stats curriculum developers&amp;rdquo; (like myself).</description>
    </item>
    
    <item>
      <title>Simulation of p-values</title>
      <link>/2016/12/01/simu_p/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/01/simu_p/</guid>
      <description>Teaching or learning stats can be a challenging endeavor. In my experience, starting with concrete (as opposed to abstract) examples helps many a learner. What also helps (for me) is visualizing.
As p-values are still part and parcel of probably any given stats curriculum, here is a convenient function to simulate p-values and to plot them.
&amp;ldquo;Simulating p-values&amp;rdquo; amounts to drawing many samples from a given, specified population (eg., µ=100, s=15, normally distributed).</description>
    </item>
    
    <item>
      <title>Crashkurs zur Erstellung von Barplots für Umfrage-Daten</title>
      <link>/2016/11/13/crashkurs_barplots/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/crashkurs_barplots/</guid>
      <description>Eine recht häufige Art von Daten in der Wirtschaft kommen von Umfragen in der Belegschaft. Diese Daten gilt es dann aufzubereiten und graphisch wiederzugeben. Dafür gibt dieser Post einige grundlegende Hinweise. Grundwissen mit R setzen wir voraus :-)
Eine ausführlichere Beschreibung hier sich z.B. hier.
Packages laden Nicht vergessen: Ein Computerprogramm (z.B. ein R-Package) kann man nur dann laden, wenn man es vorher installier hat (aber es reicht, das Programm/R-Package einmal zu installieren).</description>
    </item>
    
    <item>
      <title>Some thoughts (and simulation) on overfitting</title>
      <link>/2016/11/13/overfitting_simulation/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/overfitting_simulation/</guid>
      <description>Overfitting is a common problem in data analysis. Some go as far as saying that &amp;ldquo;most of&amp;rdquo; published research is false (John Ionnadis); overfitting being one, maybe central, problem of it. In this post, we explore some aspects on the notion of overfitting.
Assume we have 10 metric variables v (personality/health/behavior/gene indicator variables), and, say, 10 variables for splitting up subgroups (aged vs. young, female vs. male, etc.), so 10 dichotomic variables.</description>
    </item>
    
    <item>
      <title>Some reflections on stochastic independence</title>
      <link>/2016/11/08/stochastic_independence/</link>
      <pubDate>Tue, 08 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/08/stochastic_independence/</guid>
      <description>We are often interested in the question whether two variables are &amp;ldquo;associated&amp;rdquo;, &amp;ldquo;correlated&amp;rdquo; (I mean the normal English term) or &amp;ldquo;dependent&amp;rdquo;. What exactly, or rather in normal words, does that mean? Let&#39;s look at some easy case.
NOTE: The example has been updated to reflect a more tangible and sensible scenario (find the old one in the previous commit at Github).
Titanic data For example, let&#39;s look at survival rates of the Titanic disaster, to see whether the probability of survival (event A) depends on the whether you embarked for 1st class (event B).</description>
    </item>
    
    <item>
      <title>CLES plot</title>
      <link>/2016/10/17/cles-plot/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/17/cles-plot/</guid>
      <description>In data analysis, we often ask &amp;ldquo;Do these two groups differ in the outcome variable&amp;rdquo;? Asking this question, a tacit assumption may be that the grouping variable is the cause of the difference in the outcome variable. For example, assume the two groups are &amp;ldquo;treatment group&amp;rdquo; and &amp;ldquo;control group&amp;rdquo;, and the outcome variable is &amp;ldquo;pain reduction&amp;rdquo;.
A typical approach would be to report the strenght of the difference by help of Cohen&#39;s d.</description>
    </item>
    
    <item>
      <title>Why is SD(X) unequal to MAD(X)?</title>
      <link>/2016/08/31/why-sd-is-unequal-to-mad/</link>
      <pubDate>Wed, 31 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/31/why-sd-is-unequal-to-mad/</guid>
      <description>It may seem bewildering that the standard deviation (sd) of a vector X is (generally) unequal to the mean absolute deviation from the mean (MAD) of X, ie.
$$sd(X) \ne MAD(X)$$.
One could now argue this way: well, sd(X) involves computing the mean of the squared $$x_i$$, then taking the square root of this mean, thereby &amp;ldquo;coming back&amp;rdquo; to the initial size or dimension of x (i.e, first squaring, then taking the square root).</description>
    </item>
    
    <item>
      <title>Why absolute correlation value (r) cannot exceed 1. An intuition.</title>
      <link>/2016/08/28/why-abs-correlation-is-max-1/</link>
      <pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/28/why-abs-correlation-is-max-1/</guid>
      <description>Pearson&#39;s correlation is a well-known and widely used instrument to gauge the degree of linear association of two variables (see this post for an intuition on correlation).
There a many formulas for correlation, but a short and easy one is this one:
$$r = \varnothing(z_x z_y)$$.
In words, $$r$$ can be seen as the average product of z-scores.
In &amp;ldquo;raw values&amp;rdquo;, r is given by
$$ r = \frac{\frac{1}{n}\sum{\Delta X \Delta Y}}{\sqrt{\frac{1}{n}\sum{\Delta X^2}} \sqrt{\frac{1}{n}\sum{\Delta Y^2}}} $$.</description>
    </item>
    
    <item>
      <title>Multiple t-Tests with dplyr</title>
      <link>/2016/08/18/multiple-t-tests-with-dplyr/</link>
      <pubDate>Thu, 18 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/18/multiple-t-tests-with-dplyr/</guid>
      <description>t-Test on multiple columns Suppose you have a data set where you want to perform a t-Test on multiple columns with some grouping variable. As an example, say you a data frame where each column depicts the score on some test (1st, 2nd, 3rd assignment&amp;hellip;). In each row is a different student. So you glance at the grading list (OMG!) of a teacher!
How to do do that in R? Probably, the most &amp;ldquo;natural&amp;rdquo; solution would be some lapply() call.</description>
    </item>
    
    <item>
      <title>Introduction to the measurement theory, and conjoint measurement theory</title>
      <link>/2016/08/17/intro_measurement/</link>
      <pubDate>Wed, 17 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/17/intro_measurement/</guid>
      <description>What is measurement? Why should I care?
Measurement is a basis of an empirical science. Image a geometer (a person measuring distances on the earth) with a metering rul made of rubber! Poor guy! Without proper measurement, even the smartest theory cannot be expected to be found, precisely because it cannot be measured.
So, what exactly is measurement? Measurement can be seen as tying numbers to empirical objects. But not in some arbritrary style.</description>
    </item>
    
    <item>
      <title>Intuition on correlation</title>
      <link>/2016/07/25/correlation-intuition/</link>
      <pubDate>Mon, 25 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/25/correlation-intuition/</guid>
      <description>reading time: 10 min.
Pearson’s correlation (short: correlation) is one of statistics’ all time classics. With an age of about a century, it is some kind of grand dad of analytic tools – but an oldie who is still very busy!
Formula, interpretation and application of correlation is well known.
In some non-technical lay terms, correlation captures the (linear) degree of co-variation of two linear variables. For example: if tall people have large feet (and small people small feet), on average, we say that height and foot size are correlated.</description>
    </item>
    
    <item>
      <title>Practical data cleansing in R</title>
      <link>/2016/07/24/data-cleansing/</link>
      <pubDate>Sun, 24 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/24/data-cleansing/</guid>
      <description>What is “data cleansing” about?
Data analysis, in practice, consists typically of some different steps which can be subsumed as “preparing data” and “model data” (not considering communication here):
(Inspired by this)
Often, the first major part — “prepare” — is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!). In practice, one rather has to get his (her) hands dirt…</description>
    </item>
    
    <item>
      <title>Why metric scale level cannot be taken for granted</title>
      <link>/2016/07/21/measurement-01/</link>
      <pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/21/measurement-01/</guid>
      <description>One main business for psychologists is to examine questionnaire data. Extraversion, intelligence, attitudes… That’s bread-and-butter job for (research) psychologists.
Similarly, it is common to take the metric level of questionnaire data for granted. Well, not for the item level, it is said. But for the aggregated level, oh yes, that’s OK.
Despite its popularity, the measurement basics of such practice are less clear. On which grounds can this comfortable practice be defended?</description>
    </item>
    
    <item>
      <title>Intuition on Cohen&#39;s d</title>
      <link>/2016/07/15/cohens-d-intuition/</link>
      <pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/15/cohens-d-intuition/</guid>
      <description>reading time: 5-10 min.
Cohen&#39;s d is a widely known and extensively used measure of effect size. That is, d is used to gauge how strong an effect is (given the fact that the effect exists). For example, one way to estimate d is as follows:
data(tips, package = &amp;#34;reshape2&amp;#34;) library(compute.es) t1 &amp;lt;- t.test(tip ~ sex, data = tips) t1$statistic ## t ## -1.489536 table(tips$sex) ## ## Female Male ## 87 157 tes(t1$statistic, 87, 157) ## Mean Differences ES: ## ## d [ 95 %CI] = -0.</description>
    </item>
    
    <item>
      <title>Why have z-transformed values a mean of zero and a sd of 1?</title>
      <link>/2016/07/02/z-value-intuition/</link>
      <pubDate>Sat, 02 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/02/z-value-intuition/</guid>
      <description>z-transformation is an ubiquitous operation in data analysis. It is often quite practical.
Example: Assume Dr Zack scored 42 points on a test (say, IQ). Average score is 40 in the relevant population, and SD is 1, let’s say. So Zack’s score is 2 points above average. 2 points equals to SDs in this example. We can thus safely infer that Zack is about 2 SDs above average (leaving measurement precision and other issues at side).</description>
    </item>
    
  </channel>
</rss>