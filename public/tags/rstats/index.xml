<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rstats on Data Se</title>
    <link>/tags/rstats/</link>
    <description>Recent content in Rstats on Data Se</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/rstats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Compute all pairwise differences in matrix</title>
      <link>/2018/11/21/compute-all-pairwise-differences-in-matrix/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/21/compute-all-pairwise-differences-in-matrix/</guid>
      <description>A quite frequent task in many fields of applied math is to compute pairwise differences of elements in a matrix. Actually, it need not be a difference; a product is frequent, too. In this post, we explore some (base) R ways to achieve this.
library(mosaic) library(gdata) library(tidyverse) Using outer() An elegant approach, using base R, is applying outer(). That’s useful if one has two vectors, and wants to compute the outer product:</description>
    </item>
    
    <item>
      <title>Plot columns repeatedly</title>
      <link>/2018/11/02/plot-columns-repeatedly/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/02/plot-columns-repeatedly/</guid>
      <description>Suppose you have a large number of columns of a dataframe, and you want to plot each column – say a histogram for each column.
This post shows some ways of achieving this.
Let’s take the mtcars dataset as an example.
data(mtcars) We will use the tidyverse approach:
library(tidyverse) Way 1 mtcars %&amp;gt;% select_if(is_numeric) %&amp;gt;% map2(., names(.), ~ {ggplot(data = data_frame(.x), aes(x = .x)) + geom_histogram() + labs(x= .y)}) #&amp;gt; $mpg #&amp;gt; #&amp;gt; $cyl #&amp;gt; #&amp;gt; $disp #&amp;gt; #&amp;gt; $hp #&amp;gt; #&amp;gt; $drat #&amp;gt; #&amp;gt; $wt #&amp;gt; #&amp;gt; $qsec #&amp;gt; #&amp;gt; $vs #&amp;gt; #&amp;gt; $am #&amp;gt; #&amp;gt; $gear #&amp;gt; #&amp;gt; $carb Some explanations:</description>
    </item>
    
    <item>
      <title>OECD Wellbegin - Explorative Analysis</title>
      <link>/2018/10/16/oecd-wellbegin-explorative-analysis/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/16/oecd-wellbegin-explorative-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>OECD Wellbeing - Explorative Analyse</title>
      <link>/2018/10/16/oecd-wellbeing-explorative-analyse/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/16/oecd-wellbeing-explorative-analyse/</guid>
      <description>In diesem Post untersuchen wir einige Aspekte der explorativen Datenanalyse für den Datensatz oecd wellbeing aus dem Jahr 2016.
Hinweis: Als Vertiefung gekennzeichnete Abschnitt sind nicht prüfungsrelevant.
Benötigte Pakete Ein Standard-Paket zur grundlegenden Datenanalyse:
library(mosaic)  Datensatz laden Der Datensatz kann hier bezogen werden.
Doi: https://doi.org/10.1787/data-00707-en.
Falls der Datensatz lokal (auf Ihrem Rechner) vorliegt, können Sie ihn in gewohnter Manier laden. Geben Sie dazu den Pfad zum Datensatz ein:</description>
    </item>
    
    <item>
      <title>OECD Wellbeing dataset (2016)</title>
      <link>/2018/10/16/oecd-wellbeing-dataset-2016/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/16/oecd-wellbeing-dataset-2016/</guid>
      <description>Packages We will need the following packages in this post:
library(mosaic) library(knitr) library(DT)  The OECD wellbeing study series The OECD keeps measuring the wellbeing (and associated variables) among its members states.
On the project website, the OECD states:
 In recent years, concerns have emerged regarding the fact that macro-economic statistics, such as GDP, don’t provide a sufficiently detailed picture of the living conditions that ordinary people experience. While these concerns were already evident during the years of strong growth and good economic performance that characterised the early part of the decade, the financial and economic crisis has further amplified them.</description>
    </item>
    
    <item>
      <title>Talk - Populism in tweets of German politicians (talk at DGPs 2018)</title>
      <link>/2018/09/14/talk-populism-in-tweets-of-german-politicians-talk-at-dgps-2018/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/14/talk-populism-in-tweets-of-german-politicians-talk-at-dgps-2018/</guid>
      <description>The slides of my talk Populism in tweets of German politicians
can be found here http://data-se.netlify.com/slides/populist-twitter/populist-twitter-dgps2018.html#1.
Data, code, and more can be found at Github: https://github.com/sebastiansauer/polits_tweet_mining</description>
    </item>
    
    <item>
      <title>DataExploR: Typische Businessfragen mit R analysieren</title>
      <link>/2018/09/12/dataexplor-typische-businessfragen-mit-r-analysieren/</link>
      <pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/12/dataexplor-typische-businessfragen-mit-r-analysieren/</guid>
      <description>In diesem Post untersuchen wir eine recht häufige Fragestellung im Bereich der Datenanalyse – die Auswertung von Umfragedaten. Umfragen sind eine gängige Angelegenheit in vielen Organisationen: man möchte wissen, ob die Kunden zufrieden sind oder was die Mitarbeiter vom Management denken. Wir werden nicht alle Aspekte der Analyse betrachten – da gibt es viel zu tun –, sondern ein paar zentrale Aspekte herausgreifen.
Laden wir zuerst ein paar nützliche Pakete:</description>
    </item>
    
    <item>
      <title>Wenn Excel aufgibt: Datenvisualisierung kann zu komplex für Excel werden</title>
      <link>/2018/09/11/wenn-excel-aufgibt-datenvisualisierung-kann-zu-komplex-f%C3%BCr-excel-werden/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/11/wenn-excel-aufgibt-datenvisualisierung-kann-zu-komplex-f%C3%BCr-excel-werden/</guid>
      <description>Ms Excel ist ein beliebtes Werkzeug der Datenanalyse, auch für Datenvisualisierung. Es gibt einige Beispiele, dass andere Werkzeuge, wie R, zu ansehnlicheren Diagrammen führen können, s. diesen Post. In diesem Post geht es um eine verwandte Frage: Gibt es Diagramme, die nicht – oder nur sehr aufwendig – mit Excel zu erstellen sind?
Die Meine Antwort lautet: Ja, die gibt es. Betrachten wir ein Beispiel.
Bayesianische Modelle visualisieren Als Hintergrund dient uns eine Analyse (s.</description>
    </item>
    
    <item>
      <title>Wenn Excel aussteigt: Datensatz umbauen zur Visualisierung</title>
      <link>/2018/09/11/wenn-excel-aussteigt-datensatz-umbauen-zur-visualisierung/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/11/wenn-excel-aussteigt-datensatz-umbauen-zur-visualisierung/</guid>
      <description>Warum R? Ich liebe Excel! Excel hat viele Vorteile; viele Menschen haben lange Jahre intensiv mit Excel gearbeitet und kennen sich sehr gut mit dieser Software aus. Warum sollte man mit einer neuen Software wie R arbeiten, wenn man Daten analysieren möchte?
Ein Grund ist, dass manche Sachen mit R leichter sind als mit Excel. Zum Beispiel dieser Fall: Sie haben einen Datensatz, in dem Ihre Umsätze pro Quartal wiedergegeben sind.</description>
    </item>
    
    <item>
      <title>Plotting a logistic regression - some considerations</title>
      <link>/2018/09/03/plotting-a-logistic-regression-some-considerations/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/03/plotting-a-logistic-regression-some-considerations/</guid>
      <description>library(mosaic) data(tips, package = &amp;quot;reshape2&amp;quot;) Recode sex:
tips %&amp;gt;% mutate(sex_n = case_when( sex == &amp;quot;Female&amp;quot; ~ 0, sex == &amp;quot;Male&amp;quot; ~ 1 )) -&amp;gt; tips2 Fit model:
glm1 &amp;lt;- glm(sex_n ~ total_bill, data = tips2, family = &amp;quot;binomial&amp;quot;) Way 1 plotModel(glm1)  Way 2 Add predictions to data frame:
tips2 %&amp;gt;% mutate(pred = predict(glm1, newdata = tips, type = &amp;quot;response&amp;quot;)) %&amp;gt;% mutate(predict_Male = pred &amp;gt; .5) -&amp;gt; tips3 Check values of predictions:</description>
    </item>
    
    <item>
      <title>Reproducible academic writing with RMarkdown - Talk at DGPs 2018</title>
      <link>/2018/09/03/reproducible-academic-writing-with-rmarkdown-talk-at-dgps-2018/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/03/reproducible-academic-writing-with-rmarkdown-talk-at-dgps-2018/</guid>
      <description>Talk at DGPs 2018.
Get slides here: http://data-se.netlify.com/slides/rmd-writing/rmd-writing_dgps2018.html.</description>
    </item>
    
    <item>
      <title>Bayesian modeling of populist party success in German federal elections - A notebook from the lab</title>
      <link>/2018/08/25/bayesian-modeling-of-populist-party-success-in-german-federal-elections/</link>
      <pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/25/bayesian-modeling-of-populist-party-success-in-german-federal-elections/</guid>
      <description>Following up on an earlier post, we will model the voting success of the (most prominent) populist party, AfD, in the recent federal elections. This time, Bayesian modeling techniques will be used, drawing on the excellent textbook my McElreath.
Note that this post is rather a notebook of my thinking, doing, and erring. I’ve made no efforts to hide scaffolding. I think it will be confusing to the uniniate and the initiate as well …</description>
    </item>
    
    <item>
      <title>Binning and recoding with R - some recommendations</title>
      <link>/2018/08/09/binning-and-recoding-with-r-some-recommendations/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/09/binning-and-recoding-with-r-some-recommendations/</guid>
      <description>Recoding means changing the levels of a variable, for instance changing “1” to “woman” and “2” to “man”. Binning means aggregating several variable levels to one, for instance aggregating the values From “1.00 meter” to “1.60 meter” to “small_size”.
Both operations are frequently necessary in practical data analysis. In this post, we review some methods to accomplish these two tasks.
Let’s load some example data:
data(tips, package = &amp;quot;reshape2&amp;quot;) Some packages:</description>
    </item>
    
    <item>
      <title>Finding NAs in multiples columns (per row)</title>
      <link>/2018/08/09/finding-nas-in-multiples-columns-per-rows/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/09/finding-nas-in-multiples-columns-per-rows/</guid>
      <description>Assume you would like to check for missing data, but not for one column only but for several columns.
First, data and some packages:
data(mtcars) library(tidyverse) Then, let’s introduce some missing data:
mtcars[c(1,2), 1] &amp;lt;- NA mtcars[c(1, 3:4), 2] &amp;lt;- NA Don’t check columns individually Of course, you do not want to repeat yourself, and check each column individually, like this:
sum(is.na(mtcars[[1]])) #&amp;gt; [1] 2 sum(is.na(mtcars[, 1])) # same #&amp;gt; [1] 2 Neither one would like to check each row individually:</description>
    </item>
    
    <item>
      <title>test 2018-07-26</title>
      <link>/2018/07/26/test-2018-07-26/</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/26/test-2018-07-26/</guid>
      <description>test</description>
    </item>
    
    <item>
      <title>Power calculation for the general linear model</title>
      <link>/2018/07/24/power-calculation-for-the-general-linear-model/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/24/power-calculation-for-the-general-linear-model/</guid>
      <description>Before conducting an experiment, one should compute the power - or, preferably, estimate the precision of the expected results. There are numerous way to achieve this, here’s one using the R package pwr.
Package pwr library(pwr) The workhorse function here is pwr.f2.test. Note that f2 refers to the effect size \(f^2\) (see here), defined as:
\[f^2 = \frac{R^2}{1-R^2}\].
See for details of the function its help page:
help(&amp;quot;pwr.f2.test&amp;quot;) pwr.f2.test(u = NULL, v = NULL, f2 = NULL, sig.</description>
    </item>
    
    <item>
      <title>How to prepare data for a gantt diagram</title>
      <link>/2018/07/05/how-to-prepare-data-for-a-gantt-diagram/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/05/how-to-prepare-data-for-a-gantt-diagram/</guid>
      <description>There’s the new cool world of project management - agile, scrumbling, cool. There’s the old sluggish way of project management using stuff like gantt diagrams. Let’s stick to the old world and come up with a gantt diagram.
The gant diagram itself is no big deal. Just some horizontal lines referring to dates. Somewhat more interesting is to populate a raw data frame in a way that allows for convenient plotting.</description>
    </item>
    
    <item>
      <title>Work with bibtex bib files like a pro</title>
      <link>/2018/07/05/work-with-bibtex-bib-files-like-a-pro/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/05/work-with-bibtex-bib-files-like-a-pro/</guid>
      <description>Recently, I had to curate a list of publications for our institution. Where’s the point? One might ask. Let’s leave aside that a number of colleagues do not use citation management software to work with their publications. They just hack the citation, if and when needed, in some word files. Done. Fair enough, unless someone tries to come up with a list of all the publication of that institution. In that case, the curator will need some structured data, otherwise he or she will end up copy-pasting the rest of the day.</description>
    </item>
    
    <item>
      <title>Easy way to convert factors zu numbers</title>
      <link>/2018/06/22/easy-way-to-convert-factors-zu-numbers/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/22/easy-way-to-convert-factors-zu-numbers/</guid>
      <description>Converting factors to numbers in R can be frustrating. Consider the following sitation: We have some data, and try to convert a factor (sex in tips, see below) to a numeric variable:
library(tidyverse) library(sjmisc) # for recoding data(tips, package = &amp;quot;reshape2&amp;quot;) glimpse(tips) #&amp;gt; Observations: 244 #&amp;gt; Variables: 7 #&amp;gt; $ total_bill &amp;lt;dbl&amp;gt; 16.99, 10.34, 21.01, 23.68, 24.59, 25.29, 8.77, 26.... #&amp;gt; $ tip &amp;lt;dbl&amp;gt; 1.01, 1.66, 3.50, 3.31, 3.61, 4.</description>
    </item>
    
    <item>
      <title>Some musings on the logistic map</title>
      <link>/2018/06/19/some-musings-on-the-logistic-map/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/19/some-musings-on-the-logistic-map/</guid>
      <description>The logistic map is a well-known and simple growth model that is defined by the iterative equation
\[x_{t+1} = 4rx_t(1-t_t)\],
where \(r\) is a parameter that can be thought of as a fertility and reproduction rate of the population. The allowed values of \(x\) range between 0 an 1 inclusively, where 0 means the population is extinct. The maximum of 1 can be interpreted as the ecological carrying capacity of the system.</description>
    </item>
    
    <item>
      <title>Visualizing mean values between two groups  - the tidyverse way</title>
      <link>/2018/06/10/visualizing-summary-statistics-the-tidyverse-way/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/10/visualizing-summary-statistics-the-tidyverse-way/</guid>
      <description>A frequent job in data visualizing is to present summary statistics. In this post, I show one way to plot mean values between groups using the tidyverse approach in comparison to the mosaic way.
library(tidyverse) data(mtcars) library(mosaic) library(knitr) library(sjmisc) library(sjPlot) Visualizing mean values between two groups First, let’s compute the mean hp for automatic cars (am == 0) vs. manual cars (am == 1).
mtcars %&amp;gt;% group_by(am) %&amp;gt;% summarise(hp_am = mean(hp)) -&amp;gt; hp_am Now just hand over this data frame of summarized data to ggplot:</description>
    </item>
    
    <item>
      <title>Playing around with geo mapping: combining demographic data with spatial data</title>
      <link>/2018/05/28/playing-around-with-geo-mapping-combining-demographic-data-with-spatial-data/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/28/playing-around-with-geo-mapping-combining-demographic-data-with-spatial-data/</guid>
      <description>In this post, we will play around with some basic geo mapping. More preciseyl, we will explore some easy ways to plot a choropleth map.
First, let’s load some geo data from Bundeswahlleiter, and combine it with some socio demographic data from the same source.
Preparation Let’s load some packages:
library(tidyverse) ## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.1 library(sf) library(viridis) suppressPackageStartupMessages(library(googleVis)) Geo data:
my_path_wahlkreise &amp;lt;- &amp;quot;~/Documents/datasets/geo_maps/btw17_geometrie_wahlkreise_shp/Geometrie_Wahlkreise_19DBT.shp&amp;quot; file.</description>
    </item>
    
    <item>
      <title>Playing around with dumbbell plots</title>
      <link>/2018/05/23/playing-around-with-dumbbell-plots/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/23/playing-around-with-dumbbell-plots/</guid>
      <description>Dumbbell plots can be used to show differences between two groups. Bob Rudis demonstrated a beautiful application of such plots using ggplot2 board methods.
In this plot, I will explain or comment his code, and adapt a few changes.
First, load some packages.
pacman::p_load(tidyverse, ggalt) Let’s make up some data. Tip: Make up some data conveniently in Excel, copy it to the clipboard, and then paste it as tribble (see below) into R.</description>
    </item>
    
    <item>
      <title>Playing around with dataviz: Comparing distributions between groups</title>
      <link>/2018/05/18/playing-around-dataviz-comparing-distributions-between-groups/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/18/playing-around-dataviz-comparing-distributions-between-groups/</guid>
      <description>What’ a nice way to display distributional differences between a (larger) number of groups? Boxplots is one way to go. In addition, the raw data may be shown as dots, but should be demphasized. Third, a trend or big picture comparing the groups will make sense in some cases.
Ok, based on this reasoning, let’s do som visualizing. Let’s load some data (movies), and the usual culprits of packages.</description>
    </item>
    
    <item>
      <title>Playing around with dataviz: Showing correlations</title>
      <link>/2018/05/18/playing-around-with-dataviz-showing-correlations/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/18/playing-around-with-dataviz-showing-correlations/</guid>
      <description>In this plot, we are looking into some ways of displaying association between (two) quantitative variables, aka correlation. Our goal is to present a rich representation of the correlation.
Let’s take the dataset flights as an example.
data(flights, package = &amp;quot;nycflights13&amp;quot;) library(tidyverse) ## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.1 library(viridis) flights %&amp;gt;% filter(arr_delay &amp;lt; 100, dep_delay &amp;lt; 100) %&amp;gt;% ggplot(aes(x = dep_delay, y = arr_delay, color = origin)) + geom_point(alpha = .</description>
    </item>
    
    <item>
      <title>Simulating a correlation matrix.</title>
      <link>/2018/05/18/simulating-a-correlation-matrix/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/18/simulating-a-correlation-matrix/</guid>
      <description>Simulation based inference is a powerful tool as it lets you derive population estimates without having to solve difficult equations. As a more advanced example for simulation, consider the following situation. You are interested in the correlation of air time and delay of planes. More precisely you assume that this correlation is the same for different carriers (airlines). In other words, the (absolute) difference between all different pairs of correlation coefficient is zero, according to the hypothesis.</description>
    </item>
    
    <item>
      <title>Why is the sample mean a good point estimator of the population mean? A simulation and some thoughts.</title>
      <link>/2018/05/18/why-is-the-sample-mean-a-good-point-estimator-of-the-population-mean-a-simulation-and-some-thoughts/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/18/why-is-the-sample-mean-a-good-point-estimator-of-the-population-mean-a-simulation-and-some-thoughts/</guid>
      <description>It is frequently stated that the sample mean is a good or even the best point estimator of the according population value. But why is that? In this post we are trying to get an intuition by using simulation inference methods.
Assume you played throwing coins with some one at some dark corner. “Some one” throws the coin 10 times, and wins 8 times (the guy was betting on heads, but that’s only for the sake of the story).</description>
    </item>
    
    <item>
      <title>Visualisation of interaction for the logistic regression</title>
      <link>/2018/04/02/visualisation-of-interaction-for-logistic-regression/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/02/visualisation-of-interaction-for-logistic-regression/</guid>
      <description>In this post we are plotting an interaction for a logistic regression. Interaction per se is a concept difficult to grasp; for a GLM it may be even more difficult especially for continuous variables’ interaction. Plotting helps to better or more easy grasp what a model tries to tell us.
First, load some packages.
library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ purrr 0.</description>
    </item>
    
    <item>
      <title>How to cite in markdown, a short primer</title>
      <link>/2018/03/26/how-to-cite-in-markdown-a-short-primer/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/26/how-to-cite-in-markdown-a-short-primer/</guid>
      <description>I would never write a paper again using Word - except I would be forced, too, what will happen, I think. But similarly, I don’t want to write papers using Latex - too many curly braces.
Best of both worlds: Markdown. Comes with “builtin” R.
Here’s an example how to do scholarly citation in markdown.
In-Text citation Use this notation, to cite some book or paper or whatever:
The Good, the Bad, and the Ugly face similarly difficulties [@thedude2012].</description>
    </item>
    
    <item>
      <title>Tangible data of normal distributed data</title>
      <link>/2018/03/16/tangible-data-of-normal-distributed-data/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/16/tangible-data-of-normal-distributed-data/</guid>
      <description>A classical example for a normally distributed variable is height. However, I kept on looking for data as to the mean and sd for some populations, such as Germany. Now I found some reliably looking data here.
We will not question whether the assumption of normality holds, we just assume it.
In the source, we can read that in Germany, the adult men population has the following parameters:
mean: 174cm</description>
    </item>
    
    <item>
      <title>Map students to presentation slots</title>
      <link>/2018/03/11/map-students-to-presentation-slots/</link>
      <pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/11/map-students-to-presentation-slots/</guid>
      <description>As a teacher, I not only teach but also assess the achievements of students. One example of a typical student assignments is a presentation. You know, powerpoint slides and stuff.
For that purpose, I often need to map students to one of several time slots. Here’s the R code I use for that purpose.
library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ purrr 0.</description>
    </item>
    
    <item>
      <title>Intuition to Simpson&#39;s paradox</title>
      <link>/2018/03/09/intuition-to-simpson-s-paradox/</link>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/09/intuition-to-simpson-s-paradox/</guid>
      <description>Say, you have to choose between two doctors (Anna and Berta). To decide which one is better, you check their success rates. Suppose that they deal with two conditions (Coolities and Dummities). So let’s compare their success rate for each of the two conditions (and the total success rate):
This is the proportion of healing (success) of the first doctor, Dr. Anna for each of the two conditions:
 Coolities: 7 out of 8 patients are healed from Coolities Dummieties: 1 out of 2 patients are healed from Dummities  This is the proportion of healing (success) of the first doctor, Dr.</description>
    </item>
    
    <item>
      <title>How to create columns in a dataframe in R</title>
      <link>/2018/03/07/how-to-create-columns-in-a-dataframe-in-r/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/07/how-to-create-columns-in-a-dataframe-in-r/</guid>
      <description>Note that we will use this library for this post:
library(dplyr) ## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.1 ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union By the way, loading mosaic, will load dplyr too.
One of the major data wrangling activities (in R and elsewhere) is to create a new column in a data frame.</description>
    </item>
    
    <item>
      <title>Simulate p-hacking - adding observations</title>
      <link>/2018/01/24/simulate-p-hacking-adding-observations/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/24/simulate-p-hacking-adding-observations/</guid>
      <description>Let’s simulate p-values as a funtion of sample size. We assume that some researcher collects one data point, computes the p-value, and repeats until p-value falls below some arbitrary threshold. Oh and yes, there is no real effect. For the sake of spending the budget, assume that our researcher collects a sample size of \(n=100\).
This idea stems from this great article False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant; cf.</description>
    </item>
    
    <item>
      <title>Visualizing a logistic regression the easy way</title>
      <link>/2018/01/23/visualizing-a-logistic-regression-the-easy-way/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/23/visualizing-a-logistic-regression-the-easy-way/</guid>
      <description>Let’s visualize a GLM (logistic regression).
First laod some data:
data(tips, package = &amp;quot;reshape2&amp;quot;) Compute a glm:
glm_tips &amp;lt;- glm(sex ~ tip, data = tips, family = &amp;quot;binomial&amp;quot;) Plot the model using mosaic:
library(mosaic) ## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.1 plotModel(glm_tips) The curve does not look really s-typed (ogive) but that’s ok because the data suggest not a strong trend. The plot is not very beautiful either, but hey - it’s quick to produce 😁.</description>
    </item>
    
    <item>
      <title>Zusammenhang von Lernen und Noten im Statistikunterricht</title>
      <link>/2017/12/20/zusammenhang-von-lernen-und-noten-im-statistikunterricht/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/20/zusammenhang-von-lernen-und-noten-im-statistikunterricht/</guid>
      <description>Führt Lernen zu besseren Noten? Eigene Erfahrung und allgemeiner Konsens stimmen dem zu; zumindest schadet Lernen des Stoffes nicht und hilft oft, gute Noten bei einer Prüfung zu diesem Stoff zu erzielen. Aber welche Belege, wissenschaftliche Belege gibt es dazu? An unserer Hochschule, die FOM, haben wir eine kleine Untersuchung zu dieser Frage durchgeführt. Genauer gesagt haben wir unseren Studierenden einen Statistik-Test vorlegt und gefagt, wie sehr sie sich für diesen Test vorbereitet hätten.</description>
    </item>
    
    <item>
      <title>A p-value picture</title>
      <link>/2017/11/29/a-p-value-picture/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/29/a-p-value-picture/</guid>
      <description>Much ado and to say about the p-value. Let me add one more point; actually not really from myself, but from Diez, Barr, and Cetinkaya-Rundel (2012), p. 189; good book in one is looking for “orthodox” statistics.
library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.6 ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ readr 1.</description>
    </item>
    
    <item>
      <title>Grundlagen des Textminings mit R</title>
      <link>/2017/11/28/textmining-grundlagen/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/textmining-grundlagen/</guid>
      <description>Lernziele:
  - Sie kennen zentrale Ziele und Begriffe des Textminings. - Sie wissen, was ein &amp;#39;tidy text dataframe&amp;#39; ist. - Sie können Worthäufigkeiten auszählen. - Sie können Worthäufigkeiten anhand einer Wordcloud visualisieren. In dieser Übung benötigte R-Pakete:
library(tidyverse) # Datenjudo ## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.1 library(stringr) # Textverarbeitung library(tidytext) # Textmining library(lsa) # Stopwörter library(SnowballC) # Wörter trunkieren library(wordcloud) # Wordcloud anzeigen  Bitte installieren Sie rechtzeitig alle Pakete, z.</description>
    </item>
    
    <item>
      <title>Grundlagen des Textminings mit R - Teil 2</title>
      <link>/2017/11/28/grundlagen-des-textminings-mit-r-teil-2/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/grundlagen-des-textminings-mit-r-teil-2/</guid>
      <description>In dieser Übung benötigte R-Pakete:
library(tidyverse) # Datenjudo ## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.1 library(stringr) # Textverarbeitung library(tidytext) # Textmining library(lsa) # Stopwörter library(SnowballC) # Wörter trunkieren library(wordcloud) # Wordcloud anzeigen library(skimr) # Überblicksstatistiken  Bitte installieren Sie rechtzeitig alle Pakete, z.B. in RStudio über den Reiter Packages … Install.
 ## ## Attaching package: &amp;#39;knitr&amp;#39; ## The following object is masked from &amp;#39;package:skimr&amp;#39;: ## ## kable Aus dem letzten Post Daten einlesen:</description>
    </item>
    
    <item>
      <title>Dummy variables and regression</title>
      <link>/2017/11/27/dummy-variables-and-regression/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/27/dummy-variables-and-regression/</guid>
      <description>For modeling cause-effect relationships, linear regression is among the most typically used methods.
Take, for example, the idea that the Gross Domestic Product (GDP) drives religiosity. Of course, we should have a strong theory that defends this choice and this directionality. Without a convincing theory it may be argued that the cause-relationship is the other way round or complete different (ie., some third variable accounts for any association between GDP and religiosity).</description>
    </item>
    
    <item>
      <title>Interactive diagrams in lieu of shiny?</title>
      <link>/2017/11/27/interactive-diagrams-in-lieu-of-shiny/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/27/interactive-diagrams-in-lieu-of-shiny/</guid>
      <description>One frequent use of the Shiny server software is displaying interactive data diagrams. The pro of using Shiny is the great flexibility; much more than “just graphics” can be done. Basically Shiny provides a flexible GUI for your R program. But if you simply aiming at displaying or exploring some data interactively, a much simplor approach may do it for you; there are some nice libraries available in R for that.</description>
    </item>
    
    <item>
      <title>My favorite stats text book</title>
      <link>/2017/11/27/my-favorite-stats-text-book/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/27/my-favorite-stats-text-book/</guid>
      <description>Some thoughts how my favorite applied stats text book would look like. I am looking at eg., business fields such as MBA as consumers.
My ideal applied stats text book is case study oriented (“Assume you would like to predict which movie will score highest next year based on some movie characteristics you know”)
 makes use of recent data analytics techniques such as tree based methods (Random Forests) or Shrinkage models (Lasso)</description>
    </item>
    
    <item>
      <title>Use case for purrr::map</title>
      <link>/2017/11/23/use-case-for-purrr-map/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/23/use-case-for-purrr-map/</guid>
      <description> library(tidyverse) ## ── Attaching packages ──────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.8 ## ✔ tidyr 0.8.2 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() d &amp;lt;- data_frame( id = c(1,1,1,1,1,1,2,2,3,3,3,4,1,2,2) ) d$id %&amp;gt;% map </description>
    </item>
    
    <item>
      <title>Compute effect sizes with R. A primer.</title>
      <link>/2017/11/21/compute-effect-sizes-with-r-a-primer/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/compute-effect-sizes-with-r-a-primer/</guid>
      <description>A typical “cook book recipe” for doing data analysis is an applied stats course is:
report descriptive statistics plot some nice diagrams test hypothesis report effect sizes  Let’s have a quick glance at these steps. We will use the dataset flights of the package nycflights13.
data(flights, package = &amp;quot;nycflights13&amp;quot;) This post will be tidyverse-driven.
library(tidyverse) library(skimr) library(mosaic) Let’s compute some summaries:
flights %&amp;gt;% select(arr_delay) %&amp;gt;% skim #&amp;gt; Skim summary statistics #&amp;gt; n obs: 336776 #&amp;gt; n variables: 1 #&amp;gt; #&amp;gt; Variable type: numeric #&amp;gt; variable missing complete n mean sd p0 p25 p50 p75 p100 #&amp;gt; arr_delay 9430 327346 336776 6.</description>
    </item>
    
    <item>
      <title>Get your stats result in a table easily</title>
      <link>/2017/11/21/get-your-stats-result-in-a-table-easily/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/get-your-stats-result-in-a-table-easily/</guid>
      <description>Having computed some staticis, one would like to display them. Either in a figure, on in a table, that’s the two typical ways.
Let’s explore some helper functions to get your stats to a table easily.
A nice overview on packages can be found here.
Let’s have a quick glance at these steps. We will use the dataset flights of the package nycflights13.
data(flights, package = &amp;quot;nycflights13&amp;quot;) This post will be tidyverse-driven:</description>
    </item>
    
    <item>
      <title>Pass multiple functions and arguments to purrr::map</title>
      <link>/2017/11/21/pass-multiple-functions-and-arguments-to-purrr-map/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/pass-multiple-functions-and-arguments-to-purrr-map/</guid>
      <description>I just run in the following problem: I wanted to map multiple functions to multiple columns, and I needed to pass some arguments to this map call. Sound theoretical, I know. Consider this example:
library(tidyverse) ## ── Attaching packages ──────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.8 ## ✔ tidyr 0.8.2 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.</description>
    </item>
    
    <item>
      <title>Great dataviz examples in rstats</title>
      <link>/2017/11/20/great-dataviz-examples-in-rstats/</link>
      <pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/20/great-dataviz-examples-in-rstats/</guid>
      <description>Here come some stunning examples of data visualizations, all built with R. R code of each diagram is available at the source. Enjoy! #beautiful.
UPDATE: I&amp;rsquo;ve included links to the R source!
Plotting geo maps along with subplots in ggplot2 I like this one by Ilya Kashnitsky:
Similarly, by the same author:
Source
Great work, @ikashnitsky!
Cirlize (Chord) diagrams Plotting association in a circular form yields aesthetic examples of diagrams, see the following examples</description>
    </item>
    
    <item>
      <title> Wie gut schätzt eine Stichprobe die Grundgesamtheit?</title>
      <link>/2017/11/17/inference/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/17/inference/</guid>
      <description>Daten Sie arbeiten bei der Flughafenaufsicht von NYC. Cooler Job.
library(nycflights13) data(flights)  Pakete laden library(mosaic)  Stichprobe ziehen Die Aufsichtsbehörde zieht eine Probe von 100 Flügen und ermittelt die &amp;ldquo;typische&amp;rdquo; Verspätung.
set.seed(42) sample(flights$arr_delay, size = 100) -&amp;gt; flights_sample  Und berechnen wir die typischen Kennwerte:
favstats(~flights_sample, na.rm = TRUE) #&amp;gt; min Q1 median Q3 max mean sd n missing #&amp;gt; -51 -18.75 -5 11.75 150 0.4387755 31.1604 98 2  Ob $n=3$ ausreichen würde?</description>
    </item>
    
    <item>
      <title>Some thoughts on tidyveal and environments in R</title>
      <link>/2017/11/16/tidyeval_basense/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/16/tidyeval_basense/</guid>
      <description>The tidyeval framework is a rather new, and in parts complementary, framework to dealing with non-standarde evaluation (NSE) in R. In short, NSE is about capturing some R-code, witholding execution, maybe editing the code, and finally execuing it later and/or somewhere else.
This post borrows heavily by Edwin Thon&amp;rsquo;s great post, and this post by the same author.
In addtion, most of the knowledge is derived from Hadley Wickham&amp;rsquo;s book Advanced R.</description>
    </item>
    
    <item>
      <title>Yart - Yet Another Markdown Report Template</title>
      <link>/2017/11/15/yart/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/15/yart/</guid>
      <description>It would be useful to have a RMarkdown template for typical (academic) reports such as class assigments and bachelor/master thesises. The LaTeX class &amp;ldquo;report&amp;rdquo; provides a suitable format for that. This package provides a simple wrapper around this class built on the standard pandoc template.
Thanks to Yart, ie, this package leans on earlier work by Aaron Wolen in his pandoc-letter repository, and extends it for use from R via the rmarkdown package.</description>
    </item>
    
    <item>
      <title>Populism in tweets of German politicians</title>
      <link>/2017/11/01/afd01/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/01/afd01/</guid>
      <description>The last months (years? since ever???) have seen a surge in populism and a rise in nationalism. Not only in Russia, the United States, Turkey, but also in some EU countries the ghost of nationalism-populism seems to be marching and gaining ground.
As to Germany, in September 24, 2017, the 19. German federal elections took place. The newly founded alt-right AfD (Alternative for Deutschland) has made a leap and moved in the Bundestag.</description>
    </item>
    
    <item>
      <title>Data, machine-friendly, of the 2017 German federal elections</title>
      <link>/2017/10/30/de-elec-data/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/30/de-elec-data/</guid>
      <description>On September 2017, the 19. German Bundestag has been elected. As of this writing, the parties are still busy sorting out whether they want to part of the government, with whom, and maybe whether they even want to form a government at all. This post is about providing the data in machine friendly form, and in English language.
All data presented in this post regarding this (and previous) elections are published by the Bundeswahlleiter.</description>
    </item>
    
    <item>
      <title>Mapping foreigner ratio to AfD election results in the German Wahlkreise</title>
      <link>/2017/10/22/afd-map-foreigners/</link>
      <pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/22/afd-map-foreigners/</guid>
      <description>In a previous post, we have shed some light on the idea that populism - as manifested in AfD election results - is associated with socioeconomic deprivation, be it subjective or objective. We found some supporting pattern in the data, although that hypothesis is far from being complete; ie., most of the variance remained unexplained.
In this post, we test the hypothesis that AfD election results are negatively associated with the proportion of foreign nationals in a Wahlkreis.</description>
    </item>
    
    <item>
      <title>Mapping foreigner ratio to AfD election results in the German Wahlkreise</title>
      <link>/2017/10/22/mapping-foreigner-ratio-to-afd-election-results-in-the-german-wahlkreise/</link>
      <pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/22/mapping-foreigner-ratio-to-afd-election-results-in-the-german-wahlkreise/</guid>
      <description>In a previous post, we have shed some light on the idea that populism - as manifested in AfD election results - is associated with socioeconomic deprivation, be it subjective or objective. We found some supporting pattern in the data, although that hypothesis is far from being complete; ie., most of the variance remained unexplained.
In this post, we test the hypothesis that AfD election results are negatively associated with the proportion of foreign nationals in a Wahlkreis.</description>
    </item>
    
    <item>
      <title>Simple way to separate train and test sample in R</title>
      <link>/2017/10/17/train-test/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/17/train-test/</guid>
      <description>For statistical modeling, it is typical to separate a train sample from a test sample. The training sample is used to build (&amp;ldquo;train&amp;rdquo;) the model, whereas the test sample is used to gauge the predictive quality of the model.
There are many ways to split off a test sample from the train sample. One quite simple, tidyverse-oriented way, is the following.
First, load the tidyverse. Next, load some data.
library(tidyverse) data(Affairs, package = &amp;quot;AER&amp;quot;)  Then, create an index vector of the length of your train sample, say 80% of the total sample size.</description>
    </item>
    
    <item>
      <title>Two R plot side by side in .Rmd-Files</title>
      <link>/2017/10/12/two-plots-rmd/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/12/two-plots-rmd/</guid>
      <description>I kept wondering who to plot two R plots side by side (ie., in one &amp;ldquo;row&amp;rdquo;) in a .Rmd chunk. Here&amp;rsquo;s a way, well actually a number of ways, some good, some &amp;hellip; not.
library(tidyverse) library(gridExtra) library(grid) library(png) library(downloader) library(grDevices) data(mtcars)  Plots from ggplot Say, you have two plots from ggplot2, and you would like them to put them next to each other, side by side (not underneath each other):</description>
    </item>
    
    <item>
      <title>Mapping unemployment ratio to AfD election results in German Wahlkreise</title>
      <link>/2017/10/10/afd-map/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/10/afd-map/</guid>
      <description>There is the idea that the alt-right German party AfD is followed by those who are deprived of chances, thoses of fearing to falling down the social ladder, and so on. Let&amp;rsquo;s test this hypothesis. No, I am not thinking on hypothesis testing, p-values, and stuff. Rather, let&amp;rsquo;s color a map of German election districts (Wahlkreise) according to whether the area is poor AND the AfD gained a lot of votes (and vice versa: the area is rich AND the AfD gained relatively few votes).</description>
    </item>
    
    <item>
      <title>Mapping unemployment rate to German district areas</title>
      <link>/2017/10/09/unemp-map/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/09/unemp-map/</guid>
      <description>A chloropleth map is a geographic map where statistical information are mapped to certain areas. Let&amp;rsquo;s plot such a chloropleth map in this post.
Packages library(sf) library(stringr) library(tidyverse) library(readxl)  Geo data Best place to get German geo data is from the &amp;ldquo;Bundesamt für Kartografie und Geodäsie (BKG)&amp;rdquo;. One may basically use the data for a purposes unless it is against the law. I have downloaded the data 2017-10-09. More specifically, we are looking at the &amp;ldquo;Verwaltungsgebiete&amp;rdquo; (vg), that is, the administrative areas of the country, ie.</description>
    </item>
    
    <item>
      <title>Drawing a country map</title>
      <link>/2017/10/06/chloromap/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/06/chloromap/</guid>
      <description>Let&amp;rsquo;s draw a map of Bavaria, a state of Germany, in this post.
Packages library(tidyverse) library(maptools) library(sf) library(RColorBrewer) library(ggmap) library(viridis) library(stringr)  Data Let&amp;rsquo;s get the data first. Basically, we need to data files:
 the shape file, ie., a geographic details of state borders and points of interest the semantic information to points of interest eg., town names  Shape file The shape file can be downloaded from this source: http://www.</description>
    </item>
    
    <item>
      <title>Different ways to count NAs over multiple columns</title>
      <link>/2017/09/08/sum-isna/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/08/sum-isna/</guid>
      <description>There are a number of ways in R to count NAs (missing values). A common use case is to count the NAs over multiple columns, ie., a whole dataframe. That&amp;rsquo;s basically the question &amp;ldquo;how many NAs are there in each column of my dataframe&amp;rdquo;? This post demonstrates some ways to answer this question.
Way 1: using sapply A typical way (or classical way) in R to achieve some iteration is using apply and friends.</description>
    </item>
    
    <item>
      <title>Different ways to present summaries in ggplot2</title>
      <link>/2017/09/08/ggplot-summaries/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/08/ggplot-summaries/</guid>
      <description>A convenient and well applicable visualization for comparing groups with respect to a metric variable is the boxplot. However, often, comparing means is accompanied by t-tests, ANOVAs, and friends. Such tests test the mean, not the median, and hence the boxplot is presenting the tested statistic. It would be better to align test and diagram. How can that be achieved using ggplot2? This posts demonstrates some possibilities.
First, let&amp;rsquo;s plot a boxplot.</description>
    </item>
    
    <item>
      <title>Replacing dplyr::do by purrr:map. Some considerations</title>
      <link>/2017/09/05/purrr-map-no-do/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/05/purrr-map-no-do/</guid>
      <description>Hadley Wickham has announced to depreceate dplyr::do in favor of purrr:map. In a recent post, I have made use of do, so some commentators informed me about that. In this post, I will show use cases of map, specifically as a replacement of do. map is for lists; read more about lists here.
library(tidyverse) library(broom)  We will use mtcars as a sample dataframe (boring, I know, but convenient).</description>
    </item>
    
    <item>
      <title>Comparing the pipe with base methods</title>
      <link>/2017/08/31/some-pipes/</link>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/31/some-pipes/</guid>
      <description>Some say, the pipe (#tidyverse) makes analyses in R easier. I agree. This post demonstrates some examples.
Let&amp;rsquo;s take the mtcars dataset as an example.
data(mtcars) ?mtcars  Say, we would like to compute the correlation between gasoline consumption (mpg) and horsepower (hp).
Base approach 1 cor(mtcars[, c(&amp;quot;mpg&amp;quot;, &amp;quot;hp&amp;quot;)])  ## mpg hp ## mpg 1.0000000 -0.7761684 ## hp -0.7761684 1.0000000  We use the [-operator (function) to select the columns; note that df[, c(col1, col2)] sees dataframes as matrices, and spits out a dataframe, not a vector:</description>
    </item>
    
    <item>
      <title>Precipitation - It never rains in Southern Nuremberg (?). Working with dates/times.</title>
      <link>/2017/08/01/weather/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/01/weather/</guid>
      <description>In this post, we will explore some date and time parsing. As an example, we will work with weather data provided by City of Nuremberg, Environmental and Meteorological Data.
We will need these packages:
library(tidyverse) # data reading and wrangling library(lubridate) # working with dates/times  First, let&amp;rsquo;s import some precipitation data:
file_name &amp;lt;- &amp;quot;~/Downloads/export-sun-nuremberg--flugfeld--airport--precipitation-data--1-hour--individuell.csv&amp;quot; rain &amp;lt;- read_csv2(file_name, skip = 13, col_names = FALSE)  ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 1)  ## Warning: 300 parsing failures.</description>
    </item>
    
    <item>
      <title>Programming with dplyr: Part 02, writing a function</title>
      <link>/2017/07/06/prop_fav/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/06/prop_fav/</guid>
      <description>Recently, since dplyr &amp;lt;= 0.6.0 a new way of dealing with NSE was introduced, called tidyeval. As with every topic that begs our attention, the question &amp;ldquo;why bother&amp;rdquo; is in place. Theone answer is &amp;ldquo;you&amp;rsquo;ll need this stuff if you want to lock dplyr verbs inside a function&amp;rdquo;. Once you like dplyr and friends, a natural second step is to use the ideas not only for interactive use, but for more &amp;ldquo;programming&amp;rdquo; type, ie.</description>
    </item>
    
    <item>
      <title>Effect sizes for the Mann-Whitney U Test: an intuition</title>
      <link>/2017/07/04/effsize_utest/</link>
      <pubDate>Tue, 04 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/04/effsize_utest/</guid>
      <description>The Mann-Whitney U-Test is a test with a wide applicability, wider than the t-Test. Why that? Because the U-Test is applicable for ordinal data, and it can be argued that confining the metric level of a psychological variable to ordinal niveau is a reasonable bet. Second, it is robust, more robust than the t-test, because it only considers ranks, not raw values. In addition, some say that the efficiency of the U-Test is very close to the t-Test (.</description>
    </item>
    
    <item>
      <title>A second look to grouping with dplyr</title>
      <link>/2017/06/28/second_look_group_by/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/28/second_look_group_by/</guid>
      <description>The one basic idea of dplyr is that each function should focus on one job. That&amp;rsquo;s why there are no functions such as compute_sumamries_by_group_with_robust_variants(df). Rather, summarising and grouping are seen as different jobs which should be accomplished by different functions. And, in turn, that&amp;rsquo;s why group_by, the grouping function of dplyr, is of considerable importance: this function should do the grouping for each operation whatsoever.
Let&amp;rsquo;s load all tidyverse libraries in one go:</description>
    </item>
    
    <item>
      <title>Programming with dplyr: Part 01, introduction</title>
      <link>/2017/06/28/prog_dplyr_01/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/28/prog_dplyr_01/</guid>
      <description>Like for [others], Hadley Wickham&amp;rsquo;s dplyr, and more generally, the tidyverse approach has considerably changed the I do data analyses. Most notably, the pipe (coming from magrittr by Stefan Milton Bache, see here) has creeped into nearly every analyses I, do.
That is, is every analyses except for functions, and other non-interactive stuff. In those programming contexts, the dplyr way does not work, due to its non standard evaluation or NSE for short.</description>
    </item>
    
    <item>
      <title>Preparation of extraversion survey data</title>
      <link>/2017/06/24/extra_prep/</link>
      <pubDate>Sat, 24 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/24/extra_prep/</guid>
      <description>For teaching purposes and out of curiosity towards some psychometric questions, I have run a survey on extraversion here. The dataset has been published at OSF (DOI 10.17605/OSF.IO/4KGZH). The survey is base on a google form, which in turn saves the data in Google spreadsheet. Before the data can be analyzed, some preparation and makeup is in place. This posts shows some general makeup, typical for survey data.
Download the data and load packages Download the data from source (Google spreadsheets); the package gsheet provides an easy interface for that purpose.</description>
    </item>
    
    <item>
      <title>Print csv-file tables as plots</title>
      <link>/2017/06/22/tab2plot/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/22/tab2plot/</guid>
      <description>tl;dr Use this convenience function to print a dataframe as a png-plot: tab2grob().
Source the function here: https://sebastiansauer.github.io/Rcode/tab2grob.R
Easiest way in R:
source(&amp;quot;https://sebastiansauer.github.io/Rcode/tab2grob.R&amp;quot;)  Printing csv-dataframes as ggplot plots Recently, I wanted to print dataframes not as normal tables, but as a png-plot. See:
Why? Well, basically as a convenience function for colleagues who are not into using Markdown &amp;amp; friends. As I am preparing some stats stuff (see my new open access course material here) using RMarkdown, I wanted to prepare the materials ready for using in Powerpoint.</description>
    </item>
    
    <item>
      <title>Identifying the package of a function</title>
      <link>/2017/06/12/finds_funs/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/12/finds_funs/</guid>
      <description>tl;dr Suppose you want to know which package(s) a given R function belongs to, say filter. Here come find_funsto help you:
find_funs(&amp;quot;filter&amp;quot;)  ## # A tibble: 4 x 3 ## package_name builtin_pckage loaded ## &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt; &amp;lt;lgl&amp;gt; ## 1 base TRUE TRUE ## 2 dplyr FALSE TRUE ## 3 plotly FALSE FALSE ## 4 stats TRUE TRUE  This function will search all installed packages for this function name.</description>
    </item>
    
    <item>
      <title>Sorting the x-axis in bargraphs using ggplot2</title>
      <link>/2017/06/05/ordering-bars/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/05/ordering-bars/</guid>
      <description>Some time ago, I posted about how to plot frequencies using ggplot2. One point that remained untouched was how to sort the order of the bars. Let&amp;rsquo;s look at that issue here.
First, let&amp;rsquo;s load some data.
data(tips, package = &amp;quot;reshape2&amp;quot;)  And the usual culprits.
library(tidyverse) library(scales) # for percentage scales  First, let&amp;rsquo;s plot a standard plot, with bars *un*sorted.
tips %&amp;gt;% count(day) %&amp;gt;% mutate(perc = n / nrow(tips)) -&amp;gt; tips2 ggplot(tips2, aes(x = day, y = perc)) + geom_bar(stat = &amp;quot;identity&amp;quot;)  Hang on, what could &amp;lsquo;unsorted&amp;rsquo; possibly mean?</description>
    </item>
    
    <item>
      <title>mean and sd of z-values</title>
      <link>/2017/05/26/z-values/</link>
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/26/z-values/</guid>
      <description>Edit: This post was updated, including two errors fixed - thanks to (private) comments from Norman Markgraf.
z-values, aka values coming from an z-transformation are a frequent creature in statistics land. Among their properties are the following:
 mean is zero variance is one (and hence sd is one)  But why is that? How come that this two properties are true? The goal of this post is to shed light on these two properties of z-values.</description>
    </item>
    
    <item>
      <title>Simple way of plotting normal/logistic/etc. curve</title>
      <link>/2017/05/24/plotting_s-curve/</link>
      <pubDate>Wed, 24 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/24/plotting_s-curve/</guid>
      <description>Plotting a function is often helpful to better understand what&amp;rsquo;s going on. Plotting curves in R base is simple by virtue of function curve. But how to draw curves using ggplot2?
That&amp;rsquo;s a little bit more complicated by can still be accomplished by 1-2 lines.
library(ggplot2)  Normal curve p &amp;lt;- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) p + stat_function(fun = dnorm, n = 101)  stat_function is some kind of parallel function to curve.</description>
    </item>
    
    <item>
      <title>A predictor&#39;s unique contribution - (visual) demonstration</title>
      <link>/2017/05/17/storks/</link>
      <pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/17/storks/</guid>
      <description>A well-known property of regression models is that they capture the unique contribution of a predictor. By &amp;ldquo;unique&amp;rdquo; we mean the effect of the predictor (on the criterion) if the other predictor(s) is/are held constant. A typical classroom example goes along the following lines.
All about storks  There&amp;rsquo;s a correlation between babies and storks. Counties with lots of storks enjoy large number of babies and v.v.
 However, I have children, I know the storks are not overly involved in that business, so says the teacher (polite laughters in the audience).</description>
    </item>
    
    <item>
      <title>Crashkurs Datenanalyse mit R</title>
      <link>/2017/05/16/crashkurs/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/16/crashkurs/</guid>
      <description>Nicht jeder liebt Datenanalyse und Statistik&amp;hellip; in gleichem Maße. Das ist zumindest meine Erfahrung aus dem Unterricht 🔥. Crashkurse zu R sind vergleichbar zu Crahskursen zu Französisch - kann man machen, aber es sollte die Maxime gelten &amp;ldquo;If everything else fails&amp;rdquo;.
Dieser Crashkurs ist für Studierende oder Anfänger der Datenanalyse gedacht, die in kurzer Zeit einen verzweifelten Versuch &amp;hellip; äh &amp;hellip; einen grundständigen Überblick über die Datenanalyse erwerben wollen.</description>
    </item>
    
    <item>
      <title>Crashkurs Datenanalyse mit R</title>
      <link>/2017/05/16/crashkurs/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/16/crashkurs/</guid>
      <description>Nicht jeder liebt Datenanalyse und Statistik… in gleichem Maße. Das ist zumindest meine Erfahrung aus dem Unterricht :neckbeard: :fire:. Crashkurse zu R sind vergleichbar zu Crahskursen zu Französisch - kann man machen, aber es sollte die Maxime gelten “If everything else fails”.
Dieser Crashkurs ist für Studierende oder Anfänger der Datenanalyse gedacht, die in kurzer Zeit einen verzweifelten Versuch … äh … einen grundständigen Überblick über die Datenanalyse erwerben wollen.</description>
    </item>
    
    <item>
      <title>Plotting true random numbers</title>
      <link>/2017/05/12/true_random/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/12/true_random/</guid>
      <description>knitr::opts_chunk$set(fig.align = &amp;quot;center&amp;quot;, out.width = &amp;quot;70%&amp;quot;, fig.asp = .61)  Every now and then, random numbers come in handy to demonstrate some statistical behavior. Of course, well-known appraoches are rnorm and friends. These functions are what is called pseudo random number generators, because they are not random at all, strictly speaking, but determined by some algorithm. An algorithm is a sort of creature that is 100% predictable once you know the input (and the details of the algorithm).</description>
    </item>
    
    <item>
      <title>Deriving the logits for logistic regression</title>
      <link>/2017/05/06/deriving-the-logits-for-logistic-regression/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/06/deriving-the-logits-for-logistic-regression/</guid>
      <description>The logistic regression is an incredible useful tool, partly because binary outcomes are so frequent in live (“she loves me - she doesn’t love me”). In parts because we can make use of well-known “normal” regression instruments.
But the formula of logistic regression appears opaque to many (beginners or those with not so much math background).
Let’s try to shed some light on the formula by discussing some accessible explanation on how to derive the formula.</description>
    </item>
    
    <item>
      <title>Variance explained vs. variance blurred</title>
      <link>/2017/05/05/explained_variance/</link>
      <pubDate>Fri, 05 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/05/explained_variance/</guid>
      <description>Frequently, someones says that some indicator variable X &amp;ldquo;explains&amp;rdquo; some proportion of some target variable, Y. What does this actually mean? By &amp;ldquo;mean&amp;rdquo; I am trying to find some intuition that &amp;ldquo;clicks&amp;rdquo; rather than citing the (well-known) formualas.
To start with, let&amp;rsquo;s load some packages and make up some random data.
library(tidyverse)  n_rows &amp;lt;- 100 set.seed(271828) df &amp;lt;- data_frame( exp_clean = rnorm(n = n_rows, mean = 2, sd = 1), cntrl_clean = rnorm(n = n_rows, mean = 0, sd = 1), exp_noisy = exp_clean + rnorm(n = n_rows, mean = 0, sd = 3), cntrl_noisy = cntrl_clean + rnorm(n = n_rows, mean = 0, sd = 3), ID = 1:n_rows)  Here, we drew 100 cases from the population of the &amp;ldquo;experimental group&amp;rdquo; (mue = 2) and 100 cases from the control group (mue = 0).</description>
    </item>
    
    <item>
      <title>Einführung in die Datenanalyse mit R-Paket &#39;dplyr&#39; - R User Group Nürnberg</title>
      <link>/2017/04/27/datenanalyse_mit_dplyr/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/27/datenanalyse_mit_dplyr/</guid>
      <description>Datenjudo mit dplyr Einleitung Innerhalb der R-Landschaft hat sich das Paket dplyr binnen kurzer Zeit zu einem der verbreitesten Pakete entwickelt; es stellt ein innovatives Konzept der Datenanalyse zur Verfügung. dplyr zeichnet sich durch zwei Ideen aus. Die erste Idee ist, dass nur Tabellen (&amp;ldquo;dataframes&amp;rdquo; oder &amp;ldquo;tibbles&amp;rdquo;) verarbeitet werden, keine anderen Datenstrukturen. Diese Tabellen werden von Funktion zu Funktion durchgereicht. Der Fokus auf Tabellen vereinfacht die Analyse, da Spalten nicht einzeln oder mittels Schleifen werden müssen.</description>
    </item>
    
    <item>
      <title>Plotting skewed distributions</title>
      <link>/2017/04/19/skewed-distribs/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/19/skewed-distribs/</guid>
      <description>Let&amp;rsquo;s plot some skewed stuff, aehm, distributions!
Actually, the point I - initially - wanted to make is that in skewed distribution, don&amp;rsquo;t use means. Or at least, be very aware that (arithmetic) means can be grossly misleading. But for today, let&amp;rsquo;s focus on drawing skewed distributions.
Some packages:
library(tidyverse) library(fGarch) # for snorm  Some skewed distribution include:
 &amp;ldquo;polluted&amp;rdquo; normal distributions, ie., mixtures of two normals Exponential distributions Gamma distributions Beta distributions  One way to visualize them is to draw their curve, ie.</description>
    </item>
    
    <item>
      <title>Error bars for interaction effects with nominal variables</title>
      <link>/2017/04/18/moderator-errorbars/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/18/moderator-errorbars/</guid>
      <description>Moderator effects (ie., interaction or synergy effects) are a topic of frequent interest in many sciences braches. A lot ink has been spilled over this topic (so did I, eg., here).
However, in that post I did now show how to visualize error in case of nominal (categorical) independent variable, and categorical moderator.
Luckily, visualization of this case is quite straight forward with ggplot2.
First, some data and packages to be loaded:</description>
    </item>
    
    <item>
      <title>The effect of sample on p-values. A simulation.</title>
      <link>/2017/04/13/pvalue_sample_size/</link>
      <pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/13/pvalue_sample_size/</guid>
      <description>It is well-known that the notorious p-values is sensitive to sample size: The larger the sample, the more bound the p-value is to fall below the magic number of .05.
Of course, the p-value is also a function of the effect size, eg., the distance between two means and the respective variances. But still, the p-values tends to become significant in the face of larges samples, and non-significant otherwise.</description>
    </item>
    
    <item>
      <title>Three ways to dichotomize a variable</title>
      <link>/2017/04/11/three_ways_recoding_cutting/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/11/three_ways_recoding_cutting/</guid>
      <description>Dichotomizing is also called dummy coding. It means: Take a variable with multiple different values (&amp;gt;2), and transform it so that the output variable has 2 different values.
Note that this &amp;ldquo;thing&amp;rdquo; can be understood as consisting of two different aspects: Recoding and cutting. Recoding means that value &amp;ldquo;a&amp;rdquo; becomes values &amp;ldquo;b&amp;rdquo; etc. Cutting means that a &amp;ldquo;rope&amp;rdquo; of numbers is cut into several shorter &amp;ldquo;ropes&amp;rdquo; (that&amp;rsquo;s why it is called cutting).</description>
    </item>
    
    <item>
      <title>Rowwise operations in dplyr</title>
      <link>/2017/03/27/rowwise_dplyr/</link>
      <pubDate>Mon, 27 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/27/rowwise_dplyr/</guid>
      <description>R thinks columnwise, not rowwise, at least in standard dataframe operations. A typical rowwise operation is to compute row means or row sums, for example to compute person sum scores for psychometric analyses.
One workaround, typical for R, is to use functions such as apply (and friends).
However, dplyr offers some quite nice alternative:
library(dplyr) mtcars %&amp;gt;% rowwise() %&amp;gt;% mutate(mymean=mean(c(cyl,mpg))) %&amp;gt;% select(cyl, mpg, mymean)  ## Source: local data frame [32 x 3] ## Groups: &amp;lt;by row&amp;gt; ## ## # A tibble: 32 × 3 ## cyl mpg mymean ## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; ## 1 6 21.</description>
    </item>
    
    <item>
      <title>Convert list to dataframe</title>
      <link>/2017/03/08/convert_list_to_dataframe/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/08/convert_list_to_dataframe/</guid>
      <description>A handy function to iterate stuff is the function purrr::map. It takes a function and applies it to all elements of a given vector. This vector can be a data frame - which is a list, tecnically - or some other sort of of list (normal atomic vectors are fine, too).
However, purrr::map is designed to return lists (not dataframes). For example, if you apply mosaic::favstats to map, you will get some favorite statistics for some variable:</description>
    </item>
    
    <item>
      <title>How to avoid Github/merge conflicts with Rmd-files</title>
      <link>/2017/03/06/avoid_merge_conflicts/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/06/avoid_merge_conflicts/</guid>
      <description>One nice features of .rmd files is that version control systems, such as git and github, can (quite) easily be combined. However, in my experience, merge conflicts are not so uncommon. That raises the question how to avoid merge conflicts when syncing with Github?
Here&amp;rsquo;s a quick overview on what to do to that hassle:
 Sync often. Hard wrap the lines to approx. 80 characters. Pull before you start to change the source files.</description>
    </item>
    
    <item>
      <title>Lieblings-R-Befehle</title>
      <link>/2017/03/05/lieblingsbefehle/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/05/lieblingsbefehle/</guid>
      <description>Hier eine Liste einiger meiner &amp;ldquo;Lieblings-R-Funktionen&amp;rdquo;; für Einführungsveranstaltungen in Statistik spielen sie (bei mir) eine wichtige Rolle. Die Liste kann sich ändern :-)
Wenn ich von einer &amp;ldquo;Tabelle&amp;rdquo; spreche, meine ich sowohl Dataframes als auch Tibbles.
Zuweisung - &amp;lt;- Mit dem Zuweisungsoperator &amp;lt;- kann man Objekten einen Wert zuweisen:
x &amp;lt;- 1 mtcars2 &amp;lt;- mtcars  Spalten als Vektor auswählen - $ Mit dem Operator $ kann man eine Spalte einer Tabelle auswählen.</description>
    </item>
    
    <item>
      <title>Checklist for Data Cleansing</title>
      <link>/2017/02/13/data_cleansing/</link>
      <pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/13/data_cleansing/</guid>
      <description>What this post is about: Data cleansing in practice with R Data analysis, in practice, consists typically of some different steps which can be subsumed as &amp;ldquo;preparing data&amp;rdquo; and &amp;ldquo;model data&amp;rdquo; (not considering communication here):
(Inspired by this)
Often, the first major part &amp;ndash; &amp;ldquo;prepare&amp;rdquo; &amp;ndash; is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!</description>
    </item>
    
    <item>
      <title>Dataset &#39;performance in stats test&#39;</title>
      <link>/2017/01/27/data_test_inference/</link>
      <pubDate>Fri, 27 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/27/data_test_inference/</guid>
      <description>This posts shows data cleaning and preparation for a data set on a statistics test (NHST inference). Data is published under a CC-licence, see here.
Data was collected 2015 to 2017 in statistics courses at the FOM university in different places in Germany. Several colleagues helped to collect the data. Thanks a lot! Now let&amp;rsquo;s enjoy the outcome (and make it freely available to all).
Raw N is 743. The test consists of 40 items which are framed as propositions; students are asked to respond with either &amp;ldquo;true&amp;rdquo; or &amp;ldquo;false&amp;rdquo; to each item.</description>
    </item>
    
    <item>
      <title>Convert logit to probability</title>
      <link>/2017/01/24/convert_logit2prob/</link>
      <pubDate>Tue, 24 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/24/convert_logit2prob/</guid>
      <description>Logistic regression may give a headache initially. While the structure and idea is the same as &amp;ldquo;normal&amp;rdquo; regression, the interpretation of the b&amp;rsquo;s (ie., the regression coefficients) can be more challenging.
This post provides a convenience function for converting the output of the glm function to a probability. Or more generally, to convert logits (that&amp;rsquo;s what spit out by glm) to a probabilty.
Note1: The objective of this post is to explain the mechanics of logits.</description>
    </item>
    
    <item>
      <title>The two ggplot2-ways of plottings bars</title>
      <link>/2017/01/20/two_ways_barplots_with_ggplot2/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/20/two_ways_barplots_with_ggplot2/</guid>
      <description>Bar plots, whereas not appropriate for means, are helpful for conveying impressions of frequencies, particularly relative frequencies, ie., proportions.
Intuition: Bar plots and histograms alike can be thought of as piles of Lego pieces, put onto each each other, where each Lego piece represents (is) one observation.
Presenting tables of frequencies are often not insightful to the eye. Bar plots are often much more accessible and present the story more clearly.</description>
    </item>
    
    <item>
      <title>Fallstudie (YACSDA) zur praktischen Datenanalyse mit dplyr</title>
      <link>/2017/01/18/fallstudie_flights/</link>
      <pubDate>Wed, 18 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/18/fallstudie_flights/</guid>
      <description>Case study in data analysis using R package dplyr in German language.
Praktische Datenanalyse mit dplyr Das R-Paket dplyr von Hadley Wickham ist ein Stargast auf der R-Showbühne; häufig diskutiert in einschlägigen Foren. Mit dyplr kann man Daten &amp;ldquo;verhackstücken&amp;rdquo; - umformen und aufbereiten (&amp;ldquo;to wrangle&amp;rdquo; auf Englisch); &amp;ldquo;praktische Datenanalyse&amp;rdquo; ist vielleicht eine gute Bezeichnung. Es finden sich online viele Einführungen, z.B. hier oder hier.
Dieser Text ist nicht als Einführung oder Erläuterung gedacht, sondern als Übung, um (neu erworbenen Fähigkeiten) in der praktischen Datenanalyse im Rahmen einer Fallstudie auszuprobieren.</description>
    </item>
    
    <item>
      <title>Visualizing Interaction Effects with ggplot2</title>
      <link>/2017/01/17/vis_interaction_effects/</link>
      <pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/17/vis_interaction_effects/</guid>
      <description>Moderator effects or interaction effect are a frequent topic of scientific endeavor. Put bluntly, such effects respond to the question whether the input variable X (predictor or independent variable IV) has an effect on the output variable (dependent variable DV) Y: &amp;ldquo;it depends&amp;rdquo;. More precisely, it depends on a second variable, M (Moderator).
More formally, a moderation effect can be summarized as follows:
 If the effect of X on Y depends on M, a moderator effect takes place.</description>
    </item>
    
    <item>
      <title>How to import a strange CSV</title>
      <link>/2017/01/12/strange_csvs/</link>
      <pubDate>Thu, 12 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/12/strange_csvs/</guid>
      <description>A typical task in data analysis is to import CSV-formatted data. CSV is nothing more than a text file with data in rectangular form; rows stand for observations (eg., persons), and columns represent variables (such as age). Columns are separed by a &amp;ldquo;separator&amp;rdquo;, often a comma. Hence the name &amp;ldquo;CSV&amp;rdquo; - &amp;ldquo;comma separeted values&amp;rdquo;. Note however that the separator can in principle anything you like (eg., &amp;ldquo;;&amp;rdquo; or tabulator or &amp;ldquo; &amp;ldquo;).</description>
    </item>
    
    <item>
      <title>R startet nicht</title>
      <link>/2017/01/11/r_startet_nicht/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/11/r_startet_nicht/</guid>
      <description>Hilfe! Mein R startet nicht! Mein R startet zwar, tut aber nicht so, wie ich will. Sicherlich hat es sich (wieder einmal) gegen mich verschworen. Wahrscheinlich hilft nur noch Verschrotten&amp;hellip; Bevor Sie zum äußersten schreiten, hier einige Tipps, die sich bewährt haben.
Lösungen, wenn R nicht (richtig) läuft  AEG: Aus. Ein. Gut. Starten Sie den Rechner neu. Gerade nach Installation neuer Software zu empfehlen.
 Sehen Sie eine Fehlermeldung, die von einem fehlenden Paket spricht (z.</description>
    </item>
    
    <item>
      <title>Convert data frame from &#39;wide&#39; to &#39;long&#39;</title>
      <link>/2017/01/06/facial_beauty/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/06/facial_beauty/</guid>
      <description>Thanks to my student Marie Halbich who took the pains to collect the data!
At times, your data set will be in &amp;ldquo;wide&amp;rdquo; format, i.e, many columns in comparison to rows. For some analyses however, it is more suitable to have the data in &amp;ldquo;long&amp;rdquo; format. That is, many rows in comparison to columns.
Let&amp;rsquo;s have a look at this data set, for example.
d &amp;lt;- read.csv(&amp;quot;https://sebastiansauer.github.io/data/facial_beauty_raw.csv&amp;quot;)  This is the data from a study tapping into the effect of computerized &amp;ldquo;beautification&amp;rdquo; of some faces on subjective &amp;ldquo;like&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>YACSDA (Fallstudie) zum Datensatz &#39;Affairs&#39;</title>
      <link>/2017/01/05/yacsda_affairs/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/05/yacsda_affairs/</guid>
      <description>This YACSDA (Yet-another-case-study-on-data-analysis) in composed in German language. Some typical data analytical steps are introduced.
Wovon ist die Häufigkeit von Affären (Seitensprüngen) in Ehen abhängig? Diese Frage soll anhand des Datensates Affair untersucht werden.
Dieser Post stellt beispielhaft eine grundlegende Methoden der praktischen Datenanalyse im Rahmen einer kleinen Fallstudie (YACSDA) vor.
Quelle der Daten: http://statsmodels.sourceforge.net/0.5.0/datasets/generated/fair.html
Der Datensatz findet sich (in ähnlicher Form) auch im R-Paket COUNT (https://cran.r-project.org/web/packages/COUNT/index.html).
Laden wir als erstes den Datensatz in R.</description>
    </item>
    
    <item>
      <title>Why is the variance additive? An intuition.</title>
      <link>/2017/01/04/additivity_variance/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/04/additivity_variance/</guid>
      <description>The variance of some data can be defined in rough terms as the mean of the squared deviations from the mean.
Let&amp;rsquo;s repeat that because it is important:
 Variance: Mean of squared deviations from the mean.
 An example helps to illustrate. Assume some class of students are forced to write an exam in a statistics class (OMG). Let&amp;rsquo;s say the grades range fom 1 to 6, 1 being the best and 6 the worst.</description>
    </item>
    
    <item>
      <title>Überleben auf der Titanic - YACSDA für nominale Daten</title>
      <link>/2016/12/22/titanic/</link>
      <pubDate>Thu, 22 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/22/titanic/</guid>
      <description>In dieser YACSDA (Yet-another-case-study-on-data-analysis) geht es um die beispielhafte Analyse nominaler Daten anhand des &amp;ldquo;klassischen&amp;rdquo; Falls zum Untergang der Titanic. Eine Frage, die sich hier aufdrängt, lautet: Kann (konnte) man sich vom Tod freikaufen, etwas polemisch formuliert. Oder neutraler: Hängt die Überlebensquote von der Klasse, in der derPassagiers reist, ab?
Diese Übung soll einige grundlegende Vorgehensweise der Datenanalyse verdeutlichen; Zielgruppe sind Einsteiger (mit Grundkenntnissen in R) in die Datenanalyse.</description>
    </item>
    
    <item>
      <title>Some tricks on dplyr::filter</title>
      <link>/2016/12/21/dplyr_filter/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/21/dplyr_filter/</guid>
      <description>The R package dplyr has some attractive features; some say, this packkage revolutionized their workflow. At any rate, I like it a lot, and I think it is very helpful.
In this post, I would like to share some useful (I hope) ideas (&amp;ldquo;tricks&amp;rdquo;) on filter, one function of dplyr. This function does what the name suggests: it filters rows (ie., observations such as persons). The addressed rows will be kept; the rest of the rows will be dropped.</description>
    </item>
    
    <item>
      <title>Pipe the Variance</title>
      <link>/2016/11/30/pipe_variance/</link>
      <pubDate>Wed, 30 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/30/pipe_variance/</guid>
      <description>One idea of problem solving is, or should be, I think, that one should tackle problems of high complexity, but not too high. That sounds trivial, cooler tone would be &amp;ldquo;as hard as possible, as easy as necessary&amp;rdquo; which is basically the same thing.
In software development including Rstats, a similar principle applies. Sounds theoretical, I admit. So see here some lines of code that has bitten me recently:
obs &amp;lt;- c(1,2,3) pred &amp;lt;- c(1,2,4) monster &amp;lt;- 1 - (sum((obs - pred)^2))/(sum((obs - mean(obs))^2)) monster  ## [1] 0.</description>
    </item>
    
    <item>
      <title>Some musings on the validation of Satow&#39;s Extraversion questionnaire</title>
      <link>/2016/11/23/validation_extraversion_questionnaire/</link>
      <pubDate>Wed, 23 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/23/validation_extraversion_questionnaire/</guid>
      <description>Measuring personality traits is one of (the?) bread-and-butter business of psychologists, at least for quantitatively oriented ones. Literally, thousand of psychometric questionnaires exits. Measures abound. Extroversion, part of the Big Five personality theory approach, is one of the most widely used, and extensively scrutinized questionnaire tapping into human personality.
One rather new, but quite often used questionnaire, is Satow&amp;rsquo;s (2012) B5T. The reason for the popularity of this instrument is that it runs under a CC-licence - in contrast to the old ducks, which coute chere.</description>
    </item>
    
    <item>
      <title>Preparing survey results data</title>
      <link>/2016/11/19/preparing_survey_data/</link>
      <pubDate>Sat, 19 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/19/preparing_survey_data/</guid>
      <description>Analyzing survey results is a frequent endeavor (for some including me). Let&amp;rsquo;s not think about arguments whether and when surveys are useful or not (for some recent criticism see Briggs&amp;rsquo; book).
Typically, respondents circle some option ranging from &amp;ldquo;don&amp;rsquo;t agree at all&amp;rdquo; to &amp;ldquo;completely agree&amp;rdquo; for each question (or &amp;ldquo;item&amp;rdquo;). Typically, four to six boxes are given where one is expected to tick one.
In this tutorial, I will discuss some typical steps to prepare the data for subsequent analyses.</description>
    </item>
    
    <item>
      <title>Crashkurs zur Erstellung von Barplots für Umfrage-Daten</title>
      <link>/2016/11/13/crashkurs_barplots/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/crashkurs_barplots/</guid>
      <description>Eine recht häufige Art von Daten in der Wirtschaft kommen von Umfragen in der Belegschaft. Diese Daten gilt es dann aufzubereiten und graphisch wiederzugeben. Dafür gibt dieser Post einige grundlegende Hinweise. Grundwissen mit R setzen wir voraus :-)
Eine ausführlichere Beschreibung hier sich z.B. hier.
Packages laden Nicht vergessen: Ein Computerprogramm (z.B. ein R-Package) kann man nur dann laden, wenn man es vorher installier hat (aber es reicht, das Programm/R-Package einmal zu installieren).</description>
    </item>
    
    <item>
      <title>New bar stacking with ggplot 2.2.0</title>
      <link>/2016/11/13/improved_bar_stacking_ggplot2_220/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/improved_bar_stacking_ggplot2_220/</guid>
      <description>Recently, ggplot2 2.2.0 was released. Among other news, stacking bar plot was improved. Here is a short demonstration.
Load libraries
library(tidyverse) library(htmlTable)  &amp;hellip; and load data:
data &amp;lt;- read.csv(&amp;quot;https://osf.io/meyhp/?action=download&amp;quot;)  DOI for this piece of data is 10.17605/OSF.IO/4KGZH.
The data consists of results of a survey on extraversion and associated behavior.
Say, we would like to visualize the responsed to the extraversion items (there are 10 of them).
So, let&amp;rsquo;s see.</description>
    </item>
    
    <item>
      <title>Some thoughts (and simulation) on overfitting</title>
      <link>/2016/11/13/overfitting_simulation/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/overfitting_simulation/</guid>
      <description>Overfitting is a common problem in data analysis. Some go as far as saying that &amp;ldquo;most of&amp;rdquo; published research is false (John Ionnadis); overfitting being one, maybe central, problem of it. In this post, we explore some aspects on the notion of overfitting.
Assume we have 10 metric variables v (personality/health/behavior/gene indicator variables), and, say, 10 variables for splitting up subgroups (aged vs. young, female vs. male, etc.), so 10 dichotomic variables.</description>
    </item>
    
    <item>
      <title>Plotting survey results using `ggplot2`</title>
      <link>/2016/11/12/plotting_surveys/</link>
      <pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/12/plotting_surveys/</guid>
      <description>Plotting (and more generally, analyzing) survey results is a frequent endeavor in many business environments. Let&amp;rsquo;s not think about arguments whether and when surveys are useful (for some recent criticism see Briggs&amp;rsquo; book).
Typically, respondents circle some option ranging from &amp;ldquo;don&amp;rsquo;t agree at all&amp;rdquo; to &amp;ldquo;completely agree&amp;rdquo; for each question (or &amp;ldquo;item&amp;rdquo;). Typically, four to six boxes are given where one is expected to tick one.
In this tutorial, I will discuss some barplot type visualizations; the presentation is based on ggplot2 (within the R environment) .</description>
    </item>
    
    <item>
      <title>Horoskopstudie zum Barnumeffekt</title>
      <link>/2016/11/09/horoskop-studie/</link>
      <pubDate>Wed, 09 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/09/horoskop-studie/</guid>
      <description>Viele Menschen glauben an Horoskope. Doch warum? Ein Grund könnte sein, dass Horoskope einfach gut sind. Was heißt gut: Sie passen auf mich aber nicht auf andere Leute (mit anderen Strernzeichen) und sie sagen Dinge, die nützlich sind.
Ein anderer Grund könnte sein, dass sie uns schmeicheln und Gemeinplätze sind, denen jeder zustimmt: &amp;ldquo;Sie sind an sich ein Super-Typ, aber manchmal etwas ungeduldig&amp;rdquo; (oh ja, absolut, passt genau!). &amp;ldquo;Heute treffen Sie jemanden, der eine große Liebe werden könnte&amp;rdquo; (Hört sich gut an!</description>
    </item>
    
    <item>
      <title>Bind lists to data frame for aggregating linear models results</title>
      <link>/2016/11/04/bind_list_to_dataframe_lm/</link>
      <pubDate>Fri, 04 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/04/bind_list_to_dataframe_lm/</guid>
      <description>I found myself doing the following: I had a bunch of predictors, one (numeric) outcome, and wanted to run I simple regression for each of the predictors. Having a bunch of model results, I would like to have them bundled in one data frame.
So, here is one way to do it.
First, load some data.
data(mtcars) str(mtcars)  ## &#39;data.frame&#39;:	32 obs. of 11 variables: ## $ mpg : num 21 21 22.</description>
    </item>
    
    <item>
      <title>How to plot a &#39;percentage plot&#39; with ggplot2</title>
      <link>/2016/11/03/percentage_plot_ggplot2_v2/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/03/percentage_plot_ggplot2_v2/</guid>
      <description>At times it is convenient to draw a frequency bar plot; at times we prefer not the bare frequencies but the proportions or the percentages per category. There are lots of ways doing so; let&amp;rsquo;s look at some ggplot2 ways.
First, let&amp;rsquo;s load some data.
data(tips, package = &amp;quot;reshape2&amp;quot;)  And the typical libraries.
library(dplyr) library(ggplot2) library(tidyr) library(scales) # for percentage scales  Way 1 tips %&amp;gt;% count(day) %&amp;gt;% mutate(perc = n / nrow(tips)) -&amp;gt; tips2 ggplot(tips2, aes(x = day, y = perc)) + geom_bar(stat = &amp;quot;identity&amp;quot;)  Way 2 ggplot(tips, aes(x = day)) + geom_bar(aes(y = (.</description>
    </item>
    
    <item>
      <title>CLES plot</title>
      <link>/2016/10/17/cles-plot/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/17/cles-plot/</guid>
      <description>In data analysis, we often ask &amp;ldquo;Do these two groups differ in the outcome variable&amp;rdquo;? Asking this question, a tacit assumption may be that the grouping variable is the cause of the difference in the outcome variable. For example, assume the two groups are &amp;ldquo;treatment group&amp;rdquo; and &amp;ldquo;control group&amp;rdquo;, and the outcome variable is &amp;ldquo;pain reduction&amp;rdquo;.
A typical approach would be to report the strenght of the difference by help of Cohen&amp;rsquo;s d.</description>
    </item>
    
    <item>
      <title>Checking for NA with dplyr</title>
      <link>/2016/10/16/nas-with-dplyr/</link>
      <pubDate>Sun, 16 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/16/nas-with-dplyr/</guid>
      <description>Often, we want to check for missing values (NAs). There are of course many ways to do so. dplyr provides a quite nice one.
First, let&amp;rsquo;s load some data:
library(readr) extra_file &amp;lt;- &amp;quot;https://raw.github.com/sebastiansauer/Daten_Unterricht/master/extra.csv&amp;quot; extra_df &amp;lt;- read_csv(extra_file)  Note that extra is a data frame consisting of survey items regarding extraversion and related behavior.
In case the dataframe is quite largish (many columns) it is helpful to have some quick way. Here, we have 25 columns.</description>
    </item>
    
    <item>
      <title>Multiple ways to subsetting data frames in R</title>
      <link>/2016/10/15/indexing-in-r/</link>
      <pubDate>Sat, 15 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/15/indexing-in-r/</guid>
      <description>Subsetting a data frame is an essential and frequently performed task. Here, some basic ideas are presented.
Get some data first.
str(mtcars)  ## &#39;data.frame&#39;:	32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 .</description>
    </item>
    
    <item>
      <title>How to read Github files into R easily</title>
      <link>/2016/10/12/download-from-github/</link>
      <pubDate>Wed, 12 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/12/download-from-github/</guid>
      <description>Downloading a folder (repository) from Github as a whole The most direct way to get data from Github to your computer/ into R, is to download the repository. That is, click the big green button:
The big, green button saying &amp;ldquo;Clone or download&amp;rdquo;, click it and choose &amp;ldquo;download zip&amp;rdquo;.
Of course, for those using Git and Github, it would be appropriate to clone the repository. And, although appearing more advanced, cloning has the definitive advantage that you&amp;rsquo;ll enjoy the whole of the Github features.</description>
    </item>
    
    <item>
      <title>Using purrr to build a data frame of vectors (eg., from effect size statistics)</title>
      <link>/2016/09/29/purrr-effsize/</link>
      <pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/29/purrr-effsize/</guid>
      <description>I just tried to accomplish the following with R: Compute effect sizes for a variable between two groups. Actually, not one numeric variable but many. And compute not only one measure of effect size but several (d, lower/upper CI, CLES,&amp;hellip;).
So how to do that?
First, let&amp;rsquo;s load some data and some (tidyverse and effect size) packages:
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, message = FALSE)  library(purrr) library(ggplot2) library(dplyr) library(broom) library(tibble) library(compute.</description>
    </item>
    
    <item>
      <title>Summary for multiple variables using purrr</title>
      <link>/2016/09/28/summary-mult-cols-purrr/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/28/summary-mult-cols-purrr/</guid>
      <description>A frequent task in data analysis is to get a summary of a bunch of variables. Often, graphical summaries (diagrams) are wanted. However, at times numerical summaries are in order. How to get that in R? That&amp;rsquo;s the question of the present post.
Of course, there are several ways. One way, using purrr, is the following. I liked it quite a bit that&amp;rsquo;s why I am showing it here.
First, let&amp;rsquo;s load some data and some packages we will make use of.</description>
    </item>
    
    <item>
      <title>EDIT: Running multiple simple regressions with purrr</title>
      <link>/2016/09/26/edit-multiple_lm_purrr_edit/</link>
      <pubDate>Mon, 26 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/26/edit-multiple_lm_purrr_edit/</guid>
      <description>EDIT based on comments/ suggeestions from @JonoCarroll Disqus profile and @tjmahr twitter profile. See below (last step; look for &amp;ldquo;EDIT&amp;rdquo;).
Thanks for the input! 👍
reading time: 10 min.
Hadley Wickham&amp;rsquo;s purrr has given a new look at handling data structures to the typical R user (some reasoning suggests that average users doesn&amp;rsquo;t exist, but that&amp;rsquo;s a different story).
I just tried the following with purrr: - Meditate about the running a simple regression, FWIW - Take a dataframe with candidate predictors and an outcome - Throw one predictor at a time into the regression, where the outcome variable remains the same (i.</description>
    </item>
    
    <item>
      <title>Running multiple simple regressions with purrr</title>
      <link>/2016/09/23/multiple-lm-purrr2/</link>
      <pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/23/multiple-lm-purrr2/</guid>
      <description>Hadley Wickham&amp;rsquo;s purrr has given a new look at handling data structures to the typical R user (some reasoning suggests that average users don&amp;rsquo;t exist, but that&amp;rsquo;s a different story).
I just tried the following with purrr:
 Meditate about the running a simple regression, FWIW Take a dataframe with candidate predictors and an outcome Throw one predictor at a time into the regression, where the outcome variable remains the same (i.</description>
    </item>
    
    <item>
      <title>Code example for plotting boxplots instead of mean bars</title>
      <link>/2016/09/22/use-boxplots/</link>
      <pubDate>Thu, 22 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/22/use-boxplots/</guid>
      <description>On a recent psychology conference I had the impression that psychologists keep preferring to show mean values, but appear less interested in more detailled plots such as the boxplot. Plots like the boxplot are richer in information, but not more difficult to perceive.
For those who would like to have an easy starter on how to visualize more informative plots (more than mean bars), here is a suggestion:
# install.pacakges(&amp;quot;Ecdat&amp;quot;) library(Ecdat) # dataset on extramarital affairs data(Fair) str(Fair)  ## &#39;data.</description>
    </item>
    
    <item>
      <title>Fallstudie zur explorative Datenanalyse (YACSDA) beim Datensatz &#39;TopGear&#39;</title>
      <link>/2016/09/14/yacsda_topgear/</link>
      <pubDate>Wed, 14 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/14/yacsda_topgear/</guid>
      <description>YADCSDA in German language.
In dieser Fallstudie (YACSDA: Yet another case study of data analysis) wird der Datensatz TopGear analysiert, vor allem mit grafischen Mitteln. Es handelt sich weniger um einen &amp;ldquo;Rundumschlag&amp;rdquo; zur Beantwortung aller möglichen interessanten Fragen (oder zur Demonstration aller möglichen Analysewerkzeuge), sondern eher um einen Einblick zu einfachen explorativen Verfahren.
library(robustHD)  ## Loading required package: perry  ## Loading required package: parallel  ## Loading required package: robustbase  data(TopGear) # Daten aus Package laden library(tidyverse)  Numerischer Überblick glimpse(TopGear)  ## Observations: 297 ## Variables: 32 ## $ Maker &amp;lt;fctr&amp;gt; Alfa Romeo, Alfa Romeo, Aston Martin, Asto.</description>
    </item>
    
    <item>
      <title>Plot of mean with exact numbers using ggplot2</title>
      <link>/2016/08/30/plot_dot_means/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/30/plot_dot_means/</guid>
      <description>Often, both in academic research and more business-driven data analysis, we want to compare some (two in many cases) means. We will not discuss here that friends should not let friends plot barplots. Following the advise of Cleveland&amp;rsquo;s seminal book we will plot the means using dots, not bars.
However, at times we do not simply want the diagram, but we (or someone) is interested in the bare, plain, naked, exact numbers too.</description>
    </item>
    
    <item>
      <title>Shading multiple areas under normal curve</title>
      <link>/2016/08/30/shade_normal_curve/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/30/shade_normal_curve/</guid>
      <description>When plotting a normal curve, it is often helpful to color (or shade) some segments. For example, often we might want to indicate whether an absolute value is greater than 2.
How can we achieve this with ggplot2? Here is one way.
First, load packages and define some constants. Specifically, we define mean, sd, and start/end (z-) value of the area we want to shade. And your favorite color is defined.</description>
    </item>
    
    <item>
      <title>Simple way to plot a normal distribution with ggplot2</title>
      <link>/2016/08/30/normal_curve_ggplot2/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/30/normal_curve_ggplot2/</guid>
      <description>Plotting a normal distribution is something needed in a variety of situation: Explaining to students (or professors) the basic of statistics; convincing your clients that a t-Test is (not) the right approach to the problem, or pondering on the vicissitudes of life&amp;hellip;
If you like ggplot2, you may have wondered what the easiest way is to plot a normal curve with ggplot2?
Here is one:
library(cowplot)  ## Loading required package: ggplot2  ## ## Attaching package: &#39;cowplot&#39;  ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave  p1 &amp;lt;- ggplot(data = data.</description>
    </item>
    
    <item>
      <title>Multiple t-Tests with dplyr</title>
      <link>/2016/08/18/multiple-t-tests-with-dplyr/</link>
      <pubDate>Thu, 18 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/18/multiple-t-tests-with-dplyr/</guid>
      <description>t-Test on multiple columns Suppose you have a data set where you want to perform a t-Test on multiple columns with some grouping variable. As an example, say you a data frame where each column depicts the score on some test (1st, 2nd, 3rd assignment&amp;hellip;). In each row is a different student. So you glance at the grading list (OMG!) of a teacher!
How to do do that in R?</description>
    </item>
    
    <item>
      <title>Looping through dataframe columns using purrr::map()</title>
      <link>/2016/08/16/looping-purrr/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/16/looping-purrr/</guid>
      <description>Let&amp;rsquo;s get purrr. Recently, I ran across this issue: A data frame with many columns; I wanted to select all numeric columns and submit them to a t-test with some grouping variables.
As this is a quite common task, and the purrr-approach (package purrr by @HadleyWickham) is quite elegant, I present the approach in this post.
Let&amp;rsquo;s load the data, the Affairs data set, and some packages:
data(Affairs, package = &amp;quot;AER&amp;quot;) library(purrr) # functional programming library(dplyr) # dataframe wrangling library(ggplot2) # plotting library(tidyr) # reshaping df  Don&amp;rsquo;t forget that the four packages need to be installed in the first place.</description>
    </item>
    
    <item>
      <title>Practical data cleansing in R</title>
      <link>/2016/07/24/data-cleansing/</link>
      <pubDate>Sun, 24 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/24/data-cleansing/</guid>
      <description>What is “data cleansing” about?
Data analysis, in practice, consists typically of some different steps which can be subsumed as “preparing data” and “model data” (not considering communication here):
(Inspired by this)
Often, the first major part — “prepare” — is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!). In practice, one rather has to get his (her) hands dirt…</description>
    </item>
    
    <item>
      <title>Yet another case study on data analysis (YACSDA) – extramarital affairs data set</title>
      <link>/2016/07/23/affairs/</link>
      <pubDate>Sat, 23 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/23/affairs/</guid>
      <description>Ok, there are heaps of them on the net. Here comes my YACSDA. Maybe the only thing about it to mention is that it comes in German language.
 Analytical language: R (3.3) Purpose: Demonstrate basic exploratory and modeling techniques Packages used: dplyr, ggplot2 Data set: Affair; source R package COUNT Analytical topics covered: descriptive statistics, visualization, liner model, logistic linear model Reproducibility: Rmarkdown, knitr, github  Code on Github</description>
    </item>
    
    <item>
      <title>Case study on data wrangling with dplyr (German)</title>
      <link>/2016/07/18/nycflights13/</link>
      <pubDate>Mon, 18 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/18/nycflights13/</guid>
      <description>reading time (full): 30 min.
Data Wrangling with dplyr is a popular activity in data science/ statistics. A number of tutorial are available, but not so many in German language.
Data set analyzed in nycflights13::flights (R package). Available on CRAN. Ok, choosing this data set is not very creative, but, hey, quite nice data:)
Thus, here is a case study in German language; code &amp;reg;is on Github.</description>
    </item>
    
    <item>
      <title>Long vs. wide format, and gather()</title>
      <link>/2016/07/04/gather-long-to-wide-format/</link>
      <pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/04/gather-long-to-wide-format/</guid>
      <description>reading time: 10 min.
A quite common task in data analysis is to change a dataset from wide to long format.
For example, this is a dataset in wide format:
Is is called wide, as, well, it is wide – several columns side by side.
For example, assume, we have measured a number of predictors (here: predictor_1, predictor_2, predictor_3), and an outcome measure (here: outcome). In this case, each variable is dichotomous (either yes or no).</description>
    </item>
    
    <item>
      <title>Cross-tabulate multiple variables</title>
      <link>/2016/07/03/cross-tabulate-multiple-variables/</link>
      <pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/03/cross-tabulate-multiple-variables/</guid>
      <description>reading time: 15-20 min.
Recently, I analyzed some data of a study where the efficacy of online psychotherapy was investigated. The investigator had assessed whether or not a participant suffered from some comorbidities (such as depression, anxiety, eating disorder…).
I wanted to know whether each of these (10 or so) comorbidities was associated with the outcome (treatment success, yes vs. no).
Of course, an easy solution would be to “half-manually” check the association, eg.</description>
    </item>
    
  </channel>
</rss>