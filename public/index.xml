<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Se</title>
    <link>/</link>
    <description>Recent content on Data Se</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Beispiel zu Simpsons Paradox</title>
      <link>/2018/03/16/beispiel-zu-simpsons-paradox/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/16/beispiel-zu-simpsons-paradox/</guid>
      <description>In diesem Post diskutieren wir ein Beispiel zu Simpson‚Äôs Paradox. Der Fokus liegt nicht auf der R-Syntax, sondern auf einer intuitiven Erl√§uterung des Simpson Paradox. (Die Syntax findet sich in √§hnlicher Form in diesem Post.)
Sagen wir, Sie m√ºssen sich zwischen zwei √Ñrzten (Dr. Arriba und Dr. Bajo) entscheiden und fragen sich, welcher ‚Äúbesser‚Äù ist. Unter ‚Äúbesser‚Äù verstehen Sie ‚Äúh√∂here Heilungsquote‚Äù.
Die beiden √Ñrzte behandeln die gleichen zwei Krankheiten: Severitis und Nervosia maskulina.</description>
    </item>
    
    <item>
      <title>Tangible data of normal distributed data</title>
      <link>/2018/03/16/tangible-data-of-normal-distributed-data/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/16/tangible-data-of-normal-distributed-data/</guid>
      <description>A classical example for a normally distributed variable is height. However, I kept on looking for data as to the mean and sd for some populations, such as Germany. How I found some reliably looking data here.
We will not question whether the assumption of normality holds, we just assume it.
In the source, we can read that in Germany, the adult men population has the following parameters:
mean: 174cm sd: 7cm</description>
    </item>
    
    <item>
      <title>Map students to presentation slots</title>
      <link>/2018/03/11/map-students-to-presentation-slots/</link>
      <pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/11/map-students-to-presentation-slots/</guid>
      <description>As a teacher, I not only teach but also assess the achievements of students. One example of a typical student assignments is a presentation. You know, powerpoint slides and stuff.
For that purpose, I often need to map students to one of several time slots. Here‚Äôs the R code I use for that purpose.
library(tidyverse) How many students are subscribed to the assignment?
stud_count &amp;lt;- 20 Let‚Äôs say there 20 students in the course.</description>
    </item>
    
    <item>
      <title>Intuition to Simpson&#39;s paradox</title>
      <link>/2018/03/09/intuition-to-simpson-s-paradox/</link>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/09/intuition-to-simpson-s-paradox/</guid>
      <description>Say, you have to choose between two doctors (Anna and Berta). To decide which one is better, you check their success rates. Suppose that they deal with two conditions (Coolities and Dummities). So let‚Äôs compare their success rate for each of the two conditions (and the total success rate):
This is the proportion of healing (success) of the first doctor, Dr. Anna for each of the two conditions:
 Coolities: 7 out of 8 patients are healed from Coolities Dummieties: 1 out of 2 patients are healed from Dummities  This is the proportion of healing (success) of the first doctor, Dr.</description>
    </item>
    
    <item>
      <title>How to create columns in a dataframe in R</title>
      <link>/2018/03/07/how-to-create-columns-in-a-dataframe-in-r/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/07/how-to-create-columns-in-a-dataframe-in-r/</guid>
      <description>Note that we will use this library for this post:
library(dplyr) By the way, loading mosaic, will load dplyr too.
One of the major data wrangling activities (in R and elsewhere) is to create a new column in a data frame. For example, assume you have some students who have completed some exercises. In each row of the dataframe - one student. In each column - one exercise (called item). The dataframe might then look like this:</description>
    </item>
    
    <item>
      <title>Papers publizieren. Versuch einer Anleitung</title>
      <link>/2018/01/25/papers-publizieren-versuch-einer-anleitung/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/25/papers-publizieren-versuch-einer-anleitung/</guid>
      <description>Unter https://sebastiansauer.github.io/Talks-ses/pubws.html#/ finden sich die HTML-Folien zu einem Talk von mir zum Thema, wie man Papers publiziert (oder es zumindest versucht).
Der Quelltext findet sich in diesem Github-Repo.
Der Talk steht unter der CC-BY-Lizenz.</description>
    </item>
    
    <item>
      <title>Simulate p-hacking - adding observations</title>
      <link>/2018/01/24/simulate-p-hacking-adding-observations/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/24/simulate-p-hacking-adding-observations/</guid>
      <description>Let‚Äôs simulate p-values as a funtion of sample size. We assume that some researcher collects one data point, computes the p-value, and repeats until p-value falls below some arbitrary threshold. Oh and yes, there is no real effect. For the sake of spending the budget, assume that our researcher collects a sample size of \(n=100\).
This idea stems from this great article False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant; cf.</description>
    </item>
    
    <item>
      <title>Visualizing a logistic regression the easy way</title>
      <link>/2018/01/23/visualizing-a-logistic-regression-the-easy-way/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/23/visualizing-a-logistic-regression-the-easy-way/</guid>
      <description>Let‚Äôs visualize a GLM (logistic regression).
First laod some data:
data(tips, package = &amp;quot;reshape2&amp;quot;) Compute a glm:
glm_tips &amp;lt;- glm(sex ~ tip, data = tips, family = &amp;quot;binomial&amp;quot;) Plot the model using mosaic:
library(mosaic) plotModel(glm_tips) The curve does not look really s-typed (ogive) but that‚Äôs ok because the data suggest not a strong trend. The plot is not very beautiful either, but hey - it‚Äôs quick to produce üòÅ.</description>
    </item>
    
    <item>
      <title>Zusammenhang von Lernen und Noten im Statistikunterricht</title>
      <link>/2017/12/20/zusammenhang-von-lernen-und-noten-im-statistikunterricht/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/20/zusammenhang-von-lernen-und-noten-im-statistikunterricht/</guid>
      <description>F√ºhrt Lernen zu besseren Noten? Eigene Erfahrung und allgemeiner Konsens stimmen dem zu; zumindest schadet Lernen des Stoffes nicht und hilft oft, gute Noten bei einer Pr√ºfung zu diesem Stoff zu erzielen. Aber welche Belege, wissenschaftliche Belege gibt es dazu? An unserer Hochschule, die FOM, haben wir eine kleine Untersuchung zu dieser Frage durchgef√ºhrt. Genauer gesagt haben wir unseren Studierenden einen Statistik-Test vorlegt und gefagt, wie sehr sie sich f√ºr diesen Test vorbereitet h√§tten.</description>
    </item>
    
    <item>
      <title>A p-value picture</title>
      <link>/2017/11/29/a-p-value-picture/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/29/a-p-value-picture/</guid>
      <description>Much ado and to say about the p-value. Let me add one more point; actually not really from myself, but from Diez, Barr, and Cetinkaya-Rundel (2012), p.¬†189; good book in one is looking for ‚Äúorthodox‚Äù statistics.
library(tidyverse) ggplot(NULL, aes(c(-5,5))) + geom_area(stat = &amp;quot;function&amp;quot;, fun = dnorm, fill = &amp;quot;grey40&amp;quot;, xlim = c(-5, 2)) + geom_area(stat = &amp;quot;function&amp;quot;, fun = dnorm, fill = &amp;quot;#00998a&amp;quot;, xlim = c(2, 5)) + labs(y = &amp;quot;&amp;quot;, x = &amp;quot;X&amp;quot;) + theme(axis.</description>
    </item>
    
    <item>
      <title>Data sets suitable for business case studies</title>
      <link>/2017/11/29/data-sets-suitable-for-business-case-studies/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/29/data-sets-suitable-for-business-case-studies/</guid>
      <description>Data sets are a useful didactical tool for demontrating procedure of data analysis. For business audiences, business type data sets propose themselves. This post curates a list of data sets I find useful for this purpose.
 Boston Housing  R package: MASS Data access case study  NYC flights data  R package: nyflights13 Data access case study  Wine Quality  R package: ??? Data access - Red wine case study  tiping data  R package: reshape2 Data access  Diamonds  R package: ggplot2 case study   Curated lists of data sets  vincentarelbundock    </description>
    </item>
    
    <item>
      <title>Grundlagen des Textminings mit R</title>
      <link>/2017/11/28/textmining-grundlagen/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/textmining-grundlagen/</guid>
      <description>Lernziele:
  - Sie kennen zentrale Ziele und Begriffe des Textminings. - Sie wissen, was ein &amp;#39;tidy text dataframe&amp;#39; ist. - Sie k√∂nnen Worth√§ufigkeiten ausz√§hlen. - Sie k√∂nnen Worth√§ufigkeiten anhand einer Wordcloud visualisieren. In dieser √úbung ben√∂tigte R-Pakete:
library(tidyverse) # Datenjudo library(stringr) # Textverarbeitung library(tidytext) # Textmining library(lsa) # Stopw√∂rter library(SnowballC) # W√∂rter trunkieren library(wordcloud) # Wordcloud anzeigen  Bitte installieren Sie rechtzeitig alle Pakete, z.B. in RStudio √ºber den Reiter Packages ‚Ä¶ Install.</description>
    </item>
    
    <item>
      <title>Grundlagen des Textminings mit R - Teil 2</title>
      <link>/2017/11/28/grundlagen-des-textminings-mit-r-teil-2/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/grundlagen-des-textminings-mit-r-teil-2/</guid>
      <description>In dieser √úbung ben√∂tigte R-Pakete:
library(tidyverse) # Datenjudo library(stringr) # Textverarbeitung library(tidytext) # Textmining library(lsa) # Stopw√∂rter library(SnowballC) # W√∂rter trunkieren library(wordcloud) # Wordcloud anzeigen library(skimr) # √úberblicksstatistiken  Bitte installieren Sie rechtzeitig alle Pakete, z.B. in RStudio √ºber den Reiter Packages ‚Ä¶ Install.
 Aus dem letzten Post Daten einlesen:
osf_link &amp;lt;- paste0(&amp;quot;https://osf.io/b35r7//?action=download&amp;quot;) afd &amp;lt;- read_csv(osf_link) Aus breit mach lang:
afd %&amp;gt;% unnest_tokens(output = token, input = content) %&amp;gt;% dplyr::filter(str_detect(token, &amp;quot;[a-z]&amp;quot;)) -&amp;gt; afd_long Stopw√∂rter entfernen:</description>
    </item>
    
    <item>
      <title>Image path for blogdown</title>
      <link>/2017/11/28/image-path-for-blogdown/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/image-path-for-blogdown/</guid>
      <description>How to include external images to a hugo post?
Suppose we have a file img1.png in project1, ie., project1/img1.png. Do this:
Copy your folder with images to static/. Use this path in your blogdown post: /project/img1.png.   Mind the leading slash!  Example time This code (on my machine) ![](/images/textmining/tidytext-crop.png){ width=&amp;quot;20%&amp;quot; }
renders this:
Note the nice width option.
 Knitr way The knitr way works similarly:
knitr::include_graphics(&amp;quot;/images/textmining/tidytext-crop.png&amp;quot;)  </description>
    </item>
    
    <item>
      <title>test</title>
      <link>/2017/11/28/test/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/test/</guid>
      <description>knitr way
knitr::include_graphics(&amp;quot;/images/textmining/tidytext-crop.png&amp;quot;) </description>
    </item>
    
    <item>
      <title>Dummy variables and regression</title>
      <link>/2017/11/27/dummy-variables-and-regression/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/27/dummy-variables-and-regression/</guid>
      <description>For modeling cause-effect relationships, linear regression is among the most typically used methods.
Take, for example, the idea that the Gross Domestic Product (GDP) drives religiosity. Of course, we should have a strong theory that defends this choice and this directionality. Without a convincing theory it may be argued that the cause-relationship is the other way round or complete different (ie., some third variable accounts for any association between GDP and religiosity).</description>
    </item>
    
    <item>
      <title>Interactive diagrams in lieu of shiny?</title>
      <link>/2017/11/27/interactive-diagrams-in-lieu-of-shiny/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/27/interactive-diagrams-in-lieu-of-shiny/</guid>
      <description>One frequent use of the Shiny server software is displaying interactive data diagrams. The pro of using Shiny is the great flexibility; much more than ‚Äújust graphics‚Äù can be done.</description>
    </item>
    
    <item>
      <title>My favorite stats text book</title>
      <link>/2017/11/27/my-favorite-stats-text-book/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/27/my-favorite-stats-text-book/</guid>
      <description>Some thoughts how my favorite applied stats text book would look like. I am looking at eg., business fields such as MBA as consumers.
My ideal applied stats text book is case study oriented (‚ÄúAssume you would like to predict which movie will score highest next year based on some movie characteristics you know‚Äù)
 makes use of recent data analytics techniques such as tree based methods (Random Forests) or Shrinkage models (Lasso)</description>
    </item>
    
    <item>
      <title>Use case for purrr::map</title>
      <link>/2017/11/23/use-case-for-purrr-map/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/23/use-case-for-purrr-map/</guid>
      <description>library(tidyverse) d &amp;lt;- data_frame( id = c(1,1,1,1,1,1,2,2,3,3,3,4,1,2,2) ) d$id %&amp;gt;% map </description>
    </item>
    
    <item>
      <title>Compute effect sizes with R. A primer.</title>
      <link>/2017/11/21/compute-effect-sizes-with-r-a-primer/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/compute-effect-sizes-with-r-a-primer/</guid>
      <description>A typical ‚Äúcook book recipe‚Äù for doing data analysis is an applied stats course is:
report descriptive statistics plot some nice diagrams test hypothesis report effect sizes  Let‚Äôs have a quick glance at these steps. We will use the dataset flights of the package nycflights13.
data(flights, package = &amp;quot;nycflights13&amp;quot;) This post will be tidyverse-driven.
library(tidyverse) library(skimr) library(mosaic) Let‚Äôs compute some summaries:
flights %&amp;gt;% select(arr_delay) %&amp;gt;% skim #&amp;gt; Skim summary statistics #&amp;gt; n obs: 336776 #&amp;gt; n variables: 1 #&amp;gt; #&amp;gt; Variable type: numeric #&amp;gt; variable missing complete n mean sd p0 p25 median p75 p100 #&amp;gt; arr_delay 9430 327346 336776 6.</description>
    </item>
    
    <item>
      <title>Get your stats result in a table easily</title>
      <link>/2017/11/21/get-your-stats-result-in-a-table-easily/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/get-your-stats-result-in-a-table-easily/</guid>
      <description>Having computed some staticis, one would like to display them. Either in a figure, on in a table, that‚Äôs the two typical ways.
Let‚Äôs explore some helper functions to get your stats to a table easily.
A nice overview on packages can be found here.
Let‚Äôs have a quick glance at these steps. We will use the dataset flights of the package nycflights13.
data(flights, package = &amp;quot;nycflights13&amp;quot;) This post will be tidyverse-driven:</description>
    </item>
    
    <item>
      <title>Hello World, this is Blogdown</title>
      <link>/2017/11/21/hello-world-this-is-blogdown/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/hello-world-this-is-blogdown/</guid>
      <description>My blog at https://sebastiansauer.github.io/posts/ has moved. It is now here! This is the new home of my blog. In (the unlikely) case you are asking yourself ‚ÄúWhy did you move your blog?‚Äù, here is the answer.
I was using Jekyll at Github pages which is great as long as you do not have a lot of R in your posts. But I did have a lot of R in my posts.</description>
    </item>
    
    <item>
      <title>Pass multiple functions and arguments to purrr::map</title>
      <link>/2017/11/21/pass-multiple-functions-and-arguments-to-purrr-map/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/21/pass-multiple-functions-and-arguments-to-purrr-map/</guid>
      <description>I just run in the following problem: I wanted to map multiple functions to multiple columns, and I needed to pass some arguments to this map call. Sound theoretical, I know. Consider this example:
library(tidyverse) Build a vector of functions:
funs &amp;lt;- purrr::compose(median, mean, sd, IQR) And apply this composed function to each column of a dataframe:
mtcars[1:2]%&amp;gt;% map(funs, na.rm = TRUE) #&amp;gt; $mpg #&amp;gt; [1] NA #&amp;gt; #&amp;gt; $cyl #&amp;gt; [1] NA mtcars %&amp;gt;% map(funs, na.</description>
    </item>
    
    <item>
      <title>Great dataviz examples in rstats</title>
      <link>/2017/11/20/great-dataviz-examples-in-rstats/</link>
      <pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/20/great-dataviz-examples-in-rstats/</guid>
      <description>Here come some stunning examples of data visualizations, all built with R. R code of each diagram is available at the source. Enjoy! #beautiful.
UPDATE: I&amp;rsquo;ve included links to the R source!
Plotting geo maps along with subplots in ggplot2 I like this one by Ilya Kashnitsky:
Similarly, by the same author:
Source
Great work, @ikashnitsky!
Cirlize (Chord) diagrams Plotting association in a circular form yields aesthetic examples of diagrams, see the following examples</description>
    </item>
    
    <item>
      <title> Wie gut sch√§tzt eine Stichprobe die Grundgesamtheit?</title>
      <link>/2017/11/17/inference/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/17/inference/</guid>
      <description>Daten Sie arbeiten bei der Flughafenaufsicht von NYC. Cooler Job.
library(nycflights13) data(flights)  Pakete laden library(mosaic)  Stichprobe ziehen Die Aufsichtsbeh√∂rde zieht eine Probe von 100 Fl√ºgen und ermittelt die &amp;ldquo;typische&amp;rdquo; Versp√§tung.
set.seed(42) sample(flights$arr_delay, size = 100) -&amp;gt; flights_sample  Und berechnen wir die typischen Kennwerte:
favstats(~flights_sample, na.rm = TRUE) #&amp;gt; min Q1 median Q3 max mean sd n missing #&amp;gt; -51 -18.75 -5 11.75 150 0.4387755 31.1604 98 2  Ob $n=3$ ausreichen w√ºrde?</description>
    </item>
    
    <item>
      <title>Some thoughts on tidyveal and environments in R</title>
      <link>/2017/11/16/tidyeval_basense/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/16/tidyeval_basense/</guid>
      <description>The tidyeval framework is a rather new, and in parts complementary, framework to dealing with non-standarde evaluation (NSE) in R. In short, NSE is about capturing some R-code, witholding execution, maybe editing the code, and finally execuing it later and/or somewhere else.
This post borrows heavily by Edwin Thon&amp;rsquo;s great post, and this post by the same author.
In addtion, most of the knowledge is derived from Hadley Wickham&amp;rsquo;s book Advanced R.</description>
    </item>
    
    <item>
      <title>Yart - Yet Another Markdown Report Template</title>
      <link>/2017/11/15/yart/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/15/yart/</guid>
      <description>It would be useful to have a RMarkdown template for typical (academic) reports such as class assigments and bachelor/master thesises. The LaTeX class &amp;ldquo;report&amp;rdquo; provides a suitable format for that. This package provides a simple wrapper around this class built on the standard pandoc template.
Thanks to Yart, ie, this package leans on earlier work by Aaron Wolen in his pandoc-letter repository, and extends it for use from R via the rmarkdown package.</description>
    </item>
    
    <item>
      <title>Package &#39;pradadata&#39; on Github - feature social science data</title>
      <link>/2017/11/07/pradadata/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/07/pradadata/</guid>
      <description>Recently, I&amp;rsquo;ve put a package on Github featureing some social science data set. Some data came from official sites; my contribution was to clear &amp;lsquo;em up, and render comfortably accessable for automatic inquiry (nice header lines, no special enconding, flat csvs&amp;hellip;.). In other cases it&amp;rsquo;s unpublished data collected by friends, students of mine or myself.
Let&amp;rsquo;s check its contents using a function by Maiasaura from this SO post.
library(pradadata) lsp &amp;lt;- function (package, all.</description>
    </item>
    
    <item>
      <title>Populism in tweets of German politicians</title>
      <link>/2017/11/01/afd01/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/01/afd01/</guid>
      <description>The last months (years? since ever???) have seen a surge in populism and a rise in nationalism. Not only in Russia, the United States, Turkey, but also in some EU countries the ghost of nationalism-populism seems to be marching and gaining ground.
As to Germany, in September 24, 2017, the 19. German federal elections took place. The newly founded alt-right AfD (Alternative for Deutschland) has made a leap and moved in the Bundestag.</description>
    </item>
    
    <item>
      <title>Data, machine-friendly, of the 2017 German federal elections</title>
      <link>/2017/10/30/de-elec-data/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/30/de-elec-data/</guid>
      <description>On September 2017, the 19. German Bundestag has been elected. As of this writing, the parties are still busy sorting out whether they want to part of the government, with whom, and maybe whether they even want to form a government at all. This post is about providing the data in machine friendly form, and in English language.
All data presented in this post regarding this (and previous) elections are published by the Bundeswahlleiter.</description>
    </item>
    
    <item>
      <title>Mapping foreigner ratio to AfD election results in the German Wahlkreise</title>
      <link>/2017/10/22/afd-map-foreigners/</link>
      <pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/22/afd-map-foreigners/</guid>
      <description>In a previous post, we have shed some light on the idea that populism - as manifested in AfD election results - is associated with socioeconomic deprivation, be it subjective or objective. We found some supporting pattern in the data, although that hypothesis is far from being complete; ie., most of the variance remained unexplained.
In this post, we test the hypothesis that AfD election results are negatively associated with the proportion of foreign nationals in a Wahlkreis.</description>
    </item>
    
    <item>
      <title>Simple way to separate train and test sample in R</title>
      <link>/2017/10/17/train-test/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/17/train-test/</guid>
      <description>For statistical modeling, it is typical to separate a train sample from a test sample. The training sample is used to build (&amp;ldquo;train&amp;rdquo;) the model, whereas the test sample is used to gauge the predictive quality of the model.
There are many ways to split off a test sample from the train sample. One quite simple, tidyverse-oriented way, is the following.
First, load the tidyverse. Next, load some data.
library(tidyverse) data(Affairs, package = &amp;quot;AER&amp;quot;)  Then, create an index vector of the length of your train sample, say 80% of the total sample size.</description>
    </item>
    
    <item>
      <title>Two R plot side by side in .Rmd-Files</title>
      <link>/2017/10/12/two-plots-rmd/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/12/two-plots-rmd/</guid>
      <description>I kept wondering who to plot two R plots side by side (ie., in one &amp;ldquo;row&amp;rdquo;) in a .Rmd chunk. Here&amp;rsquo;s a way, well actually a number of ways, some good, some &amp;hellip; not.
library(tidyverse) library(gridExtra) library(grid) library(png) library(downloader) library(grDevices) data(mtcars)  Plots from ggplot Say, you have two plots from ggplot2, and you would like them to put them next to each other, side by side (not underneath each other):</description>
    </item>
    
    <item>
      <title>Mapping unemployment ratio to AfD election results in German Wahlkreise</title>
      <link>/2017/10/10/afd-map/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/10/afd-map/</guid>
      <description>There is the idea that the alt-right German party AfD is followed by those who are deprived of chances, thoses of fearing to falling down the social ladder, and so on. Let&amp;rsquo;s test this hypothesis. No, I am not thinking on hypothesis testing, p-values, and stuff. Rather, let&amp;rsquo;s color a map of German election districts (Wahlkreise) according to whether the area is poor AND the AfD gained a lot of votes (and vice versa: the area is rich AND the AfD gained relatively few votes).</description>
    </item>
    
    <item>
      <title>Mapping unemployment rate to German district areas</title>
      <link>/2017/10/09/unemp-map/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/09/unemp-map/</guid>
      <description>A chloropleth map is a geographic map where statistical information are mapped to certain areas. Let&amp;rsquo;s plot such a chloropleth map in this post.
Packages library(sf) library(stringr) library(tidyverse) library(readxl)  Geo data Best place to get German geo data is from the &amp;ldquo;Bundesamt f√ºr Kartografie und Geod√§sie (BKG)&amp;rdquo;. One may basically use the data for a purposes unless it is against the law. I have downloaded the data 2017-10-09. More specifically, we are looking at the &amp;ldquo;Verwaltungsgebiete&amp;rdquo; (vg), that is, the administrative areas of the country, ie.</description>
    </item>
    
    <item>
      <title>Drawing a country map</title>
      <link>/2017/10/06/chloromap/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/06/chloromap/</guid>
      <description>Let&amp;rsquo;s draw a map of Bavaria, a state of Germany, in this post.
Packages library(tidyverse) library(maptools) library(sf) library(RColorBrewer) library(ggmap) library(viridis) library(stringr)  Data Let&amp;rsquo;s get the data first. Basically, we need to data files:
 the shape file, ie., a geographic details of state borders and points of interest the semantic information to points of interest eg., town names  Shape file The shape file can be downloaded from this source: http://www.</description>
    </item>
    
    <item>
      <title>Kongresse 2018 - Wirtschaftspsychologie und verwandte Gebiete</title>
      <link>/2017/09/27/kongresse_2018/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/27/kongresse_2018/</guid>
      <description>Hier finden Sie eine Auswahl an wissenschaftlichen Kongressen in 2018 aus der Wirtschaftspsychologie und angrenzenden Feldern.
Nationale Kongresse (in DACH)  64. GfA-Fr√ºhjahrskongress: Arbeit(s).Wissen.Schaf(f)t &amp;ndash; Grundlage f√ºr Management &amp;amp; Kompetenzentwicklung, 21.-23. Februar in Frankfurt am Main
Veranstalter: FOM in Frankfurt
Frist f√ºr Einreichung von Beitr√§gen: 15. September 2017
 [Jubli√§umskongress 20 Jahre Wirtschaftspsychologie]() der Gesellschaft f√ºr angewandte Wirtschaftspsychologie (GWPs), 8.-10. M√§rz 2018 in Wernigerode
Veranstalter: Gesellschaft f√ºr angewandte Wirtschaftspsychologie (GWPs) Frist f√ºr Einreichung: OFFEN</description>
    </item>
    
    <item>
      <title>Some intriguing psychology papers (open access)</title>
      <link>/2017/09/26/psy-paper-suggestions/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/26/psy-paper-suggestions/</guid>
      <description>This post presents a compilation of links to psychology papers; I have chosen papers I find intriguing particularly for working in class. All papers are open access (or a from open access repositories) which renders classroom work easier. The papers are collected from a broad range of topics but mostly with focus on general interest. The perspective is an applied one; I have not tried to select based on methodological rigor.</description>
    </item>
    
    <item>
      <title>Crashkurs Datenanalyse mit R</title>
      <link>/2017/09/12/r-crashkurs/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/12/r-crashkurs/</guid>
      <description>Willkommen zum R-Crashkurs Nicht jeder liebt Datenanalyse und Statistik&amp;hellip; in gleichem Ma√üe! Das ist zumindest meine Erfahrung aus dem Unterricht üî•. Crashkurse zu R sind vergleichbar zu Tanzkursen vor der Hochzeit: Hat schon vielen das Leben gerettet, aber ersetzt nicht ein Semester in der Pariser Tanzakademie (man beachte den Vergleich zum Unterricht an der Hochschule).
Dieser Crashkurs ist f√ºr Studierende oder Anf√§nger der Datenanalyse gedacht, die in kurzer Zeit einen verzweifelten Versuch &amp;hellip; √§h &amp;hellip; einen grundst√§ndigen √úberblick √ºber die Datenanalyse erwerben wollen.</description>
    </item>
    
    <item>
      <title>Different ways to count NAs over multiple columns</title>
      <link>/2017/09/08/sum-isna/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/08/sum-isna/</guid>
      <description>There are a number of ways in R to count NAs (missing values). A common use case is to count the NAs over multiple columns, ie., a whole dataframe. That&amp;rsquo;s basically the question &amp;ldquo;how many NAs are there in each column of my dataframe&amp;rdquo;? This post demonstrates some ways to answer this question.
Way 1: using sapply A typical way (or classical way) in R to achieve some iteration is using apply and friends.</description>
    </item>
    
    <item>
      <title>Different ways to present summaries in ggplot2</title>
      <link>/2017/09/08/ggplot-summaries/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/08/ggplot-summaries/</guid>
      <description>A convenient and well applicable visualization for comparing groups with respect to a metric variable is the boxplot. However, often, comparing means is accompanied by t-tests, ANOVAs, and friends. Such tests test the mean, not the median, and hence the boxplot is presenting the tested statistic. It would be better to align test and diagram. How can that be achieved using ggplot2? This posts demonstrates some possibilities.
First, let&amp;rsquo;s plot a boxplot.</description>
    </item>
    
    <item>
      <title>Replacing dplyr::do by purrr:map. Some considerations</title>
      <link>/2017/09/05/purrr-map-no-do/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/05/purrr-map-no-do/</guid>
      <description>Hadley Wickham has announced to depreceate dplyr::do in favor of purrr:map. In a recent post, I have made use of do, so some commentators informed me about that. In this post, I will show use cases of map, specifically as a replacement of do. map is for lists; read more about lists here.
library(tidyverse) library(broom)  We will use mtcars as a sample dataframe (boring, I know, but convenient).</description>
    </item>
    
    <item>
      <title>Comparing the pipe with base methods</title>
      <link>/2017/08/31/some-pipes/</link>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/31/some-pipes/</guid>
      <description>Some say, the pipe (#tidyverse) makes analyses in R easier. I agree. This post demonstrates some examples.
Let&amp;rsquo;s take the mtcars dataset as an example.
data(mtcars) ?mtcars  Say, we would like to compute the correlation between gasoline consumption (mpg) and horsepower (hp).
Base approach 1 cor(mtcars[, c(&amp;quot;mpg&amp;quot;, &amp;quot;hp&amp;quot;)])  ## mpg hp ## mpg 1.0000000 -0.7761684 ## hp -0.7761684 1.0000000  We use the [-operator (function) to select the columns; note that df[, c(col1, col2)] sees dataframes as matrices, and spits out a dataframe, not a vector:</description>
    </item>
    
    <item>
      <title>Shading normal curve made easy</title>
      <link>/2017/08/29/simple-shading/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/29/simple-shading/</guid>
      <description>Shading values/areas under the normal curve is a quite frequent taks in eg educational contexts. Thanks to Hadley in this post, I found this easy solution.
library(ggplot2)  ``````
ggplot(NULL, aes(c(-3,3))) + geom_area(stat = &amp;quot;function&amp;quot;, fun = dnorm, fill = &amp;quot;#00998a&amp;quot;, xlim = c(-3, 0)) + geom_area(stat = &amp;quot;function&amp;quot;, fun = dnorm, fill = &amp;quot;grey80&amp;quot;, xlim = c(0, 3))  Simple, right?
Some minor beautification:
ggplot(NULL, aes(c(-3,3))) + geom_area(stat = &amp;quot;function&amp;quot;, fun = dnorm, fill = &amp;quot;#00998a&amp;quot;, xlim = c(-3, 1)) + geom_area(stat = &amp;quot;function&amp;quot;, fun = dnorm, fill = &amp;quot;grey80&amp;quot;, xlim = c(1, 3)) + labs(x = &amp;quot;z&amp;quot;, y = &amp;quot;&amp;quot;) + scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = 1)  And some other quantiles:</description>
    </item>
    
    <item>
      <title>Programming with dplyr: Part 03, working with strings</title>
      <link>/2017/08/09/dplyr_strings/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/09/dplyr_strings/</guid>
      <description>More on programming with dplyr: converting quosures to strings In this post, we have programmed a simple function using dplyr&amp;rsquo;s programming capabilities based on tidyeval; for more intro to programming with dplyr, see here.
In this post, we&amp;rsquo;ll go one step further and programm a function where a quosure will be turned to a string. Why this? Because quite a number of functions out there except strings as input parameters.</description>
    </item>
    
    <item>
      <title>Precipitation - It never rains in Southern Nuremberg (?). Working with dates/times.</title>
      <link>/2017/08/01/weather/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/01/weather/</guid>
      <description>In this post, we will explore some date and time parsing. As an example, we will work with weather data provided by City of Nuremberg, Environmental and Meteorological Data.
We will need these packages:
library(tidyverse) # data reading and wrangling library(lubridate) # working with dates/times  First, let&amp;rsquo;s import some precipitation data:
file_name &amp;lt;- &amp;quot;~/Downloads/export-sun-nuremberg--flugfeld--airport--precipitation-data--1-hour--individuell.csv&amp;quot; rain &amp;lt;- read_csv2(file_name, skip = 13, col_names = FALSE)  ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 1)  ## Warning: 300 parsing failures.</description>
    </item>
    
    <item>
      <title>Programming with dplyr: Part 02, writing a function</title>
      <link>/2017/07/06/prop_fav/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/06/prop_fav/</guid>
      <description>Recently, since dplyr &amp;lt;= 0.6.0 a new way of dealing with NSE was introduced, called tidyeval. As with every topic that begs our attention, the question &amp;ldquo;why bother&amp;rdquo; is in place. Theone answer is &amp;ldquo;you&amp;rsquo;ll need this stuff if you want to lock dplyr verbs inside a function&amp;rdquo;. Once you like dplyr and friends, a natural second step is to use the ideas not only for interactive use, but for more &amp;ldquo;programming&amp;rdquo; type, ie.</description>
    </item>
    
    <item>
      <title>Effect sizes for the Mann-Whitney U Test: an intuition</title>
      <link>/2017/07/04/effsize_utest/</link>
      <pubDate>Tue, 04 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/04/effsize_utest/</guid>
      <description>The Mann-Whitney U-Test is a test with a wide applicability, wider than the t-Test. Why that? Because the U-Test is applicable for ordinal data, and it can be argued that confining the metric level of a psychological variable to ordinal niveau is a reasonable bet. Second, it is robust, more robust than the t-test, because it only considers ranks, not raw values. In addition, some say that the efficiency of the U-Test is very close to the t-Test (.</description>
    </item>
    
    <item>
      <title>A second look to grouping with dplyr</title>
      <link>/2017/06/28/second_look_group_by/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/28/second_look_group_by/</guid>
      <description>The one basic idea of dplyr is that each function should focus on one job. That&amp;rsquo;s why there are no functions such as compute_sumamries_by_group_with_robust_variants(df). Rather, summarising and grouping are seen as different jobs which should be accomplished by different functions. And, in turn, that&amp;rsquo;s why group_by, the grouping function of dplyr, is of considerable importance: this function should do the grouping for each operation whatsoever.
Let&amp;rsquo;s load all tidyverse libraries in one go:</description>
    </item>
    
    <item>
      <title>Programming with dplyr: Part 01, introduction</title>
      <link>/2017/06/28/prog_dplyr_01/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/28/prog_dplyr_01/</guid>
      <description>Like for [others], Hadley Wickham&amp;rsquo;s dplyr, and more generally, the tidyverse approach has considerably changed the I do data analyses. Most notably, the pipe (coming from magrittr by Stefan Milton Bache, see here) has creeped into nearly every analyses I, do.
That is, is every analyses except for functions, and other non-interactive stuff. In those programming contexts, the dplyr way does not work, due to its non standard evaluation or NSE for short.</description>
    </item>
    
    <item>
      <title>Preparation of extraversion survey data</title>
      <link>/2017/06/24/extra_prep/</link>
      <pubDate>Sat, 24 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/24/extra_prep/</guid>
      <description>For teaching purposes and out of curiosity towards some psychometric questions, I have run a survey on extraversion here. The dataset has been published at OSF (DOI 10.17605/OSF.IO/4KGZH). The survey is base on a google form, which in turn saves the data in Google spreadsheet. Before the data can be analyzed, some preparation and makeup is in place. This posts shows some general makeup, typical for survey data.
Download the data and load packages Download the data from source (Google spreadsheets); the package gsheet provides an easy interface for that purpose.</description>
    </item>
    
    <item>
      <title>Print csv-file tables as plots</title>
      <link>/2017/06/22/tab2plot/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/22/tab2plot/</guid>
      <description>tl;dr Use this convenience function to print a dataframe as a png-plot: tab2grob().
Source the function here: https://sebastiansauer.github.io/Rcode/tab2grob.R
Easiest way in R:
source(&amp;quot;https://sebastiansauer.github.io/Rcode/tab2grob.R&amp;quot;)  Printing csv-dataframes as ggplot plots Recently, I wanted to print dataframes not as normal tables, but as a png-plot. See:
Why? Well, basically as a convenience function for colleagues who are not into using Markdown &amp;amp; friends. As I am preparing some stats stuff (see my new open access course material here) using RMarkdown, I wanted to prepare the materials ready for using in Powerpoint.</description>
    </item>
    
    <item>
      <title>Review of &#34;The 7 Deadly Sins of Psychology&#34; by Chris Chambers</title>
      <link>/2017/06/22/seven-sins/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/22/seven-sins/</guid>
      <description>tl;dr: great book. Read.
The &amp;ldquo;Seven Sins&amp;rdquo; is concerned about the validity of psychological research. Can we at all, or to what degree, be certain about the conclusions reached in psychological research? More recently, replications efforts have cast doubt on our confidence in psychological research (1). In a similar vein, a recent papers states that in many research areas, researchers mostly report &amp;ldquo;successes&amp;rdquo; in the sense of that they report that their studies confirm their hypotheses - with Psychology leading in the proportion of supported hypotheses (2).</description>
    </item>
    
    <item>
      <title>Identifying the package of a function</title>
      <link>/2017/06/12/finds_funs/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/12/finds_funs/</guid>
      <description>tl;dr Suppose you want to know which package(s) a given R function belongs to, say filter. Here come find_funsto help you:
find_funs(&amp;quot;filter&amp;quot;)  ## # A tibble: 4 x 3 ## package_name builtin_pckage loaded ## &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt; &amp;lt;lgl&amp;gt; ## 1 base TRUE TRUE ## 2 dplyr FALSE TRUE ## 3 plotly FALSE FALSE ## 4 stats TRUE TRUE  This function will search all installed packages for this function name.</description>
    </item>
    
    <item>
      <title>Sorting the x-axis in bargraphs using ggplot2</title>
      <link>/2017/06/05/ordering-bars/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/05/ordering-bars/</guid>
      <description>Some time ago, I posted about how to plot frequencies using ggplot2. One point that remained untouched was how to sort the order of the bars. Let&amp;rsquo;s look at that issue here.
First, let&amp;rsquo;s load some data.
data(tips, package = &amp;quot;reshape2&amp;quot;)  And the usual culprits.
library(tidyverse) library(scales) # for percentage scales  First, let&amp;rsquo;s plot a standard plot, with bars *un*sorted.
tips %&amp;gt;% count(day) %&amp;gt;% mutate(perc = n / nrow(tips)) -&amp;gt; tips2 ggplot(tips2, aes(x = day, y = perc)) + geom_bar(stat = &amp;quot;identity&amp;quot;)  Hang on, what could &amp;lsquo;unsorted&amp;rsquo; possibly mean?</description>
    </item>
    
    <item>
      <title>mean and sd of z-values</title>
      <link>/2017/05/26/z-values/</link>
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/26/z-values/</guid>
      <description>Edit: This post was updated, including two errors fixed - thanks to (private) comments from Norman Markgraf.
z-values, aka values coming from an z-transformation are a frequent creature in statistics land. Among their properties are the following:
 mean is zero variance is one (and hence sd is one)  But why is that? How come that this two properties are true? The goal of this post is to shed light on these two properties of z-values.</description>
    </item>
    
    <item>
      <title>Simple way of plotting normal/logistic/etc. curve</title>
      <link>/2017/05/24/plotting_s-curve/</link>
      <pubDate>Wed, 24 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/24/plotting_s-curve/</guid>
      <description>Plotting a function is often helpful to better understand what&amp;rsquo;s going on. Plotting curves in R base is simple by virtue of function curve. But how to draw curves using ggplot2?
That&amp;rsquo;s a little bit more complicated by can still be accomplished by 1-2 lines.
library(ggplot2)  Normal curve p &amp;lt;- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) p + stat_function(fun = dnorm, n = 101)  stat_function is some kind of parallel function to curve.</description>
    </item>
    
    <item>
      <title>Squares maximize area - a visualization</title>
      <link>/2017/05/19/maximize_area/</link>
      <pubDate>Fri, 19 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/19/maximize_area/</guid>
      <description>An old story is that one of the farmer with a fence of some given length, say 20m. Now this farmer wants to put up his fence so that he claims the largest piece of land possible. What width (w) and height (h) should we pick?
Instead of a formal proof, let&amp;rsquo;s start with a visualization.
First, we need some packages.
library(tidyverse) library(gganimate) library(RColorBrewer) library(scales) library(knitr)  Now, let&amp;rsquo;s make up serveral ways to split up a rectengular piece of land.</description>
    </item>
    
    <item>
      <title>A predictor&#39;s unique contribution - (visual) demonstration</title>
      <link>/2017/05/17/storks/</link>
      <pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/17/storks/</guid>
      <description>A well-known property of regression models is that they capture the unique contribution of a predictor. By &amp;ldquo;unique&amp;rdquo; we mean the effect of the predictor (on the criterion) if the other predictor(s) is/are held constant. A typical classroom example goes along the following lines.
All about storks  There&amp;rsquo;s a correlation between babies and storks. Counties with lots of storks enjoy large number of babies and v.v.
 However, I have children, I know the storks are not overly involved in that business, so says the teacher (polite laughters in the audience).</description>
    </item>
    
    <item>
      <title>Crashkurs Datenanalyse mit R</title>
      <link>/2017/05/16/crashkurs/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/16/crashkurs/</guid>
      <description>Nicht jeder liebt Datenanalyse und Statistik&amp;hellip; in gleichem Ma√üe. Das ist zumindest meine Erfahrung aus dem Unterricht :neckbeard: :fire:. Crashkurse zu R sind vergleichbar zu Crahskursen zu Franz√∂sisch - kann man machen, aber es sollte die Maxime gelten &amp;ldquo;If everything else fails&amp;rdquo;.
Dieser Crashkurs ist f√ºr Studierende oder Anf√§nger der Datenanalyse gedacht, die in kurzer Zeit einen verzweifelten Versuch &amp;hellip; √§h &amp;hellip; einen grundst√§ndigen √úberblick √ºber die Datenanalyse erwerben wollen.</description>
    </item>
    
    <item>
      <title>Introductory books for data analysis</title>
      <link>/2017/05/15/books/</link>
      <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/15/books/</guid>
      <description>One way to dig into some topic such as data analysis is just-doing, trial and error. Another way is reading blogs; a fruitful avenue in my experience. However, the classical way of reading some good book is all but outdated.
Here are some recommendations of books I found helpful as a starter (books in English and German).
R for Data Science Grolemund, G., &amp;amp; Wickham, H. (2016). R for Data Science.</description>
    </item>
    
    <item>
      <title>Plotting true random numbers</title>
      <link>/2017/05/12/true_random/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/12/true_random/</guid>
      <description>knitr::opts_chunk$set(fig.align = &amp;quot;center&amp;quot;, out.width = &amp;quot;70%&amp;quot;, fig.asp = .61)  Every now and then, random numbers come in handy to demonstrate some statistical behavior. Of course, well-known appraoches are rnorm and friends. These functions are what is called pseudo random number generators, because they are not random at all, strictly speaking, but determined by some algorithm. An algorithm is a sort of creature that is 100% predictable once you know the input (and the details of the algorithm).</description>
    </item>
    
    <item>
      <title>Deriving the logits for logistic regression</title>
      <link>/2017/05/06/deriving-the-logits-for-logistic-regression/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/06/deriving-the-logits-for-logistic-regression/</guid>
      <description>The logistic regression is an incredible useful tool, partly because binary outcomes are so frequent in live (‚Äúshe loves me - she doesn‚Äôt love me‚Äù). In parts because we can make use of well-known ‚Äúnormal‚Äù regression instruments.
But the formula of logistic regression appears opaque to many (beginners or those with not so much math background).
Let‚Äôs try to shed some light on the formula by discussing some accessible explanation on how to derive the formula.</description>
    </item>
    
    <item>
      <title>Variance explained vs. variance blurred</title>
      <link>/2017/05/05/explained_variance/</link>
      <pubDate>Fri, 05 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/05/explained_variance/</guid>
      <description>Frequently, someones says that some indicator variable X &amp;ldquo;explains&amp;rdquo; some proportion of some target variable, Y. What does this actually mean? By &amp;ldquo;mean&amp;rdquo; I am trying to find some intuition that &amp;ldquo;clicks&amp;rdquo; rather than citing the (well-known) formualas.
To start with, let&amp;rsquo;s load some packages and make up some random data.
library(tidyverse)  n_rows &amp;lt;- 100 set.seed(271828) df &amp;lt;- data_frame( exp_clean = rnorm(n = n_rows, mean = 2, sd = 1), cntrl_clean = rnorm(n = n_rows, mean = 0, sd = 1), exp_noisy = exp_clean + rnorm(n = n_rows, mean = 0, sd = 3), cntrl_noisy = cntrl_clean + rnorm(n = n_rows, mean = 0, sd = 3), ID = 1:n_rows)  Here, we drew 100 cases from the population of the &amp;ldquo;experimental group&amp;rdquo; (mue = 2) and 100 cases from the control group (mue = 0).</description>
    </item>
    
    <item>
      <title>This blog now has a DOI</title>
      <link>/2017/05/04/doi_added/</link>
      <pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/04/doi_added/</guid>
      <description>A DOI is useful feature to any electronic document. What the ID number in your passport is to you is the DOI to a document. It simply helps to make sure you address the &amp;ldquo;object&amp;rdquo; you want to address.
Similarly, there may exists several &amp;ldquo;Joachims Zwiwwelkoecks&amp;rdquo; in this world (well, it may or may not be the case). However, if any of this person gets his (or her) unique ID (could by a simple number), then we would in principle always be certain that we address the right person.</description>
    </item>
    
    <item>
      <title>Einf√ºhrung in die Datenanalyse mit R-Paket &#39;dplyr&#39; - R User Group N√ºrnberg</title>
      <link>/2017/04/27/datenanalyse_mit_dplyr/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/27/datenanalyse_mit_dplyr/</guid>
      <description>Datenjudo mit dplyr Einleitung Innerhalb der R-Landschaft hat sich das Paket dplyr binnen kurzer Zeit zu einem der verbreitesten Pakete entwickelt; es stellt ein innovatives Konzept der Datenanalyse zur Verf√ºgung. dplyr zeichnet sich durch zwei Ideen aus. Die erste Idee ist, dass nur Tabellen (&amp;ldquo;dataframes&amp;rdquo; oder &amp;ldquo;tibbles&amp;rdquo;) verarbeitet werden, keine anderen Datenstrukturen. Diese Tabellen werden von Funktion zu Funktion durchgereicht. Der Fokus auf Tabellen vereinfacht die Analyse, da Spalten nicht einzeln oder mittels Schleifen werden m√ºssen.</description>
    </item>
    
    <item>
      <title>Tools for Academic Writing - Comparison</title>
      <link>/2017/04/26/writing_tools/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/26/writing_tools/</guid>
      <description>Many tools exist for academic writing including the notorious W.O.R.D.; but many more are out there. Let&amp;rsquo;s have a look at those tools, and discuss what&amp;rsquo;s important (what we expect the tool to deliver, eg., beautiful typesetting).
Typical tools for academic writing  MS Word: A &amp;ldquo;classical&amp;rdquo; choice, relied upon by myriads of white collar workers&amp;hellip; I myself have used it extensively for academic writing; the main advantage being its simplicity, that is, well, everybody knows it, and knows more or less how to handle it.</description>
    </item>
    
    <item>
      <title>Covariance as correlation</title>
      <link>/2017/04/25/cor_as_cov/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/25/cor_as_cov/</guid>
      <description>Correlation is one of the most widely used and a well-known measure of the assocation (linear association, that is) of two variables.
Perhaps less well-known is that the correlation is in principle identical to the covariation.
To see this, consider the a formula of the covariance of two empirical datasets, $$X$$ and $$Y$$:
$$COV(X,Y) = \frac{1}{n} \cdot \big( \sum (X_i -\bar{X}) \cdot (Y_i - \bar{Y}) \big) $$
In other words, the covariance of $X$ and $Y$ $COV(X,Y)$ is the average of difference of some value to its mean.</description>
    </item>
    
    <item>
      <title>Plotting skewed distributions</title>
      <link>/2017/04/19/skewed-distribs/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/19/skewed-distribs/</guid>
      <description>Let&amp;rsquo;s plot some skewed stuff, aehm, distributions!
Actually, the point I - initially - wanted to make is that in skewed distribution, don&amp;rsquo;t use means. Or at least, be very aware that (arithmetic) means can be grossly misleading. But for today, let&amp;rsquo;s focus on drawing skewed distributions.
Some packages:
library(tidyverse) library(fGarch) # for snorm  Some skewed distribution include:
 &amp;ldquo;polluted&amp;rdquo; normal distributions, ie., mixtures of two normals Exponential distributions Gamma distributions Beta distributions  One way to visualize them is to draw their curve, ie.</description>
    </item>
    
    <item>
      <title>Error bars for interaction effects with nominal variables</title>
      <link>/2017/04/18/moderator-errorbars/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/18/moderator-errorbars/</guid>
      <description>Moderator effects (ie., interaction or synergy effects) are a topic of frequent interest in many sciences braches. A lot ink has been spilled over this topic (so did I, eg., here).
However, in that post I did now show how to visualize error in case of nominal (categorical) independent variable, and categorical moderator.
Luckily, visualization of this case is quite straight forward with ggplot2.
First, some data and packages to be loaded:</description>
    </item>
    
    <item>
      <title>The effect of sample on p-values. A simulation.</title>
      <link>/2017/04/13/pvalue_sample_size/</link>
      <pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/13/pvalue_sample_size/</guid>
      <description>It is well-known that the notorious p-values is sensitive to sample size: The larger the sample, the more bound the p-value is to fall below the magic number of .05.
Of course, the p-value is also a function of the effect size, eg., the distance between two means and the respective variances. But still, the p-values tends to become significant in the face of larges samples, and non-significant otherwise.</description>
    </item>
    
    <item>
      <title>Three ways to dichotomize a variable</title>
      <link>/2017/04/11/three_ways_recoding_cutting/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/11/three_ways_recoding_cutting/</guid>
      <description>Dichotomizing is also called dummy coding. It means: Take a variable with multiple different values (&amp;gt;2), and transform it so that the output variable has 2 different values.
Note that this &amp;ldquo;thing&amp;rdquo; can be understood as consisting of two different aspects: Recoding and cutting. Recoding means that value &amp;ldquo;a&amp;rdquo; becomes values &amp;ldquo;b&amp;rdquo; etc. Cutting means that a &amp;ldquo;rope&amp;rdquo; of numbers is cut into several shorter &amp;ldquo;ropes&amp;rdquo; (that&amp;rsquo;s why it is called cutting).</description>
    </item>
    
    <item>
      <title>Rowwise operations in dplyr</title>
      <link>/2017/03/27/rowwise_dplyr/</link>
      <pubDate>Mon, 27 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/27/rowwise_dplyr/</guid>
      <description>R thinks columnwise, not rowwise, at least in standard dataframe operations. A typical rowwise operation is to compute row means or row sums, for example to compute person sum scores for psychometric analyses.
One workaround, typical for R, is to use functions such as apply (and friends).
However, dplyr offers some quite nice alternative:
library(dplyr) mtcars %&amp;gt;% rowwise() %&amp;gt;% mutate(mymean=mean(c(cyl,mpg))) %&amp;gt;% select(cyl, mpg, mymean)  ## Source: local data frame [32 x 3] ## Groups: &amp;lt;by row&amp;gt; ## ## # A tibble: 32 √ó 3 ## cyl mpg mymean ## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; ## 1 6 21.</description>
    </item>
    
    <item>
      <title>Convert list to dataframe</title>
      <link>/2017/03/08/convert_list_to_dataframe/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/08/convert_list_to_dataframe/</guid>
      <description>A handy function to iterate stuff is the function purrr::map. It takes a function and applies it to all elements of a given vector. This vector can be a data frame - which is a list, tecnically - or some other sort of of list (normal atomic vectors are fine, too).
However, purrr::map is designed to return lists (not dataframes). For example, if you apply mosaic::favstats to map, you will get some favorite statistics for some variable:</description>
    </item>
    
    <item>
      <title>How to avoid Github/merge conflicts with Rmd-files</title>
      <link>/2017/03/06/avoid_merge_conflicts/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/06/avoid_merge_conflicts/</guid>
      <description>One nice features of .rmd files is that version control systems, such as git and github, can (quite) easily be combined. However, in my experience, merge conflicts are not so uncommon. That raises the question how to avoid merge conflicts when syncing with Github?
Here&amp;rsquo;s a quick overview on what to do to that hassle:
 Sync often. Hard wrap the lines to approx. 80 characters. Pull before you start to change the source files.</description>
    </item>
    
    <item>
      <title>Lieblings-R-Befehle</title>
      <link>/2017/03/05/lieblingsbefehle/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/05/lieblingsbefehle/</guid>
      <description>Hier eine Liste einiger meiner &amp;ldquo;Lieblings-R-Funktionen&amp;rdquo;; f√ºr Einf√ºhrungsveranstaltungen in Statistik spielen sie (bei mir) eine wichtige Rolle. Die Liste kann sich √§ndern :-)
Wenn ich von einer &amp;ldquo;Tabelle&amp;rdquo; spreche, meine ich sowohl Dataframes als auch Tibbles.
Zuweisung - &amp;lt;- Mit dem Zuweisungsoperator &amp;lt;- kann man Objekten einen Wert zuweisen:
x &amp;lt;- 1 mtcars2 &amp;lt;- mtcars  Spalten als Vektor ausw√§hlen - $ Mit dem Operator $ kann man eine Spalte einer Tabelle ausw√§hlen.</description>
    </item>
    
    <item>
      <title>AfD Mining - basales Textmining zum AfD-Parteiprogramm</title>
      <link>/2017/02/21/textmining_afd_01/</link>
      <pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/21/textmining_afd_01/</guid>
      <description>F√ºr diesen Post ben√∂tigte R-Pakete:
library(stringr) # Textverarbeitung library(tidytext) # Textmining library(pdftools) # PDF einlesen library(downloader) # Daten herunterladen # library(knitr) # HTML-Tabellen library(htmlTable) # HTML-Tabellen library(lsa) # Stopw√∂rter library(SnowballC) # W√∂rter trunkieren library(wordcloud) # Wordcloud anzeigen library(gridExtra) # Kombinierte Plots library(dplyr) # Datenjudo library(ggplot2) # Visualisierung  Ein einf√ºhrendes Tutorial zu Textmining; analysiert wird das Parteiprogramm der Partei &amp;ldquo;Alternative f√ºr Deutschland&amp;rdquo; (AfD). Vor dem Hintergrund des gestiegenen Zuspruchs von Rechtspopulisten und der gro√üen Gefahr, die von diesem Gedankengut ausd√ºnstet, erscheint mir eine facettenreiche Analyse des Ph√§nomens &amp;ldquo;Rechtspopulismus&amp;rdquo; n√∂tig.</description>
    </item>
    
    <item>
      <title>Checklist for Data Cleansing</title>
      <link>/2017/02/13/data_cleansing/</link>
      <pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/13/data_cleansing/</guid>
      <description>What this post is about: Data cleansing in practice with R Data analysis, in practice, consists typically of some different steps which can be subsumed as &amp;ldquo;preparing data&amp;rdquo; and &amp;ldquo;model data&amp;rdquo; (not considering communication here):
(Inspired by this)
Often, the first major part &amp;ndash; &amp;ldquo;prepare&amp;rdquo; &amp;ndash; is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!</description>
    </item>
    
    <item>
      <title>Sentiment-W√∂rterbuch erstellen</title>
      <link>/2017/02/04/sentiment_dictionary/</link>
      <pubDate>Sat, 04 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/04/sentiment_dictionary/</guid>
      <description>Bei der Textanalyse (Textmining) ist die Sentiment-Analyse eine typische T√§tigkeit. Nat√ºrlich steht und f√§llt die Qualit√§t der Sentiment-Analyse mit der Qualit√§t des verwendeten W√∂rterbuchs (was nicht hei√üt, dass man nicht auch auf andere Klippen schellen kann).
Der Zweck dieses Posts ist es, eine Sentiment-Lexikon in deutscher Sprache einzulesen.
Dazu wird das Sentiment-Lexikon dieser Quelle verwendet (CC-BY-NC-SA 3.0). In diesem Paper finden sich Hintergr√ºnde. Von dort lassen sich die Daten herunter laden.</description>
    </item>
    
    <item>
      <title>Dataset &#39;performance in stats test&#39;</title>
      <link>/2017/01/27/data_test_inference/</link>
      <pubDate>Fri, 27 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/27/data_test_inference/</guid>
      <description>This posts shows data cleaning and preparation for a data set on a statistics test (NHST inference). Data is published under a CC-licence, see here.
Data was collected 2015 to 2017 in statistics courses at the FOM university in different places in Germany. Several colleagues helped to collect the data. Thanks a lot! Now let&amp;rsquo;s enjoy the outcome (and make it freely available to all).
Raw N is 743. The test consists of 40 items which are framed as propositions; students are asked to respond with either &amp;ldquo;true&amp;rdquo; or &amp;ldquo;false&amp;rdquo; to each item.</description>
    </item>
    
    <item>
      <title>Convert logit to probability</title>
      <link>/2017/01/24/convert_logit2prob/</link>
      <pubDate>Tue, 24 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/24/convert_logit2prob/</guid>
      <description>Logistic regression may give a headache initially. While the structure and idea is the same as &amp;ldquo;normal&amp;rdquo; regression, the interpretation of the b&amp;rsquo;s (ie., the regression coefficients) can be more challenging.
This post provides a convenience function for converting the output of the glm function to a probability. Or more generally, to convert logits (that&amp;rsquo;s what spit out by glm) to a probabilty.
Note1: The objective of this post is to explain the mechanics of logits.</description>
    </item>
    
    <item>
      <title>Gentle intro to &#39;R-squared equals squared r&#39;</title>
      <link>/2017/01/20/rsquared/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/20/rsquared/</guid>
      <description>It comes as no surprise that $$R^2$$ (&amp;ldquo;coefficient of determination&amp;rdquo;) equals $$r^2$$ in simple regression (predictor X, criterion Y), where $$r(X,Y)$$ is Pearson&amp;rsquo;s correlation coefficient. $$R^2$$ equals the fraction of explained variance in a simple regression. However, the statistical (mathematical) background is often less clear or buried in less-intuitive formula.
The goal of this post is to offer a gentle explanantion why
$$R^2 = r^2$$,
where $$r$$ is $$r(Y,\hat{Y})$$ and $$\hat{Y}$$ are the predicted values.</description>
    </item>
    
    <item>
      <title>The two ggplot2-ways of plottings bars</title>
      <link>/2017/01/20/two_ways_barplots_with_ggplot2/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/20/two_ways_barplots_with_ggplot2/</guid>
      <description>Bar plots, whereas not appropriate for means, are helpful for conveying impressions of frequencies, particularly relative frequencies, ie., proportions.
Intuition: Bar plots and histograms alike can be thought of as piles of Lego pieces, put onto each each other, where each Lego piece represents (is) one observation.
Presenting tables of frequencies are often not insightful to the eye. Bar plots are often much more accessible and present the story more clearly.</description>
    </item>
    
    <item>
      <title>Fallstudie (YACSDA) zur praktischen Datenanalyse mit dplyr</title>
      <link>/2017/01/18/fallstudie_flights/</link>
      <pubDate>Wed, 18 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/18/fallstudie_flights/</guid>
      <description>Case study in data analysis using R package dplyr in German language.
Praktische Datenanalyse mit dplyr Das R-Paket dplyr von Hadley Wickham ist ein Stargast auf der R-Showb√ºhne; h√§ufig diskutiert in einschl√§gigen Foren. Mit dyplr kann man Daten &amp;ldquo;verhackst√ºcken&amp;rdquo; - umformen und aufbereiten (&amp;ldquo;to wrangle&amp;rdquo; auf Englisch); &amp;ldquo;praktische Datenanalyse&amp;rdquo; ist vielleicht eine gute Bezeichnung. Es finden sich online viele Einf√ºhrungen, z.B. hier oder hier.
Dieser Text ist nicht als Einf√ºhrung oder Erl√§uterung gedacht, sondern als √úbung, um (neu erworbenen F√§higkeiten) in der praktischen Datenanalyse im Rahmen einer Fallstudie auszuprobieren.</description>
    </item>
    
    <item>
      <title>I am unavailable for review</title>
      <link>/2017/01/17/unavailable_for_review/</link>
      <pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/17/unavailable_for_review/</guid>
      <description>Dear editorial team,
Thanks for considering me for review. After some thought-meandering I came to the conclusion that traditional publishers - such as the present publisher of this journal - support a business model that I deem unfair and inappropriate for regular science and for the interests of science and scientists alike. That is, the fees are much too high thereby sucking resources out of the science system and out of society which could be used for the better otherwise.</description>
    </item>
    
    <item>
      <title>Kongresse 2017 - Wirtschaftspsychologie und verwandte Gebiete</title>
      <link>/2017/01/17/kongresstermine_2017/</link>
      <pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/17/kongresstermine_2017/</guid>
      <description>Hier finden Sie eine Auswahl an wissenschaftlichen Kongressen in 2017 aus der Wirtschaftspsychologie und angrenzender Felder.
Nationale Kongresse 2017 (in Deutschland)  GWPS, 2.-4. M√§rz in Darmstadt
Fachtagung der Gesellschaft f√ºr angewandte Wirtschaftspsychologie (GWPs)
Submission Deadline: 30. Nov 2016
 TeaP, 26.-29. M√§rz in Dresden
Conference of Experimental Psychologists Submission Deadline: 15. Nov. 2016
 DiffPsy, 4.-6. September in M√ºnchen
Arbeitstagung der Fachgruppe Differenzielle Psychologie, Pers√∂nlichkeitspsychologie und Psychologische Diagnostik</description>
    </item>
    
    <item>
      <title>Visualizing Interaction Effects with ggplot2</title>
      <link>/2017/01/17/vis_interaction_effects/</link>
      <pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/17/vis_interaction_effects/</guid>
      <description>Moderator effects or interaction effect are a frequent topic of scientific endeavor. Put bluntly, such effects respond to the question whether the input variable X (predictor or independent variable IV) has an effect on the output variable (dependent variable DV) Y: &amp;ldquo;it depends&amp;rdquo;. More precisely, it depends on a second variable, M (Moderator).
More formally, a moderation effect can be summarized as follows:
 If the effect of X on Y depends on M, a moderator effect takes place.</description>
    </item>
    
    <item>
      <title>How to import a strange CSV</title>
      <link>/2017/01/12/strange_csvs/</link>
      <pubDate>Thu, 12 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/12/strange_csvs/</guid>
      <description>A typical task in data analysis is to import CSV-formatted data. CSV is nothing more than a text file with data in rectangular form; rows stand for observations (eg., persons), and columns represent variables (such as age). Columns are separed by a &amp;ldquo;separator&amp;rdquo;, often a comma. Hence the name &amp;ldquo;CSV&amp;rdquo; - &amp;ldquo;comma separeted values&amp;rdquo;. Note however that the separator can in principle anything you like (eg., &amp;ldquo;;&amp;rdquo; or tabulator or &amp;ldquo; &amp;ldquo;).</description>
    </item>
    
    <item>
      <title>R startet nicht</title>
      <link>/2017/01/11/r_startet_nicht/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/11/r_startet_nicht/</guid>
      <description>Hilfe! Mein R startet nicht! Mein R startet zwar, tut aber nicht so, wie ich will. Sicherlich hat es sich (wieder einmal) gegen mich verschworen. Wahrscheinlich hilft nur noch Verschrotten&amp;hellip; Bevor Sie zum √§u√üersten schreiten, hier einige Tipps, die sich bew√§hrt haben.
L√∂sungen, wenn R nicht (richtig) l√§uft  AEG: Aus. Ein. Gut. Starten Sie den Rechner neu. Gerade nach Installation neuer Software zu empfehlen.
 Sehen Sie eine Fehlermeldung, die von einem fehlenden Paket spricht (z.</description>
    </item>
    
    <item>
      <title>Convert data frame from &#39;wide&#39; to &#39;long&#39;</title>
      <link>/2017/01/06/facial_beauty/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/06/facial_beauty/</guid>
      <description>Thanks to my student Marie Halbich who took the pains to collect the data!
At times, your data set will be in &amp;ldquo;wide&amp;rdquo; format, i.e, many columns in comparison to rows. For some analyses however, it is more suitable to have the data in &amp;ldquo;long&amp;rdquo; format. That is, many rows in comparison to columns.
Let&amp;rsquo;s have a look at this data set, for example.
d &amp;lt;- read.csv(&amp;quot;https://sebastiansauer.github.io/data/facial_beauty_raw.csv&amp;quot;)  This is the data from a study tapping into the effect of computerized &amp;ldquo;beautification&amp;rdquo; of some faces on subjective &amp;ldquo;like&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>YACSDA (Fallstudie) zum Datensatz &#39;Affairs&#39;</title>
      <link>/2017/01/05/yacsda_affairs/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/05/yacsda_affairs/</guid>
      <description>This YACSDA (Yet-another-case-study-on-data-analysis) in composed in German language. Some typical data analytical steps are introduced.
Wovon ist die H√§ufigkeit von Aff√§ren (Seitenspr√ºngen) in Ehen abh√§ngig? Diese Frage soll anhand des Datensates Affair untersucht werden.
Dieser Post stellt beispielhaft eine grundlegende Methoden der praktischen Datenanalyse im Rahmen einer kleinen Fallstudie (YACSDA) vor.
Quelle der Daten: http://statsmodels.sourceforge.net/0.5.0/datasets/generated/fair.html
Der Datensatz findet sich (in √§hnlicher Form) auch im R-Paket COUNT (https://cran.r-project.org/web/packages/COUNT/index.html).
Laden wir als erstes den Datensatz in R.</description>
    </item>
    
    <item>
      <title>Why is the variance additive? An intuition.</title>
      <link>/2017/01/04/additivity_variance/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/04/additivity_variance/</guid>
      <description>The variance of some data can be defined in rough terms as the mean of the squared deviations from the mean.
Let&amp;rsquo;s repeat that because it is important:
 Variance: Mean of squared deviations from the mean.
 An example helps to illustrate. Assume some class of students are forced to write an exam in a statistics class (OMG). Let&amp;rsquo;s say the grades range fom 1 to 6, 1 being the best and 6 the worst.</description>
    </item>
    
    <item>
      <title>A Plain Markdown Post</title>
      <link>/2016/12/30/hello-markdown/</link>
      <pubDate>Fri, 30 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/30/hello-markdown/</guid>
      <description>This is a post written in plain Markdown (*.md) instead of R Markdown (*.Rmd). The major differences are:
 You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (```{r}); A plain Markdown post is rendered through Blackfriday, and an R Markdown document is compiled by rmarkdown and Pandoc.  There are many differences in syntax between Blackfriday&amp;rsquo;s Markdown and Pandoc&amp;rsquo;s Markdown.</description>
    </item>
    
    <item>
      <title>√úberleben auf der Titanic - YACSDA f√ºr nominale Daten</title>
      <link>/2016/12/22/titanic/</link>
      <pubDate>Thu, 22 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/22/titanic/</guid>
      <description>In dieser YACSDA (Yet-another-case-study-on-data-analysis) geht es um die beispielhafte Analyse nominaler Daten anhand des &amp;ldquo;klassischen&amp;rdquo; Falls zum Untergang der Titanic. Eine Frage, die sich hier aufdr√§ngt, lautet: Kann (konnte) man sich vom Tod freikaufen, etwas polemisch formuliert. Oder neutraler: H√§ngt die √úberlebensquote von der Klasse, in der derPassagiers reist, ab?
Diese √úbung soll einige grundlegende Vorgehensweise der Datenanalyse verdeutlichen; Zielgruppe sind Einsteiger (mit Grundkenntnissen in R) in die Datenanalyse.</description>
    </item>
    
    <item>
      <title>M√ºncher Mietpreis: √úbung zum p-Wert</title>
      <link>/2016/12/21/mietpreis_p-wert/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/21/mietpreis_p-wert/</guid>
      <description>Sie m√∂chten die Hypothese (H0) testen, dass der mittlere Mietpreis in M√ºnchen 16,28‚Ç¨ betr√§gt (wie der M√ºnchner Merkur einmal behauptet hat). Daf√ºr ziehen Sie eine Stichprobe der Gr√∂√üe n = 36. Gehen Sie von einer SD von 3‚Ç¨ in der Population aus (Menge aller Mietwohnungen in M√ºnchen). Alpha sei 5%. Der Mittelwert Ihrer Stichprobe ist 16,79‚Ç¨. Nehmen Sie als H1 die Hypothese, dass der wahre mittlere Mietpreis h√∂her ist.</description>
    </item>
    
    <item>
      <title>Some tricks on dplyr::filter</title>
      <link>/2016/12/21/dplyr_filter/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/21/dplyr_filter/</guid>
      <description>The R package dplyr has some attractive features; some say, this packkage revolutionized their workflow. At any rate, I like it a lot, and I think it is very helpful.
In this post, I would like to share some useful (I hope) ideas (&amp;ldquo;tricks&amp;rdquo;) on filter, one function of dplyr. This function does what the name suggests: it filters rows (ie., observations such as persons). The addressed rows will be kept; the rest of the rows will be dropped.</description>
    </item>
    
    <item>
      <title>Some thoughts on &#39;Dear stats curriculum developers&#39;</title>
      <link>/2016/12/08/stats_curriculum/</link>
      <pubDate>Thu, 08 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/08/stats_curriculum/</guid>
      <description>Recently, Andrew Gelman (@StatModeling at Twitter) published a post with this title - &amp;ldquo;‚ÄúDear Major Textbook Publisher‚Äù: A Rant&amp;rdquo;.
In essence, he discussed how a good stats intro text book should be like. And complained about the low quality of some many textbooks out there.
As I am also in the business guilty of coming up with stats curriculum for my students (applied courses for business type students mostly), I discuss some thoughts for &amp;ldquo;stats curriculum developers&amp;rdquo; (like myself).</description>
    </item>
    
    <item>
      <title>Simulation of p-values</title>
      <link>/2016/12/01/simu_p/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/01/simu_p/</guid>
      <description>Teaching or learning stats can be a challenging endeavor. In my experience, starting with concrete (as opposed to abstract) examples helps many a learner. What also helps (for me) is visualizing.
As p-values are still part and parcel of probably any given stats curriculum, here is a convenient function to simulate p-values and to plot them.
&amp;ldquo;Simulating p-values&amp;rdquo; amounts to drawing many samples from a given, specified population (eg., ¬µ=100, s=15, normally distributed).</description>
    </item>
    
    <item>
      <title>Pipe the Variance</title>
      <link>/2016/11/30/pipe_variance/</link>
      <pubDate>Wed, 30 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/30/pipe_variance/</guid>
      <description>One idea of problem solving is, or should be, I think, that one should tackle problems of high complexity, but not too high. That sounds trivial, cooler tone would be &amp;ldquo;as hard as possible, as easy as necessary&amp;rdquo; which is basically the same thing.
In software development including Rstats, a similar principle applies. Sounds theoretical, I admit. So see here some lines of code that has bitten me recently:
obs &amp;lt;- c(1,2,3) pred &amp;lt;- c(1,2,4) monster &amp;lt;- 1 - (sum((obs - pred)^2))/(sum((obs - mean(obs))^2)) monster  ## [1] 0.</description>
    </item>
    
    <item>
      <title>Some musings on the validation of Satow&#39;s Extraversion questionnaire</title>
      <link>/2016/11/23/validation_extraversion_questionnaire/</link>
      <pubDate>Wed, 23 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/23/validation_extraversion_questionnaire/</guid>
      <description>Measuring personality traits is one of (the?) bread-and-butter business of psychologists, at least for quantitatively oriented ones. Literally, thousand of psychometric questionnaires exits. Measures abound. Extroversion, part of the Big Five personality theory approach, is one of the most widely used, and extensively scrutinized questionnaire tapping into human personality.
One rather new, but quite often used questionnaire, is Satow&amp;rsquo;s (2012) B5T. The reason for the popularity of this instrument is that it runs under a CC-licence - in contrast to the old ducks, which coute chere.</description>
    </item>
    
    <item>
      <title>Preparing survey results data</title>
      <link>/2016/11/19/preparing_survey_data/</link>
      <pubDate>Sat, 19 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/19/preparing_survey_data/</guid>
      <description>Analyzing survey results is a frequent endeavor (for some including me). Let&amp;rsquo;s not think about arguments whether and when surveys are useful or not (for some recent criticism see Briggs&amp;rsquo; book).
Typically, respondents circle some option ranging from &amp;ldquo;don&amp;rsquo;t agree at all&amp;rdquo; to &amp;ldquo;completely agree&amp;rdquo; for each question (or &amp;ldquo;item&amp;rdquo;). Typically, four to six boxes are given where one is expected to tick one.
In this tutorial, I will discuss some typical steps to prepare the data for subsequent analyses.</description>
    </item>
    
    <item>
      <title>Crashkurs zur Erstellung von Barplots f√ºr Umfrage-Daten</title>
      <link>/2016/11/13/crashkurs_barplots/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/crashkurs_barplots/</guid>
      <description>Eine recht h√§ufige Art von Daten in der Wirtschaft kommen von Umfragen in der Belegschaft. Diese Daten gilt es dann aufzubereiten und graphisch wiederzugeben. Daf√ºr gibt dieser Post einige grundlegende Hinweise. Grundwissen mit R setzen wir voraus :-)
Eine ausf√ºhrlichere Beschreibung hier sich z.B. hier.
Packages laden Nicht vergessen: Ein Computerprogramm (z.B. ein R-Package) kann man nur dann laden, wenn man es vorher installier hat (aber es reicht, das Programm/R-Package einmal zu installieren).</description>
    </item>
    
    <item>
      <title>New bar stacking with ggplot 2.2.0</title>
      <link>/2016/11/13/improved_bar_stacking_ggplot2_220/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/improved_bar_stacking_ggplot2_220/</guid>
      <description>Recently, ggplot2 2.2.0 was released. Among other news, stacking bar plot was improved. Here is a short demonstration.
Load libraries
library(tidyverse) library(htmlTable)  &amp;hellip; and load data:
data &amp;lt;- read.csv(&amp;quot;https://osf.io/meyhp/?action=download&amp;quot;)  DOI for this piece of data is 10.17605/OSF.IO/4KGZH.
The data consists of results of a survey on extraversion and associated behavior.
Say, we would like to visualize the responsed to the extraversion items (there are 10 of them).
So, let&amp;rsquo;s see.</description>
    </item>
    
    <item>
      <title>Some thoughts (and simulation) on overfitting</title>
      <link>/2016/11/13/overfitting_simulation/</link>
      <pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/13/overfitting_simulation/</guid>
      <description>Overfitting is a common problem in data analysis. Some go as far as saying that &amp;ldquo;most of&amp;rdquo; published research is false (John Ionnadis); overfitting being one, maybe central, problem of it. In this post, we explore some aspects on the notion of overfitting.
Assume we have 10 metric variables v (personality/health/behavior/gene indicator variables), and, say, 10 variables for splitting up subgroups (aged vs. young, female vs. male, etc.), so 10 dichotomic variables.</description>
    </item>
    
    <item>
      <title>Plotting survey results using `ggplot2`</title>
      <link>/2016/11/12/plotting_surveys/</link>
      <pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/12/plotting_surveys/</guid>
      <description>Plotting (and more generally, analyzing) survey results is a frequent endeavor in many business environments. Let&amp;rsquo;s not think about arguments whether and when surveys are useful (for some recent criticism see Briggs&amp;rsquo; book).
Typically, respondents circle some option ranging from &amp;ldquo;don&amp;rsquo;t agree at all&amp;rdquo; to &amp;ldquo;completely agree&amp;rdquo; for each question (or &amp;ldquo;item&amp;rdquo;). Typically, four to six boxes are given where one is expected to tick one.
In this tutorial, I will discuss some barplot type visualizations; the presentation is based on ggplot2 (within the R environment) .</description>
    </item>
    
    <item>
      <title>Horoskopstudie zum Barnumeffekt</title>
      <link>/2016/11/09/horoskop-studie/</link>
      <pubDate>Wed, 09 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/09/horoskop-studie/</guid>
      <description>Viele Menschen glauben an Horoskope. Doch warum? Ein Grund k√∂nnte sein, dass Horoskope einfach gut sind. Was hei√üt gut: Sie passen auf mich aber nicht auf andere Leute (mit anderen Strernzeichen) und sie sagen Dinge, die n√ºtzlich sind.
Ein anderer Grund k√∂nnte sein, dass sie uns schmeicheln und Gemeinpl√§tze sind, denen jeder zustimmt: &amp;ldquo;Sie sind an sich ein Super-Typ, aber manchmal etwas ungeduldig&amp;rdquo; (oh ja, absolut, passt genau!). &amp;ldquo;Heute treffen Sie jemanden, der eine gro√üe Liebe werden k√∂nnte&amp;rdquo; (H√∂rt sich gut an!</description>
    </item>
    
    <item>
      <title>Some reflections on stochastic independence</title>
      <link>/2016/11/08/stochastic_independence/</link>
      <pubDate>Tue, 08 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/08/stochastic_independence/</guid>
      <description>We are often interested in the question whether two variables are &amp;ldquo;associated&amp;rdquo;, &amp;ldquo;correlated&amp;rdquo; (I mean the normal English term) or &amp;ldquo;dependent&amp;rdquo;. What exactly, or rather in normal words, does that mean? Let&amp;rsquo;s look at some easy case.
NOTE: The example has been updated to reflect a more tangible and sensible scenario (find the old one in the previous commit at Github).
Titanic data For example, let&amp;rsquo;s look at survival rates of the Titanic disaster, to see whether the probability of survival (event A) depends on the whether you embarked for 1st class (event B).</description>
    </item>
    
    <item>
      <title>Bind lists to data frame for aggregating linear models results</title>
      <link>/2016/11/04/bind_list_to_dataframe_lm/</link>
      <pubDate>Fri, 04 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/04/bind_list_to_dataframe_lm/</guid>
      <description>I found myself doing the following: I had a bunch of predictors, one (numeric) outcome, and wanted to run I simple regression for each of the predictors. Having a bunch of model results, I would like to have them bundled in one data frame.
So, here is one way to do it.
First, load some data.
data(mtcars) str(mtcars)  ## &#39;data.frame&#39;:	32 obs. of 11 variables: ## $ mpg : num 21 21 22.</description>
    </item>
    
    <item>
      <title>How to plot a &#39;percentage plot&#39; with ggplot2</title>
      <link>/2016/11/03/percentage_plot_ggplot2_v2/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/03/percentage_plot_ggplot2_v2/</guid>
      <description>At times it is convenient to draw a frequency bar plot; at times we prefer not the bare frequencies but the proportions or the percentages per category. There are lots of ways doing so; let&amp;rsquo;s look at some ggplot2 ways.
First, let&amp;rsquo;s load some data.
data(tips, package = &amp;quot;reshape2&amp;quot;)  And the typical libraries.
library(dplyr) library(ggplot2) library(tidyr) library(scales) # for percentage scales  Way 1 tips %&amp;gt;% count(day) %&amp;gt;% mutate(perc = n / nrow(tips)) -&amp;gt; tips2 ggplot(tips2, aes(x = day, y = perc)) + geom_bar(stat = &amp;quot;identity&amp;quot;)  Way 2 ggplot(tips, aes(x = day)) + geom_bar(aes(y = (.</description>
    </item>
    
    <item>
      <title>Different ways to set figure size in RMarkdown</title>
      <link>/2016/11/02/figure_sizing_knitr/</link>
      <pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/02/figure_sizing_knitr/</guid>
      <description>Markdown is thought as a &amp;ldquo;lightweight&amp;rdquo; markup language, hence the name markdown. That&amp;rsquo;s why formatting options are scarce. However, there are some extensions, for instance brought by RMarkdown.
One point of particular interest is the sizing of figures. Let&amp;rsquo;s look at some ways how to size a figure with RMarkdown.
We take some data first:
data(mtcars) names(mtcars)  ## [1] &amp;quot;mpg&amp;quot; &amp;quot;cyl&amp;quot; &amp;quot;disp&amp;quot; &amp;quot;hp&amp;quot; &amp;quot;drat&amp;quot; &amp;quot;wt&amp;quot; &amp;quot;qsec&amp;quot; &amp;quot;vs&amp;quot; &amp;quot;am&amp;quot; &amp;quot;gear&amp;quot; ## [11] &amp;quot;carb&amp;quot;  Not let&amp;rsquo;s plot.</description>
    </item>
    
    <item>
      <title>CLES plot</title>
      <link>/2016/10/17/cles-plot/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/17/cles-plot/</guid>
      <description>In data analysis, we often ask &amp;ldquo;Do these two groups differ in the outcome variable&amp;rdquo;? Asking this question, a tacit assumption may be that the grouping variable is the cause of the difference in the outcome variable. For example, assume the two groups are &amp;ldquo;treatment group&amp;rdquo; and &amp;ldquo;control group&amp;rdquo;, and the outcome variable is &amp;ldquo;pain reduction&amp;rdquo;.
A typical approach would be to report the strenght of the difference by help of Cohen&amp;rsquo;s d.</description>
    </item>
    
    <item>
      <title>Checking for NA with dplyr</title>
      <link>/2016/10/16/nas-with-dplyr/</link>
      <pubDate>Sun, 16 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/16/nas-with-dplyr/</guid>
      <description>Often, we want to check for missing values (NAs). There are of course many ways to do so. dplyr provides a quite nice one.
First, let&amp;rsquo;s load some data:
library(readr) extra_file &amp;lt;- &amp;quot;https://raw.github.com/sebastiansauer/Daten_Unterricht/master/extra.csv&amp;quot; extra_df &amp;lt;- read_csv(extra_file)  Note that extra is a data frame consisting of survey items regarding extraversion and related behavior.
In case the dataframe is quite largish (many columns) it is helpful to have some quick way. Here, we have 25 columns.</description>
    </item>
    
    <item>
      <title>Multiple ways to subsetting data frames in R</title>
      <link>/2016/10/15/indexing-in-r/</link>
      <pubDate>Sat, 15 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/15/indexing-in-r/</guid>
      <description>Subsetting a data frame is an essential and frequently performed task. Here, some basic ideas are presented.
Get some data first.
str(mtcars)  ## &#39;data.frame&#39;:	32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 .</description>
    </item>
    
    <item>
      <title>How to read Github files into R easily</title>
      <link>/2016/10/12/download-from-github/</link>
      <pubDate>Wed, 12 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/12/download-from-github/</guid>
      <description>Downloading a folder (repository) from Github as a whole The most direct way to get data from Github to your computer/ into R, is to download the repository. That is, click the big green button:
The big, green button saying &amp;ldquo;Clone or download&amp;rdquo;, click it and choose &amp;ldquo;download zip&amp;rdquo;.
Of course, for those using Git and Github, it would be appropriate to clone the repository. And, although appearing more advanced, cloning has the definitive advantage that you&amp;rsquo;ll enjoy the whole of the Github features.</description>
    </item>
    
    <item>
      <title>Simple (R-)Markdown template for &#39;Onepager-reports&#39; etc. </title>
      <link>/2016/10/05/template-onepager/</link>
      <pubDate>Wed, 05 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/05/template-onepager/</guid>
      <description>In my role as a teacher, I (have to) write a lot of marking feedback reports. My university provides a website to facilitate the process, that&amp;rsquo;s great. I have also been writing my reports with Pages, Word, or friends. But somewhat cooler, more attractive, and more reproducible would be using (a markup language such as) Markdown. Basically, that&amp;rsquo;s easy, but it would be of help to have a template that makes up a nice and nicely formatted report, like this:</description>
    </item>
    
    <item>
      <title>Using purrr to build a data frame of vectors (eg., from effect size statistics)</title>
      <link>/2016/09/29/purrr-effsize/</link>
      <pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/29/purrr-effsize/</guid>
      <description>I just tried to accomplish the following with R: Compute effect sizes for a variable between two groups. Actually, not one numeric variable but many. And compute not only one measure of effect size but several (d, lower/upper CI, CLES,&amp;hellip;).
So how to do that?
First, let&amp;rsquo;s load some data and some (tidyverse and effect size) packages:
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, message = FALSE)  library(purrr) library(ggplot2) library(dplyr) library(broom) library(tibble) library(compute.</description>
    </item>
    
    <item>
      <title>Summary for multiple variables using purrr</title>
      <link>/2016/09/28/summary-mult-cols-purrr/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/28/summary-mult-cols-purrr/</guid>
      <description>A frequent task in data analysis is to get a summary of a bunch of variables. Often, graphical summaries (diagrams) are wanted. However, at times numerical summaries are in order. How to get that in R? That&amp;rsquo;s the question of the present post.
Of course, there are several ways. One way, using purrr, is the following. I liked it quite a bit that&amp;rsquo;s why I am showing it here.
First, let&amp;rsquo;s load some data and some packages we will make use of.</description>
    </item>
    
    <item>
      <title>EDIT: Running multiple simple regressions with purrr</title>
      <link>/2016/09/26/edit-multiple_lm_purrr_edit/</link>
      <pubDate>Mon, 26 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/26/edit-multiple_lm_purrr_edit/</guid>
      <description>EDIT based on comments/ suggeestions from @JonoCarroll Disqus profile and @tjmahr twitter profile. See below (last step; look for &amp;ldquo;EDIT&amp;rdquo;).
Thanks for the input! :thumbsup:
reading time: 10 min.
Hadley Wickham&amp;rsquo;s purrr has given a new look at handling data structures to the typical R user (some reasoning suggests that average users doesn&amp;rsquo;t exist, but that&amp;rsquo;s a different story).
I just tried the following with purrr: - Meditate about the running a simple regression, FWIW - Take a dataframe with candidate predictors and an outcome - Throw one predictor at a time into the regression, where the outcome variable remains the same (i.</description>
    </item>
    
    <item>
      <title>Running multiple simple regressions with purrr</title>
      <link>/2016/09/23/multiple-lm-purrr2/</link>
      <pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/23/multiple-lm-purrr2/</guid>
      <description>Hadley Wickham&amp;rsquo;s purrr has given a new look at handling data structures to the typical R user (some reasoning suggests that average users don&amp;rsquo;t exist, but that&amp;rsquo;s a different story).
I just tried the following with purrr:
 Meditate about the running a simple regression, FWIW Take a dataframe with candidate predictors and an outcome Throw one predictor at a time into the regression, where the outcome variable remains the same (i.</description>
    </item>
    
    <item>
      <title>Code example for plotting boxplots instead of mean bars</title>
      <link>/2016/09/22/use-boxplots/</link>
      <pubDate>Thu, 22 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/22/use-boxplots/</guid>
      <description>On a recent psychology conference I had the impression that psychologists keep preferring to show mean values, but appear less interested in more detailled plots such as the boxplot. Plots like the boxplot are richer in information, but not more difficult to perceive.
For those who would like to have an easy starter on how to visualize more informative plots (more than mean bars), here is a suggestion:
# install.pacakges(&amp;quot;Ecdat&amp;quot;) library(Ecdat) # dataset on extramarital affairs data(Fair) str(Fair)  ## &#39;data.</description>
    </item>
    
    <item>
      <title>How to promote open science? Some practical recommendations</title>
      <link>/2016/09/22/openscience/</link>
      <pubDate>Thu, 22 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/22/openscience/</guid>
      <description>I just attended the biannual conference of the German society of psychology (DPGs) in Leipzig; open science was a central, albeit not undisputed topic; a lot of interesting related twitter discussion.
image source: Felix Sch√∂nbrodt
Interestingly, a strong voice of German scientiests uttered their concerns about being scooped if/when sharing their data (during the official meeting of the society). This being said (sad), the German research foundation (DFG) has updated its guidelines now stressing (more strongly) that publicly funded projects should share their data, with the rationale that the data do not belong to the individual scientiest but to the public, as the public funded it (I find that convincing).</description>
    </item>
    
    <item>
      <title>Fallstudie zur explorative Datenanalyse (YACSDA) beim Datensatz &#39;TopGear&#39;</title>
      <link>/2016/09/14/yacsda_topgear/</link>
      <pubDate>Wed, 14 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/14/yacsda_topgear/</guid>
      <description>YADCSDA in German language.
In dieser Fallstudie (YACSDA: Yet another case study of data analysis) wird der Datensatz TopGear analysiert, vor allem mit grafischen Mitteln. Es handelt sich weniger um einen &amp;ldquo;Rundumschlag&amp;rdquo; zur Beantwortung aller m√∂glichen interessanten Fragen (oder zur Demonstration aller m√∂glichen Analysewerkzeuge), sondern eher um einen Einblick zu einfachen explorativen Verfahren.
library(robustHD)  ## Loading required package: perry  ## Loading required package: parallel  ## Loading required package: robustbase  data(TopGear) # Daten aus Package laden library(tidyverse)  Numerischer √úberblick glimpse(TopGear)  ## Observations: 297 ## Variables: 32 ## $ Maker &amp;lt;fctr&amp;gt; Alfa Romeo, Alfa Romeo, Aston Martin, Asto.</description>
    </item>
    
    <item>
      <title>Why Likert scales are (in general) not metric</title>
      <link>/2016/09/07/likert-not-metric/</link>
      <pubDate>Wed, 07 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/07/likert-not-metric/</guid>
      <description>Likert scales are psychologists&amp;rsquo; bread-and-butter tool. Literally, thousands (!) of such &amp;ldquo;scales&amp;rdquo; (as they are called, rightfully or not) do exist. To get a feeling: The APA links to this database where 25,000 tests are listed (as stated by the website)! That is indeed an enormous number.
Most of these psychological tests use so called Likert scales (see this Wikipedia article). For example:
(Source: Wikipedia by Nicholas Smith)
Given their widespread use, the question how useful such tests are has arisen many times; see here, here, or here.</description>
    </item>
    
    <item>
      <title>Why is SD(X) unequal to MAD(X)?</title>
      <link>/2016/08/31/why-sd-is-unequal-to-mad/</link>
      <pubDate>Wed, 31 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/31/why-sd-is-unequal-to-mad/</guid>
      <description>MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]} });   It may seem bewildering that the standard deviation (sd) of a vector X is (generally) unequal to the mean absolute deviation from the mean (MAD) of X, ie.
$$sd(X) \ne MAD(X)$$.
One could now argue this way: well, sd(X) involves computing the mean of the squared $$x_i$$, then taking the square root of this mean, thereby &amp;ldquo;coming back&amp;rdquo; to the initial size or dimension of x (i.</description>
    </item>
    
    <item>
      <title>Plot of mean with exact numbers using ggplot2</title>
      <link>/2016/08/30/plot_dot_means/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/30/plot_dot_means/</guid>
      <description>Often, both in academic research and more business-driven data analysis, we want to compare some (two in many cases) means. We will not discuss here that friends should not let friends plot barplots. Following the advise of Cleveland&amp;rsquo;s seminal book we will plot the means using dots, not bars.
However, at times we do not simply want the diagram, but we (or someone) is interested in the bare, plain, naked, exact numbers too.</description>
    </item>
    
    <item>
      <title>Shading multiple areas under normal curve</title>
      <link>/2016/08/30/shade_normal_curve/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/30/shade_normal_curve/</guid>
      <description>When plotting a normal curve, it is often helpful to color (or shade) some segments. For example, often we might want to indicate whether an absolute value is greater than 2.
How can we achieve this with ggplot2? Here is one way.
First, load packages and define some constants. Specifically, we define mean, sd, and start/end (z-) value of the area we want to shade. And your favorite color is defined.</description>
    </item>
    
    <item>
      <title>Simple way to plot a normal distribution with ggplot2</title>
      <link>/2016/08/30/normal_curve_ggplot2/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/30/normal_curve_ggplot2/</guid>
      <description>Plotting a normal distribution is something needed in a variety of situation: Explaining to students (or professors) the basic of statistics; convincing your clients that a t-Test is (not) the right approach to the problem, or pondering on the vicissitudes of life&amp;hellip;
If you like ggplot2, you may have wondered what the easiest way is to plot a normal curve with ggplot2?
Here is one:
library(cowplot)  ## Loading required package: ggplot2  ## ## Attaching package: &#39;cowplot&#39;  ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave  p1 &amp;lt;- ggplot(data = data.</description>
    </item>
    
    <item>
      <title>Why absolute correlation value (r) cannot exceed 1. An intuition.</title>
      <link>/2016/08/28/why-abs-correlation-is-max-1/</link>
      <pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/28/why-abs-correlation-is-max-1/</guid>
      <description>Pearson&amp;rsquo;s correlation is a well-known and widely used instrument to gauge the degree of linear association of two variables (see this post for an intuition on correlation).
There a many formulas for correlation, but a short and easy one is this one:
$$r = \varnothing(z_x z_y)$$.
In words, $$r$$ can be seen as the average product of z-scores.
In &amp;ldquo;raw values&amp;rdquo;, r is given by
$$ r = \frac{\frac{1}{n}\sum{\Delta X \Delta Y}}{\sqrt{\frac{1}{n}\sum{\Delta X^2}} \sqrt{\frac{1}{n}\sum{\Delta Y^2}}} $$.</description>
    </item>
    
    <item>
      <title>The effect of a status symbol on success in online dating: an experimental study (data paper)</title>
      <link>/2016/08/27/data_status_dating/</link>
      <pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/27/data_status_dating/</guid>
      <description>This article has been published at The Winnower, it is distributed under the terms of the Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and redistribution in any medium, provided that the original author and source are credited.
Data can be accessed here.
Access the paper here.
CITATION: Sebastian Sauer, Alexander Wolff, The effect of a status symbol on success in online dating: an experimental study (data paper), The Winnower 3:e147241.</description>
    </item>
    
    <item>
      <title>Multiple t-Tests with dplyr</title>
      <link>/2016/08/18/multiple-t-tests-with-dplyr/</link>
      <pubDate>Thu, 18 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/18/multiple-t-tests-with-dplyr/</guid>
      <description>t-Test on multiple columns Suppose you have a data set where you want to perform a t-Test on multiple columns with some grouping variable. As an example, say you a data frame where each column depicts the score on some test (1st, 2nd, 3rd assignment&amp;hellip;). In each row is a different student. So you glance at the grading list (OMG!) of a teacher!
How to do do that in R?</description>
    </item>
    
    <item>
      <title>Introduction to the measurement theory, and conjoint measurement theory</title>
      <link>/2016/08/17/intro_measurement/</link>
      <pubDate>Wed, 17 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/17/intro_measurement/</guid>
      <description>What is measurement? Why should I care?
Measurement is a basis of an empirical science. Image a geometer (a person measuring distances on the earth) with a metering rul made of rubber! Poor guy! Without proper measurement, even the smartest theory cannot be expected to be found, precisely because it cannot be measured.
So, what exactly is measurement? Measurement can be seen as tying numbers to empirical objects. But not in some arbritrary style.</description>
    </item>
    
    <item>
      <title>Looping through dataframe columns using purrr::map()</title>
      <link>/2016/08/16/looping-purrr/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/16/looping-purrr/</guid>
      <description>Let&amp;rsquo;s get purrr. Recently, I ran across this issue: A data frame with many columns; I wanted to select all numeric columns and submit them to a t-test with some grouping variables.
As this is a quite common task, and the purrr-approach (package purrr by @HadleyWickham) is quite elegant, I present the approach in this post.
Let&amp;rsquo;s load the data, the Affairs data set, and some packages:
data(Affairs, package = &amp;quot;AER&amp;quot;) library(purrr) # functional programming library(dplyr) # dataframe wrangling library(ggplot2) # plotting library(tidyr) # reshaping df  Don&amp;rsquo;t forget that the four packages need to be installed in the first place.</description>
    </item>
    
    <item>
      <title>Intuition on correlation</title>
      <link>/2016/07/25/correlation-intuition/</link>
      <pubDate>Mon, 25 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/25/correlation-intuition/</guid>
      <description>reading time: 10 min.
Pearson‚Äôs correlation (short: correlation) is one of statistics‚Äô all time classics. With an age of about a century, it is some kind of grand dad of analytic tools ‚Äì but an oldie who is still very busy!
Formula, interpretation and application of correlation is well known.
In some non-technical lay terms, correlation captures the (linear) degree of co-variation of two linear variables. For example: if tall people have large feet (and small people small feet), on average, we say that height and foot size are correlated.</description>
    </item>
    
    <item>
      <title>Practical data cleansing in R</title>
      <link>/2016/07/24/data-cleansing/</link>
      <pubDate>Sun, 24 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/24/data-cleansing/</guid>
      <description>What is ‚Äúdata cleansing‚Äù about?
Data analysis, in practice, consists typically of some different steps which can be subsumed as ‚Äúpreparing data‚Äù and ‚Äúmodel data‚Äù (not considering communication here):
(Inspired by this)
Often, the first major part ‚Äî ‚Äúprepare‚Äù ‚Äî is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!). In practice, one rather has to get his (her) hands dirt‚Ä¶</description>
    </item>
    
    <item>
      <title>Yet another case study on data analysis (YACSDA) ‚Äì extramarital affairs data set</title>
      <link>/2016/07/23/affairs/</link>
      <pubDate>Sat, 23 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/23/affairs/</guid>
      <description>Ok, there are heaps of them on the net. Here comes my YACSDA. Maybe the only thing about it to mention is that it comes in German language.
 Analytical language: R (3.3) Purpose: Demonstrate basic exploratory and modeling techniques Packages used: dplyr, ggplot2 Data set: Affair; source R package COUNT Analytical topics covered: descriptive statistics, visualization, liner model, logistic linear model Reproducibility: Rmarkdown, knitr, github  Code on Github</description>
    </item>
    
    <item>
      <title>Why metric scale level cannot be taken for granted</title>
      <link>/2016/07/21/measurement-01/</link>
      <pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/21/measurement-01/</guid>
      <description>One main business for psychologists is to examine questionnaire data. Extraversion, intelligence, attitudes‚Ä¶ That‚Äôs bread-and-butter job for (research) psychologists.
Similarly, it is common to take the metric level of questionnaire data for granted. Well, not for the item level, it is said. But for the aggregated level, oh yes, that‚Äôs OK.
Despite its popularity, the measurement basics of such practice are less clear. On which grounds can this comfortable practice be defended?</description>
    </item>
    
    <item>
      <title>What to read in summer (German)</title>
      <link>/2016/07/20/what-to-read/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/20/what-to-read/</guid>
      <description>Below some consideration on what to read in summer times. In German language.
Lesezeit/reading time: 10-15 Min.
Literaturempfehlung Sommer 2016
Was soll ich lesen? Sommer, Sonne, Sonnenschein ‚Äî ab in den S√ºden. Die Zeile ‚ÄúLesen, lesen, lesen, lesen‚Äù w√ºrde sich nach meinem Daf√ºrhalten auch ganz gut in den Song einpassen. Daf√ºr hier ein paar Literaturempfehlungen. Von einer anst√§ndigen Sommerlekt√ºre erwarte ich zweierlei: Dass die Kunst unterhaltsam sei. Zweitens, wenn als der Dampf sich nach dem Lesen erhebt, dass etwas zur√ºckbleibt, au√üer dem Dampf.</description>
    </item>
    
    <item>
      <title>Case study on data wrangling with dplyr (German)</title>
      <link>/2016/07/18/nycflights13/</link>
      <pubDate>Mon, 18 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/18/nycflights13/</guid>
      <description>reading time (full): 30 min.
Data Wrangling with dplyr is a popular activity in data science/ statistics. A number of tutorial are available, but not so many in German language.
Data set analyzed in nycflights13::flights (R package). Available on CRAN. Ok, choosing this data set is not very creative, but, hey, quite nice data:)
Thus, here is a case study in German language; code &amp;reg;is on Github.</description>
    </item>
    
    <item>
      <title>Intuition on Cohen&#39;s d</title>
      <link>/2016/07/15/cohens-d-intuition/</link>
      <pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/15/cohens-d-intuition/</guid>
      <description>reading time: 5-10 min.
Cohen&amp;rsquo;s d is a widely known and extensively used measure of effect size. That is, d is used to gauge how strong an effect is (given the fact that the effect exists). For example, one way to estimate d is as follows:
data(tips, package = &amp;quot;reshape2&amp;quot;) library(compute.es) t1 &amp;lt;- t.test(tip ~ sex, data = tips) t1$statistic  ## t ## -1.489536  table(tips$sex)  ## ## Female Male ## 87 157  tes(t1$statistic, 87, 157)  ## Mean Differences ES: ## ## d [ 95 %CI] = -0.</description>
    </item>
    
    <item>
      <title>How to add a logo to a slidify presentation</title>
      <link>/2016/07/05/slidify-logo/</link>
      <pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/05/slidify-logo/</guid>
      <description>reading time: 15-20 min.
Slidify is a cool tool to render HTML5 slide decks, see here, here or here for examples.
Features include:
 reproducibility. You write your slide deck as you would write any other text, similar to Latex/Beamer. But you write using Markdown, which is easier and less clumsy. As you write plain text, you are free to use git. modern look. Just a website, nothing more. But with cool, modern features.</description>
    </item>
    
    <item>
      <title>Long vs. wide format, and gather()</title>
      <link>/2016/07/04/gather-long-to-wide-format/</link>
      <pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/04/gather-long-to-wide-format/</guid>
      <description>reading time: 10 min.
A quite common task in data analysis is to change a dataset from wide to long format.
For example, this is a dataset in wide format:
Is is called wide, as, well, it is wide ‚Äì several columns side by side.
For example, assume, we have measured a number of predictors (here: predictor_1, predictor_2, predictor_3), and an outcome measure (here: outcome). In this case, each variable is dichotomous (either yes or no).</description>
    </item>
    
    <item>
      <title>Cross-tabulate multiple variables</title>
      <link>/2016/07/03/cross-tabulate-multiple-variables/</link>
      <pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/03/cross-tabulate-multiple-variables/</guid>
      <description>reading time: 15-20 min.
Recently, I analyzed some data of a study where the efficacy of online psychotherapy was investigated. The investigator had assessed whether or not a participant suffered from some comorbidities (such as depression, anxiety, eating disorder‚Ä¶).
I wanted to know whether each of these (10 or so) comorbidities was associated with the outcome (treatment success, yes vs. no).
Of course, an easy solution would be to ‚Äúhalf-manually‚Äù check the association, eg.</description>
    </item>
    
    <item>
      <title>Why have z-transformed values a mean of zero and a sd of 1?</title>
      <link>/2016/07/02/z-value-intuition/</link>
      <pubDate>Sat, 02 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/02/z-value-intuition/</guid>
      <description>z-transformation is an ubiquitous operation in data analysis. It is often quite practical.
Example: Assume Dr Zack scored 42 points on a test (say, IQ). Average score is 40 in the relevant population, and SD is 1, let‚Äôs say. So Zack‚Äôs score is 2 points above average. 2 points equals to SDs in this example. We can thus safely infer that Zack is about 2 SDs above average (leaving measurement precision and other issues at side).</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Sun, 20 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>I blog about data science, particularly using R, and with an applied interest to social sciences.
As a non-virtual person, I work as a professor at FOM University of Applied Sciences.
Posts reflect mostly my current thinking; and posts are not immune to thought updates. With luck things get less wrong in the course of time. All opions are my own. Faults are my own.
See also my Google Scholar profile and my ORCID.</description>
    </item>
    
  </channel>
</rss>