<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.78.2" />


<title>Spell out your model explicitly - Data Se</title>
<meta property="og:title" content="Spell out your model explicitly - Data Se">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo-data-se.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/">Blog</a></li>
    
    <li><a href="/privacy/">Data privacy</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">Spell out your model explicitly</h1>

    
    <span class="article-date">2020/06/10</span>
    

    <div class="article-content">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="load-packages" class="section level1">
<h1>Load packages</h1>
<pre class="r"><code>library(tidyverse)
library(hrbrthemes)
library(MASS)
library(moments)</code></pre>
</div>
<div id="why-you-should-spell-out-your-model-explicitly" class="section level1">
<h1>Why you should spell out your model explicitly</h1>
<p>Often, assumptions of widely used models, such as linear models, appear opaque. Why is heteroscedasticity important? Where is a list of the model assumptions I need to consider?</p>
<p>As it turns out, there are straight forward answers to these (and similar) questions. The solution is to explicitly spell out your model. All “assumptions” can easily read off from these model specifications.</p>
</div>
<div id="example-for-explicit-and-complex-model" class="section level1">
<h1>Example for explicit (and complex) model</h1>
<p>Say, we would like to model Covid-19 cases (<span class="math inline">\(C\)</span>) as a function of tests applied in some population. We could specify our model like this:</p>
<p><span class="math display">\[
C  \sim \mathcal{P}(\lambda_i) \\
\text{log}\lambda_i = \alpha + \beta_1\,x_{i}^{s} \\
\alpha \sim \mathcal{E}(.5) \\
\beta_1 \sim \mathcal{E}(0.1)
\]</span></p>
<p>Here, we model <span class="math inline">\(C\)</span> as Poisson (<span class="math inline">\(\mathcal{P}\)</span>) distribution variable, because <span class="math inline">\(C\)</span> is a count variable, and Poisson is a maximum entropy variable for counts. However, this is a apriori reasoning; after seeing the data we might want to consider different model despite the danger of overfitting.</p>
<p>Recall that <span class="math inline">\(\lambda\)</span> must be positive or null. That’s why we put a “log” before the likelihood (the linear model specification) line. Similarly, we can only take the log of a positive number, so we need to make sure that <span class="math inline">\((\alpha + \beta_1 \, X )&gt; 0\)</span>. Here, the Exponential distribution (<span class="math inline">\(\mathcal{E}\)</span>) is handy, as it makes sure that <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> ist positive.</p>
<p>It’s helpful to use standardized predictors, <span class="math inline">\(x_i^s\)</span>, because that allows us to model the intercept <span class="math inline">\(\alpha\)</span> as the mean value of <span class="math inline">\(C\)</span>, among other things. It would be a fruitful discussion to contemplate the usefulness of the mean for long-tail distributions, see <a href="https://nassimtaleb.org/2020/01/final-version-fat-tails/">Taleb Incerto</a>, but that’s beyond the scope of this post.</p>
<p>We must take care of the log-normal (<span class="math inline">\(\mathcal{LN}\)</span>), because log-normal are fat-tailed so the mean will be large. Seeing is believing, so let’s consider <span class="math inline">\(A \sim \mathcal{LN}(0, 10)\)</span>. Draw some samples from that population:</p>
<pre class="r"><code>set.seed(42)
a &lt;- rlnorm(n = 1000, meanlog = 0, sdlog = 10)
options(scipen = 3)
mean(a)
#&gt; [1] 1716379931934

format(mean(a), scientific = T)
#&gt; [1] &quot;1.71638e+12&quot;</code></pre>
<p>For a smaller sd:</p>
<pre class="r"><code>set.seed(42)
a &lt;- rlnorm(n = 1000, meanlog = 4, sdlog = 1)
options(scipen = 3)
mean(a)
#&gt; [1] 88.55222

format(mean(a), scientific = T)
#&gt; [1] &quot;8.855222e+01&quot;</code></pre>
<p>Let’s apply the same reasoning for <span class="math inline">\(\beta_{1}\)</span>.</p>
</div>
<div id="read-off-modelling-assumptions" class="section level1">
<h1>Read off modelling assumptions</h1>
<p>Some of the “assumptions” implied by the above model are:</p>
<ul>
<li>Certain distributions of the variables involved</li>
<li>Certain function forms map the input variable to the output variable</li>
<li>The conditional errors (conditional to <span class="math inline">\(\hat{Y}\)</span>) are iid - independent and identically distributed, that is, at each value of <span class="math inline">\(\hat{Y}\)</span> the errors are drawn from the same distribution, and they are drawn without correlation (eg., drawn randomly from an infinite population). This fact incorporated heteroscedasticity, as the residuals have the same variance irrespective to <span class="math inline">\(\hat{Y}\)</span>.</li>
<li>Our ignorance according to the “true” value of some variable is incorporated. For example, we are not sure about the true value of <span class="math inline">\(\beta_1\)</span>, there is uncertainty involved, which is reflected by the fact that <span class="math inline">\(\beta_1\)</span> is drawn from a probability distribution.</li>
</ul>
</div>
<div id="simulate-data" class="section level1">
<h1>Simulate data</h1>
<p>Say, <span class="math inline">\(n=1000\)</span></p>
<p>According to our model:</p>
<pre class="r"><code>set.seed(42)
n &lt;- 1000
alpha &lt;- rexp(n = 1000, rate = 1)
beta &lt;- rexp(n = 1000, rate = 0.1)</code></pre>
<p>See:</p>
<pre class="r"><code>ggplot(data.frame(alpha)) +
  aes(x = alpha) +
  geom_density()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data.frame(beta)) +
  aes(x = beta) +
  geom_density()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-5-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>As we do not have real data <span class="math inline">\(X\)</span>, we need to simulate it. Assuming that <span class="math inline">\(X\)</span> is again exponentially distributed (to avoid negative values):</p>
<pre class="r"><code>set.seed(42)
X &lt;- rexp(1000, rate = .5)</code></pre>
<pre class="r"><code>ggplot(data.frame(X)) +
  aes(x = X) +
  geom_density() +
  theme_ipsum_rc()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="compute-the-likelihood" class="section level1">
<h1>Compute the likelihood</h1>
<pre class="r"><code>lambda_log &lt;- (alpha + beta*X) %&gt;% log()

lambda &lt;- exp(lambda_log)</code></pre>
<pre class="r"><code>ggplot(data.frame(lambda)) +
  aes(x = lambda) +
  geom_density()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Oh my, that’s a long tail.</p>
</div>
<div id="estimate-c" class="section level1">
<h1>Estimate <span class="math inline">\(C\)</span></h1>
<pre class="r"><code>set.seed(42)
C &lt;- rpois(n = 1000, lambda = lambda)</code></pre>
</div>
<div id="put-the-data-together" class="section level1">
<h1>Put the data together</h1>
<pre class="r"><code>C_df &lt;- tibble(
  C = C,
  lambda = lambda,
  lambda_log = lambda_log,
  X = X,
  alpha = alpha,
  beta = beta)</code></pre>
<pre class="r"><code>ggplot(C_df) +
  aes(x= C) +
  geom_density() +
  theme_ipsum_rc(grid = &quot;Y&quot;)</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-12-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="fit-distributions" class="section level1">
<h1>Fit distributions</h1>
<pre class="r"><code>pois_param &lt;- fitdistr(C_df$C, 
                       densfun = &quot;poisson&quot;)$estimate
str(pois_param)
#&gt;  Named num 23.2
#&gt;  - attr(*, &quot;names&quot;)= chr &quot;lambda&quot;

pois_param
#&gt; lambda 
#&gt; 23.184</code></pre>
<p>Supported distributions are:</p>
<blockquote>
<p>“beta”, “cauchy”, “chi-squared”, “exponential”, “gamma”, “geometric”, “log-normal”, “lognormal”, “logistic”, “negative binomial”, “normal”, “Poisson”, “t” and “weibull”</p>
</blockquote>
<p>For comparison, let’s fit another distribution, say a log-normal.</p>
<pre class="r"><code>lnorm_param &lt;- fitdistr(C_df$C[C_df$C &gt; 0], 
                        densfun = &quot;lognormal&quot;)$estimate
lnorm_param
#&gt;  meanlog    sdlog 
#&gt; 2.389367 1.337632</code></pre>
<p>… or a Gamma:</p>
<pre class="r"><code>gamma_param &lt;- fitdistr(C_df$C[C_df$C &gt; 0], 
                        densfun = &quot;gamma&quot;)$estimate
gamma_param
#&gt;      shape       rate 
#&gt; 0.71162128 0.02796269</code></pre>
</div>
<div id="compare-fit" class="section level1">
<h1>Compare fit</h1>
<p>To compare fit, a qqplot can be helpful.</p>
<div id="poisson" class="section level2">
<h2>Poisson</h2>
<pre class="r"><code>C_df %&gt;% 
  ggplot(aes(sample = C)) +
  geom_qq(distribution = qpois, 
          dparams = as.list(pois_param),
          color = &quot;dodgerblue&quot;) +
  stat_qq_line(distribution = qpois, 
               dparams = as.list(pois_param),
               color = &quot;dodgerblue&quot;) +
  theme_ipsum_rc()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-16-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="gamma" class="section level2">
<h2>Gamma</h2>
<pre class="r"><code>C_df %&gt;% 
  ggplot(aes(sample = C)) +
  geom_qq(distribution = qgamma, 
          dparams = as.list(gamma_param),
          color = &quot;darkseagreen&quot;) +
  stat_qq_line(distribution = qgamma, 
               dparams = as.list(gamma_param),
               color = &quot;darkseagreen&quot;) +
  theme_ipsum_rc()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-17-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="log-normal" class="section level2">
<h2>Log-normal</h2>
<pre class="r"><code>C_df %&gt;% 
  ggplot(aes(sample = C)) +
  geom_qq(distribution = qlnorm, 
          dparams = as.list(lnorm_param),
          color = &quot;firebrick&quot;) +
  stat_qq_line(distribution = qlnorm, 
               dparams = as.list(lnorm_param),
               color = &quot;firebrick&quot;)  +
  theme_ipsum_rc()</code></pre>
<p><img src="/post/2020-06-10-spell-out-your-model-explicitly_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>As can be seen, our distributions do not fit well, even given the fact that we simulated the data to be Poisson distributed. This may be due to the fact that we have induced such a long tail. Maybe there are other things going on. Any ideas? Let me know.</p>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-data-se-netlify-com.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

