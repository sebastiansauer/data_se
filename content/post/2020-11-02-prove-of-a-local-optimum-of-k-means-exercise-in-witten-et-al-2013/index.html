---
title: Prove of a local optimum of k-means (exercise in Witten et al., 2013)
author: Sebastian Sauer
date: '2020-11-02'
slug: prove-of-a-local-optimum-of-k-means-exercise-in-witten-et-al-2013
categories:
  - rstats
tags:
  - math
---

<script src="index_files/header-attrs/header-attrs.js"></script>


<div id="load-packages" class="section level1">
<h1>Load packages</h1>
<pre class="r"><code>library(tidyverse)</code></pre>
<p>The <em>K-Means optimization</em> reduces the variance in each iteration. To illuminate on that <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">Witten et al.</a> in <em>An Introduction to Statistical Learning (2013)</em> present the following entity (p. 388, chap. 10):</p>
<p><span class="math display">\[\frac{1}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{ij} - x_{i^\prime j})^2 = 
2 \sum\limits_{i \in C_k} \sum\limits_{j=1}^{p} (x_{ij} - \bar{x}_{kj})^2\]</span></p>
<p>A proof can be found <a href="https://blog.princehonest.com/stat-learning/ch10/1.html">here</a>; I’ll add some explanations.</p>
<p>Note 1. Note that <span class="math inline">\(\sum\limits_{i,i^{\prime} \in C_k}(\dots)\)</span> essentially amounts to
<span class="math inline">\(\sum\limits_{i \in C_k}\sum\limits_{i^{\prime} \in C_k}(\dots)\)</span>, when the order of summation does not matter.</p>
<p>Define <span class="math inline">\(TSS\)</span> (total sum of deviation squares) in the following way: <span class="math inline">\(TSS \equiv \frac{1}{|C_k|} \sum\limits_{i, \in C_k} \sum\limits_{j=1}^p (x_{ij} - x_{i^\prime j})^2\)</span>.</p>
<p>Define <span class="math inline">\(ISS\)</span> (individual sum of deviation squares) in the following way: <span class="math inline">\(ISS \equiv (x_{ij} - x_{i^\prime j})^2\)</span>, for some <span class="math inline">\(i\)</span>.</p>
<p>Note 2. Note that the mean sum of deviation squares amounts to <span class="math inline">\(MSS \equiv \frac{1}{|C_k|} TSS\)</span>. Note further that <span class="math inline">\(TSS = |C_K| \cdot MSS\)</span>.</p>
<div id="add--barx_kj-barx_kj" class="section level2">
<h2>Add <span class="math inline">\(-\bar{x}_{kj}+ \bar{x}_{kj}\)</span></h2>
<p>As <span class="math inline">\(-\bar{x}_{kj}+ \bar{x}_{kj} = 0\)</span> we can safely add this term to the equation.</p>
</div>
<div id="expand-the-binomial-formula" class="section level2">
<h2>Expand the binomial formula</h2>
<p><span class="math display">\[= \frac{1}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p ((x_{ij} - \bar{x}_{kj}) - (x_{i^\prime j} - \bar{x}_{kj}))^2\]</span></p>
</div>
<div id="expand-the-sum" class="section level2">
<h2>Expand the sum</h2>
<p>We can make use of note 2 in order to get rid of one summation.</p>
<p><span class="math display">\[= \frac{1}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p ((x_{ij} - \bar{x}_{kj})^2 - 2 (x_{ij} - \bar{x}_{kj})(x_{i^\prime j} - \bar{x}_{kj}) + (x_{i^\prime j} - \bar{x}_{kj})^2)\]</span></p>
</div>
<div id="deviation-from-mean-sum-to-to-zero" class="section level2">
<h2>Deviation from mean sum to to zero</h2>
<p>Note that the third term sums up to zero for uncorrelated variables; this term is analogous to the covariance.</p>
<p><span class="math display">\[= \frac{|C_k|}{|C_k|} \sum\limits_{i \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})^2 +
  \frac{|C_k|}{|C_k|} \sum\limits_{i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{i^\prime j} - \bar{x}_{kj})^2 -
  \frac{2}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})(x_{i^\prime j} - \bar{x}_{kj})
\\
= 2 \sum\limits_{i \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})^2 + 0
\]</span></p>
</div>
</div>
