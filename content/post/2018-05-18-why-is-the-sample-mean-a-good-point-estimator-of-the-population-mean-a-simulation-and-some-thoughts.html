---
title: Why is the sample mean a good point estimator of the population mean? A simulation
  and some thoughts.
author: Sebastian Sauer
date: '2018-05-18'
slug: why-is-the-sample-mean-a-good-point-estimator-of-the-population-mean-a-simulation-and-some-thoughts
categories:
  - rstats
tags:
  - inference
  - simulation
  - rstats
---



<p>It is frequently stated that the sample mean is a good or even the best point estimator of the according population value. But why is that? In this post we are trying to get an intuition by using simulation inference methods.</p>
<p>Assume you played throwing coins with some one at some dark corner. “Some one” throws the coin 10 times, and wins 8 times (he was betting on heads, but that’s only for the sake of the story). With boiling suspicion (and empty pockets) you head home. Did this guy cheated? Was he playing tricks on you? It appear that the coin was unfair, ie., biased? It appears that 80% is a good estimator of the “true” value of the coin (for heads). But let’s try to see better why this is plausible or even rationale.</p>
<p>Let’s turn the question around, and assume for the moment that 80% <em>is</em> the true parameter, <span class="math inline">\(\pi=.8\)</span>. If we were to draw many samples from this distribution (binomial with parameter <span class="math inline">\(\pi = .8\)</span>), how would our sample distribtion look like?</p>
<p>Let’s try it.</p>
<pre class="r"><code>library(mosaic)</code></pre>
<pre><code>## Loading required package: dplyr</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggformula</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## 
## New to ggformula?  Try the tutorials: 
##  learnr::run_tutorial(&quot;introduction&quot;, package = &quot;ggformula&quot;)
##  learnr::run_tutorial(&quot;refining&quot;, package = &quot;ggformula&quot;)</code></pre>
<pre><code>## Loading required package: mosaicData</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Note: If you use the Matrix package, be sure to load it BEFORE loading mosaic.</code></pre>
<pre><code>## 
## Attaching package: &#39;mosaic&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean</code></pre>
<pre><code>## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median,
##     prop.test, quantile, sd, t.test, var</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum</code></pre>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ tibble  1.4.2     ✔ purrr   0.2.4
## ✔ tidyr   0.8.0     ✔ stringr 1.3.1
## ✔ readr   1.1.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ mosaic::count() masks dplyr::count()
## ✖ purrr::cross()  masks mosaic::cross()
## ✖ mosaic::do()    masks dplyr::do()
## ✖ tidyr::expand() masks Matrix::expand()
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ✖ mosaic::tally() masks dplyr::tally()</code></pre>
<pre class="r"><code>pi = .8</code></pre>
<p>First, we let R draw 10000 samples of size <span class="math inline">\(n=10\)</span> from the (<span class="math inline">\(H_0\)</span>) distribution where <span class="math inline">\(\pi = .8\)</span>:</p>
<pre class="r"><code>h0_samples &lt;- do(10000) * rflip(n = 10, prob = .8)</code></pre>
<p>Plotting it gives us a “big picture” of typial and rare events:</p>
<pre class="r"><code>gf_bar(~heads, data = h0_samples) +
  scale_x_continuous(breaks = 0:10)</code></pre>
<p><img src="/post/2018-05-18-why-is-the-sample-mean-a-good-point-estimator-of-the-population-mean-a-simulation-and-some-thoughts_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>As can be seen, 8 (the sample value) is the most frequent value. In more elaborate terms, 8 is the most probable value if many samples of this size are drawn from the <span class="math inline">\(H_0\)</span> distribution. Divide all values by 10 (the sample size), and you will get mean values:</p>
<pre class="r"><code>h0_samples %&gt;% 
  mutate(heads_avg = heads / 10) %&gt;% 
  summarise(heads_avg = mean(heads_avg),
            heads = mean(heads))</code></pre>
<pre><code>##   heads_avg heads
## 1    0.7991 7.991</code></pre>
<p>Similarly, say we draw many samples of <span class="math inline">\(n=30\)</span> from a normal distribution with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>, the most frequent mean will be (approximately) <span class="math inline">\(\mu\)</span>:</p>
<pre class="r"><code>h0_samples_norm &lt;- 
  do(10000) * mean(rnorm(n = 30, mean = 0, sd = 1))</code></pre>
<pre class="r"><code>gf_histogram(~mean, data = h0_samples_norm)</code></pre>
<p><img src="/post/2018-05-18-why-is-the-sample-mean-a-good-point-estimator-of-the-population-mean-a-simulation-and-some-thoughts_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Intuitively, there is some plausibility in the results: The population mean is the most frequent value, hence it should appear often (most often) if we draw many samples.</p>
<p>Now we argue as follows:</p>
<p><em>If</em> the data are distributed as given by the <span class="math inline">\(H_0\)</span>, <em>then</em> the most frequent value of the sample distribution will be the most likely value in the population.</p>
<p>But are <em>my</em> data really drawn from this distribution? There are infinitely many others. Why just this one, one may counter. Good point. One could argue that our conclusion is not warranted, because we have no evidence of preferring one distribution in favor of some other. One the other hand, some argue that all we know for sure is our data. And our data favors one certain value. So there is some rationality in choosing this value, because we know no other value for sure (actually we don’t have no other values at all).</p>
<p>We could ofcourse assign some probability on each distribution we deem plausible. Then we would get a distribution of parameters. That’s the Bayesian way. For the sake of simplicity, we will not pursue this path further in this post (although it is a great path).</p>
<p>Coming back to the coin examples; let’s do some more “hard” reasoning. It is well known that the distribution of a fair coin, flipped 10 times, follows this distribution:</p>
<p><span class="math inline">\(p(k|\pi, n) = \binom {n}{k} \cdot \pi^k\cdot (1-\pi)^{n-k}\)</span></p>
<p>Given the fact that <span class="math inline">\(\pi=.8\)</span> how probable is, say, 8 hits? This probability is called likelihood; this function is called likelihood function. The likelihoods sum up to more than 100%.</p>
<p>The binomial coefficient is the same as <code>chooose(n, k)</code>, in R.</p>
<pre class="r"><code>choose(n = 10, k = 8) * .8^8 * (1-.8)^(10-8)</code></pre>
<pre><code>## [1] 0.3019899</code></pre>
<p>More convenient is this off-the-shelf function.</p>
<pre class="r"><code>dbinom(x = 8, size = 10, prob = .8)</code></pre>
<pre><code>## [1] 0.3019899</code></pre>
<p>Let’s compute that for each of the possible 11 hits (0 to 10):</p>
<pre class="r"><code>0:10 %&gt;% 
  dbinom(x = ., size = 10, prob = .8) %&gt;% 
  as_tibble() %&gt;% 
  add_column(hits = 0:10) %&gt;% 
  gf_col(value~hits, data = .) +
  scale_x_continuous(breaks = 0:10)</code></pre>
<p><img src="/post/2018-05-18-why-is-the-sample-mean-a-good-point-estimator-of-the-population-mean-a-simulation-and-some-thoughts_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Looks similar to our simulation above, doesn’t it? Reassuring. Again, 8 of 10 hits is the most frequent.</p>
<p>Somewhat more stringent, we could compute the maximum of the likelihood function. To get the maximum of a function, one computes the first derivation, and then sets it to zero.</p>
<p>To make computation easier, we can take the logarithm of the likelihood function; taking the logarithm will not change the position of the maximum. But products will become sums by the log function, hence computation will become easier:</p>
<p><span class="math inline">\(\ln p(k| \pi, n) = \ln \binom {n} {k} + k \ln \pi + (n-k) \ln (1-\pi)\)</span></p>
<p>Now we take the first derivation with respect to <span class="math inline">\(\pi\)</span>, regarding the other variables as constants:</p>
<p><span class="math inline">\(\frac{d \ln p(k| \pi, n)}{d\pi} = 0 + k \frac{1}{p} + (n-k) \frac {1}{1-\pi}(-1)\)</span></p>
<p>Setting this term to zero:</p>
<p><span class="math inline">\(\frac{k}{\pi} + (n-k)\frac{-1}{1-\pi}=0\)</span></p>
<p>Moving the second term to the rhs:</p>
<p><span class="math inline">\(\frac{k}{\pi}=(n-k)\frac{-1}{1-\pi}\)</span></p>
<p>Multiplying by <span class="math inline">\(\frac{\pi}{1-\pi}\)</span>:</p>
<p><span class="math inline">\(k(1-\pi) = (n-k)\pi\)</span></p>
<p>Multiplying out:</p>
<p><span class="math inline">\(k - k\pi = n\pi - k\pi\)</span></p>
<p>Adding <span class="math inline">\(k\pi\)</span> yields:</p>
<p><span class="math inline">\(k = n\pi\)</span>, rearringing:</p>
<p><span class="math inline">\(\pi = \frac{k}{n}\)</span></p>
<p>Hence, the relative number of hits (in the sample) is the best estimator of <span class="math inline">\(\pi\)</span>. In the absence of other information, it appears plausible to take <span class="math inline">\(k/n\)</span> as the “best guess” for <span class="math inline">\(\pi\)</span>. In other words, <span class="math inline">\(k/n\)</span> maximizes the probabilty of <span class="math inline">\(\pi\)</span>.</p>
