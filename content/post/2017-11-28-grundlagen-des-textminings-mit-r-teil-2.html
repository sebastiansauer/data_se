---
title: Grundlagen des Textminings mit R - Teil 2
author: Sebastian Sauer
date: '2017-11-28'
slug: grundlagen-des-textminings-mit-r-teil-2
categories:
  - rstats
tags:
  - rstats
  - textmining
  - German
---



<p>In dieser Übung benötigte R-Pakete:</p>
<pre class="r"><code>library(tidyverse)  # Datenjudo
library(stringr)  # Textverarbeitung
library(tidytext)  # Textmining
library(lsa)  # Stopwörter 
library(SnowballC)  # Wörter trunkieren
library(wordcloud)  # Wordcloud anzeigen
library(skimr)  # Überblicksstatistiken</code></pre>
<blockquote>
<p>Bitte installieren Sie <em>rechtzeitig</em> alle Pakete, z.B. in RStudio über den Reiter <em>Packages … Install</em>.</p>
</blockquote>
<pre><code>## 
## Attaching package: &#39;knitr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:skimr&#39;:
## 
##     kable</code></pre>
<div id="aus-dem-letzten-post" class="section level2">
<h2>Aus dem letzten Post</h2>
<p>Daten einlesen:</p>
<pre class="r"><code>osf_link &lt;- paste0(&quot;https://osf.io/b35r7/?action=download&quot;)
afd &lt;- read_csv(osf_link)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   page = col_double(),
##   content = col_character()
## )</code></pre>
<p>Aus breit mach lang:</p>
<pre class="r"><code>afd %&gt;% 
  unnest_tokens(output = token, input = content) %&gt;% 
  dplyr::filter(str_detect(token, &quot;[a-z]&quot;)) -&gt; afd_long</code></pre>
<p>Stopwörter entfernen:</p>
<pre class="r"><code>data(stopwords_de, package = &quot;lsa&quot;)

stopwords_de &lt;- data_frame(word = stopwords_de)</code></pre>
<pre><code>## Warning: `data_frame()` is deprecated, use `tibble()`.
## This warning is displayed once per session.</code></pre>
<pre class="r"><code># Für das Joinen werden gleiche Spaltennamen benötigt
stopwords_de &lt;- stopwords_de %&gt;% 
  rename(token = word)  

afd_long %&gt;% 
  anti_join(stopwords_de) -&gt; afd_no_stop</code></pre>
<pre><code>## Joining, by = &quot;token&quot;</code></pre>
<p>Wörter zählen:</p>
<pre class="r"><code>afd_no_stop %&gt;% 
  count(token, sort = TRUE) -&gt; afd_count</code></pre>
<p>Wörter trunkieren:</p>
<pre class="r"><code>afd_no_stop %&gt;% 
  mutate(token_stem = wordStem(.$token, language = &quot;de&quot;)) %&gt;% 
  count(token_stem, sort = TRUE) -&gt; afd_count_stemmed</code></pre>
</div>
<div id="regex" class="section level2">
<h2>Regulärausdrücke</h2>
<p>Das <code>"[a-z]"</code> in der Syntax oben steht für “alle Buchstaben von a-z”. Diese flexible Art von “String-Verarbeitung mit Jokern” nennt man <em>Regulärausdrücke</em> (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regulärausdrücken, die die Verarbeitung von Texten erleichert. Mit dem Paket <code>stringr</code> geht das - mit etwas Übung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:</p>
<pre class="r"><code>string &lt;-&quot;Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\n\nhttps://t.co/YHyqTguVWx&quot;  </code></pre>
<p>Möchte man Ziffern identifizieren, so hilft der Reulärausdruck <code>[:digit:]</code>:</p>
<p>“Gibt es mindestens eine Ziffer in dem String?”</p>
<pre class="r"><code>str_detect(string, &quot;[:digit:]&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>“Finde die Position der ersten Ziffer! Welche Ziffer ist es?”</p>
<pre class="r"><code>str_locate(string, &quot;[:digit:]&quot;)</code></pre>
<pre><code>##      start end
## [1,]    51  51</code></pre>
<pre class="r"><code>str_extract(string, &quot;[:digit:]&quot;)</code></pre>
<pre><code>## [1] &quot;1&quot;</code></pre>
<p>“Finde alle Ziffern!”</p>
<pre class="r"><code>str_extract_all(string, &quot;[:digit:]&quot;)</code></pre>
<pre><code>## [[1]]
## [1] &quot;1&quot; &quot;7&quot; &quot;0&quot; &quot;1&quot; &quot;8&quot;</code></pre>
<p>“Finde alle Stellen an denen genau 2 Ziffern hintereinander folgen!”</p>
<pre class="r"><code>str_extract_all(string, &quot;[:digit:]{2}&quot;)</code></pre>
<pre><code>## [[1]]
## [1] &quot;17&quot; &quot;18&quot;</code></pre>
<p>Der Quantitätsoperator <code>{n}</code> findet alle Stellen, in der der der gesuchte Ausdruck genau <span class="math inline">\(n\)</span> mal auftaucht.</p>
<p>“Gebe die Hashtags zurück!”</p>
<pre class="r"><code>str_extract_all(string, &quot;#[:alnum:]+&quot;)</code></pre>
<pre><code>## [[1]]
## [1] &quot;#AfD&quot;   &quot;#btw17&quot;</code></pre>
<p>Der Operator <code>[:alnum:]</code> steht für “alphanumerischer Charakter” - also eine Ziffer oder ein Buchstabe; synonym hätte man auch <code>\\w</code> schreiben können (w wie word). Warum werden zwei Backslashes gebraucht? Mit <code>\\w</code> wird signalisiert, dass nicht der Buchstabe <em>w</em>, sondern etwas Besonderes, eben der Regex-Operator <code>\w</code> gesucht wird.</p>
<p>“Gebe URLs zurück!”</p>
<pre class="r"><code>str_extract_all(string, &quot;https?://[:graph:]+&quot;)</code></pre>
<pre><code>## [[1]]
## [1] &quot;https://t.co/YHyqTguVWx&quot;</code></pre>
<p>Das Fragezeichen <code>?</code> ist eine Quantitätsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier <em>s</em>) null oder einmal gefunden wird. <code>[:graph:]</code> ist die Summe von <code>[:alpha:]</code> (Buchstaben, groß und klein), <code>[:digit:]</code> (Ziffern) und <code>[:punct:]</code> (Satzzeichen u.ä.).</p>
<p>“Zähle die Wörter im String!”</p>
<pre class="r"><code>str_count(string, boundary(&quot;word&quot;))</code></pre>
<pre><code>## [1] 13</code></pre>
<p>“Liefere nur Buchstaben<em>folgen</em> zurück, lösche alles übrige”</p>
<pre class="r"><code>str_extract_all(string, &quot;[:alpha:]+&quot;)</code></pre>
<pre><code>## [[1]]
##  [1] &quot;Correlation&quot;  &quot;of&quot;           &quot;unemployment&quot; &quot;and&quot;         
##  [5] &quot;AfD&quot;          &quot;votes&quot;        &quot;at&quot;           &quot;btw&quot;         
##  [9] &quot;r&quot;            &quot;https&quot;        &quot;t&quot;            &quot;co&quot;          
## [13] &quot;YHyqTguVWx&quot;</code></pre>
<p>Der Quantitätsoperator <code>+</code> liefert alle Stellen zurück, in denen der gesuchte Ausdruck <em>einmal oder häufiger</em> vorkommt. Die Ergebnisse werden als Vektor von Wörtern zurückgegeben. Ein anderer Quantitätsoperator ist <code>*</code>, der für 0 oder mehr Treffer steht. Möchte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenfüngen, hilft <code>paste(string)</code> oder <code>str_c(string, collapse = " ")</code>.</p>
<pre class="r"><code>str_replace_all(string, &quot;[^[:alpha:]+]&quot;, &quot;&quot;)</code></pre>
<pre><code>## [1] &quot;CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx&quot;</code></pre>
<p>Mit dem Negationsoperator <code>[^x]</code> wird der Regulärausrck <code>x</code> negiert; die Syntax oben heißt also “ersetze in <code>string</code> alles außer Buchstaben durch Nichts”. Mit “Nichts” sind hier Strings der Länge Null gemeint; ersetzt man einen belieibgen String durch einen String der Länge Null, so hat man den String gelöscht.</p>
<p>Das Cheatsheet zur Strings bzw zu <code>stringr</code> von RStudio gibt einen guten Überblick über Regex; im Internet finden sich viele Beispiele.</p>
</div>
<div id="sentiment-analyse" class="section level2">
<h2>Sentiment-Analyse</h2>
<p>Eine weitere interessante Analyse ist, die “Stimmung” oder “Emotionen” (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:</p>

<div class="rmdpseudocode">
<p>Schau dir jeden Token aus dem Text an.<br />
Prüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.<br />
Wenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.<br />
Wenn nein, dann gehe weiter zum nächsten Wort.<br />
Liefere zum Schluss die Summenwerte pro Sentiment zurück.</p>
</div>

<p>Es gibt Sentiment-Lexika, die lediglich einen Punkt für “positive Konnotation” bzw. “negative Konnotation” geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier das Sentimentlexikon <code>sentiws</code>. Sie können es hier herunterladen:</p>
<pre class="r"><code>sentiws &lt;- read_csv(&quot;https://osf.io/x89wq/?action=download&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   neg_pos = col_character(),
##   word = col_character(),
##   value = col_double(),
##   inflections = col_character()
## )</code></pre>
<p>Alternativ können Sie die Daten aus dem Paket <code>pradadata</code> laden. Allerdings müssen Sie dieses Paket von Github installieren:</p>
<pre class="r"><code>install.packages(&quot;devtools&quot;, dep = TRUE)
devtools::install_github(&quot;sebastiansauer/pradadata&quot;)</code></pre>
<pre class="r"><code>data(sentiws, package = &quot;pradadata&quot;)</code></pre>
<p>Tabelle @ref(tab:afd_count) zeigt einen Ausschnitt aus dem Sentiment-Lexikon <em>SentiWS</em>.</p>
<table>
<caption><span id="tab:sentiws-head">Table 1: </span>Auszug aus SentiwS</caption>
<thead>
<tr class="header">
<th align="left">neg_pos</th>
<th align="left">word</th>
<th align="right">value</th>
<th align="left">inflections</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">neg</td>
<td align="left">Abbau</td>
<td align="right">-0.0580</td>
<td align="left">Abbaus,Abbaues,Abbauen,Abbaue</td>
</tr>
<tr class="even">
<td align="left">neg</td>
<td align="left">Abbruch</td>
<td align="right">-0.0048</td>
<td align="left">Abbruches,Abbrüche,Abbruchs,Abbrüchen</td>
</tr>
<tr class="odd">
<td align="left">neg</td>
<td align="left">Abdankung</td>
<td align="right">-0.0048</td>
<td align="left">Abdankungen</td>
</tr>
<tr class="even">
<td align="left">neg</td>
<td align="left">Abdämpfung</td>
<td align="right">-0.0048</td>
<td align="left">Abdämpfungen</td>
</tr>
<tr class="odd">
<td align="left">neg</td>
<td align="left">Abfall</td>
<td align="right">-0.0048</td>
<td align="left">Abfalles,Abfälle,Abfalls,Abfällen</td>
</tr>
<tr class="even">
<td align="left">neg</td>
<td align="left">Abfuhr</td>
<td align="right">-0.3367</td>
<td align="left">Abfuhren</td>
</tr>
</tbody>
</table>
<div id="ungewichtete-sentiment-analyse" class="section level3">
<h3>Ungewichtete Sentiment-Analyse</h3>
<p>Nun können wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zählen wir die Treffer für positive bzw. negative Terme. Zuvor müssen wir aber noch die Daten (<code>afd_long</code>) mit dem Sentimentlexikon zusammenführen (joinen). Das geht nach bewährter Manier mit <code>inner_join</code>; “inner” sorgt dabei dafür, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle @ref(tab:afd_senti_tab) zeigt Summe, Anzahl und Anteil der Emotionswerte.</p>
<pre class="r"><code>afd_long %&gt;% 
  inner_join(sentiws, by = c(&quot;token&quot; = &quot;word&quot;)) %&gt;% 
  select(-inflections) -&gt; afd_senti  # die Spalte brauchen wir nicht

afd_senti %&gt;% 
  group_by(neg_pos) %&gt;% 
  summarise(polarity_sum = sum(value),
            polarity_count = n()) %&gt;% 
  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %&gt;% round(2)) -&gt; afd_senti_tab</code></pre>
<table>
<caption>(#tab:afd_senti_tab)Zusammenfassung von SentiWS</caption>
<thead>
<tr class="header">
<th align="left">neg_pos</th>
<th align="right">polarity_sum</th>
<th align="right">polarity_count</th>
<th align="right">polarity_prop</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">neg</td>
<td align="right">-52.6461</td>
<td align="right">219</td>
<td align="right">0.27</td>
</tr>
<tr class="even">
<td align="left">pos</td>
<td align="right">29.6063</td>
<td align="right">586</td>
<td align="right">0.73</td>
</tr>
</tbody>
</table>
<p>Die Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv getönte Wörter als negativ getönte. Allerdings sind die negativen Wörter offenbar deutlich stärker emotional aufgeladen, dennn die Summe an Emotionswert der negativen Wörter ist überraschenderweise deutlich größer als die der positiven.</p>
<p>Betrachten wir also die intensivsten negativ und positive konnotierten Wörter näher.</p>
<pre class="r"><code>afd_senti %&gt;% 
  distinct(token, .keep_all = TRUE) %&gt;% 
  mutate(value_abs = abs(value)) %&gt;% 
  top_n(20, value_abs) %&gt;% 
  pull(token)</code></pre>
<pre><code>##  [1] &quot;ungerecht&quot;    &quot;besonders&quot;    &quot;gefährlich&quot;   &quot;überflüssig&quot; 
##  [5] &quot;behindern&quot;    &quot;gefährden&quot;    &quot;brechen&quot;      &quot;unzureichend&quot;
##  [9] &quot;gemein&quot;       &quot;verletzt&quot;     &quot;zerstören&quot;    &quot;trennen&quot;     
## [13] &quot;falsch&quot;       &quot;vermeiden&quot;    &quot;zerstört&quot;     &quot;schwach&quot;     
## [17] &quot;belasten&quot;     &quot;schädlich&quot;    &quot;töten&quot;        &quot;verbieten&quot;</code></pre>
<p>Diese “Hitliste” wird zumeist (19/20) von negativ polarisierten Begriffen aufgefüllt, wobei “besonders” ein Intensivierwort ist, welches das Bezugswort verstärt (“besonders gefährlich”). Das Argument <code>keep_all = TRUE</code> sorgt dafür, dass alle Spalten zurückgegeben werden, nicht nur die durchsuchte Spalte <code>token</code>. Mit <code>pull</code> haben wir aus dem Dataframe, der von den dplyr-Verben überwegeben wird, die Spalte <code>pull</code> “herausgezogen”; hier nur um Platz zu sparen bzw. der Übersichtlichkeit halber.</p>
<p>Nun könnte man noch den erzielten “Netto-Sentimentswert” des Corpus ins Verhältnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wäre ein negativer Sentimentwer in einem beliebigen Corpus nicht überraschend. <code>skimr::skim()</code> gibt uns einen Überblick der üblichen deskriptiven Statistiken.</p>
<pre class="r"><code>sentiws %&gt;% 
  select(value, neg_pos) %&gt;% 
  #group_by(neg_pos) %&gt;% 
  skim()</code></pre>
<p>Insgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der Überzahl im Lexikon. Unser Corpus hat eine ähnliche mittlere emotionale Konnotation wie das Lexikon:</p>
<pre class="r"><code>afd_senti %&gt;% 
  summarise(senti_sum = mean(value) %&gt;% round(2))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   senti_sum
##       &lt;dbl&gt;
## 1     -0.03</code></pre>
</div>
</div>
