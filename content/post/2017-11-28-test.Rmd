---
title: test
author: Sebastian Sauer
date: '2017-11-28'
slug: test
categories:
  - div
tags:
  - blogdown
---




>    Lernziele:

      - Sie kennen zentrale Ziele und Begriffe des Textminings.
      - Sie wissen, was ein 'tidy text dataframe' ist.
      - Sie können Worthäufigkeiten auszählen.
      - Sie können Worthäufigkeiten anhand einer Wordcloud visualisieren.



In diesem Kapitel benötigte R-Pakete:
```{r libs-textmining}
library(tidyverse)  # Datenjudo
library(stringr)  # Textverarbeitung
library(tidytext)  # Textmining
library(lsa)  # Stopwörter 
library(SnowballC)  # Wörter trunkieren
library(wordcloud)  # Wordcloud anzeigen
library(skimr)  # Überblicksstatistiken
```


```{r libs-textmining-hidden, echo = FALSE}
library(knitr)
```



Ein großer Teil der zur Verfügung stehenden Daten liegt nicht als braves Zahlenmaterial vor, sondern in "unstrukturierter" Form, z.B. in Form von Texten. Im Gegensatz zur Analyse von numerischen Daten ist die Analyse von Texten weniger verbreitet bisher. In Anbetracht der Menge und der Informationsreichhaltigkeit von Text erscheint die Analyse von Text als vielversprechend.


In gewisser Weise ist das Textmining ein alternative zu klassischen qualitativen Verfahren der Sozialforschung. Geht es in der qualitativen Sozialforschung primär um das Verstehen eines Textes, so kann man für das Textmining ähnliche Ziele formulieren. Allerdings: Das Textmining ist wesentlich schwächer und beschränkter in der Tiefe des Verstehens. Der Computer ist einfach noch (?) wesentlich *dümmer* als ein Mensch, zumindest in dieser Hinsicht. Allerdings ist er auch wesentlich *schneller* als ein Mensch, was das Lesen betrifft. Daher bietet sich das Textmining für das Lesen großer Textmengen an, in denen eine geringe Informationsdichte vermutet wird. Sozusagen maschinelles Sieben im großen Stil. Da fällt viel durch die Maschen, aber es werden Tonnen von Sand bewegt.

In der Regel wird das Textmining als *gemischte* Methode verwendet: sowohl qualitative als auch qualitative Aspekte spielen eine Rolle. Damit vermittelt das Textmining auf konstruktive Art und Weise zwischen den manchmal antagonierenden Schulen der qualitativ-idiographischen und der quantitativ-nomothetischen Sichtweise auf die Welt. Man könnte es auch als qualitative Forschung mit moderner Technik bezeichnen - mit den skizzierten Einschränkungen wohlgemerkt.

## Zentrale Begriffe


Die computergestützte Analyse von Texten speiste (und speist) sich reichhaltig aus Quellen der Linguistik; entsprechende Fachtermini finden Verwendung:  

- Ein *Corpus* bezeichnet die Menge der zu analysierenden Dokumente; das könnten z.B. alle Reden der Bundeskanzlerin Angela Merkel sein oder alle Tweets von "\@realDonaldTrump".

- Ein *Token* (*Term*) ist ein elementarer Baustein eines Texts, die kleinste Analyseeinheit, häufig ein Wort. 

- Unter *tidy text* versteht man einen Dataframe, in dem pro Zeile nur *ein* Token (z.B. Wort) steht [@Silge2016]. Synonym könnte man von einem "langen" Dataframe sprechen.



## Grundlegende Analyse

### Tidy Text Dataframes

Wozu ist es nützlich, einen Text-Dataframe in einen langen Dataframe umzuwandeln? Der Grund ist, dass immer wenn nur ein Wort (allgemeiner: Term) pro Zelle steht, dann können wir die Spalte einfach auszählen. Wir können z.B. `count` nutzen, um zu zählen, wie häufig ein Wort vorkommt. Sprich: Sobald wir einen langen (Text-)Dataframe haben, können wir unsere bekannte Methoden einsetzen.

Basteln wir uns einen *tidy text* Dataframe. Wir gehen dabei von einem Vektor mit mehreren Text-Elementen aus, das ist ein realistischer Startpunkt. Unser Text-Vektor^[Nach dem Gedicht "Jahrgang 1899" von Erich Kästner] besteht aus 4 Elementen.


```{r}
text <- c("Wir haben die Frauen zu Bett gebracht,",
          "als die Männer in Frankreich standen.",
          "Wir hatten uns das viel schöner gedacht.",
          "Wir waren nur Konfirmanden.")
```

Als nächstes machen wir daraus einen Dataframe.

```{r}
text_df <- data_frame(Zeile = 1:4,
                      text = text)
```



```{r echo = FALSE}
knitr::kable(text_df)
```

Übrigens, falls Sie eine beliebige Textdatei einlesen möchten, können Sie das so tun:

```{r eval = FALSE}
text <- read_lines("Brecht.txt")
```

Der Befehl `read_lines` (aus `readr`^[Teil der Tidyverse-Familie]) liest Zeilen (Zeile für Zeile) aus einer Textdatei.


>    Wenn Sie *keinen* Pfad angeben, dann geht R davon aus, dass sich die angegebene Datei im Arbeitsverzeichnis befindet. Welcher Ordner ist Ihr Arbeitsverzeichnis? Klicken Sie in RStudio auf *Session > Set Working Directory...*  um zu erfahren, welcher Ordner im Moment Ihr Arbeitsverzeichnis ist. Dort können Sie auch Ihr Arbeitsverzeichnis einstellen.


Als nächstes "dehnen" wir den Dataframe zu einem *tidy text* Dataframe (s. Abb. \@ref(fig:tidytextdf)); das besorgt die Funktion `unnest_tokens` aus `tidytext. 'unnest' heißt dabei so viel wie 'Entschachteln', also von breit auf lang dehnen. Mit 'tokens' sind hier einfach die Wörter gemeint (es könnten aber auch andere Analyseeinheiten sein, Sätze zum Beispiel).


```{r tidytextdf, echo = FALSE, fig.cap = "Illustration eines Tidy Text Dataframe"}
knitr::include_graphics("images/textmining/tidytext-crop.png")
```


```{r}
text_df %>% 
  unnest_tokens(output = wort, input = text) -> tidytext_df

head(tidytext_df)
```

Der Parameter `output` sagt, wie neue 'saubere' (lange) Spalte heißen soll; `input` sagt der Funktion, welche Spalte sie als ihr Futter (Input) betrachten soll (welche Spalte in tidy text umgewandelt werden soll).

```
Nehme den Datensatz "text_df" UND DANN  
dehne die einzelnen Elemente der Spalte "text", so dass jedes Element seine eigene Spalte bekommt.  
Ach ja: Diese "gedehnte" Spalte soll "wort" heißen (weil nur einzelne Wörter drinnen stehen) UND DANN  
speichere den resultierenden Dataframe ab als `tidytext_df`.
```


>   In einem 'tidy text Dataframe' steht in jeder Zeile ein Wort (token) und die Häufigkeit des Worts im Dokument.

Überprüfen Sie, ob das stimmt: Betrachten Sie den Dataframe `tidytext_df`.


Das `unnest_tokens` kann übersetzt werden als "entschachtele" oder "dehne" die Tokens - so dass in *jeder Zeile* nur noch *ein Wort* (genauer: Token) steht. Die Syntax ist `unnest_tokens(Ausgabespalte, Eingabespalte)`. Nebenbei werden übrigens alle Buchstaben auf Kleinschreibung getrimmt.

Als nächstes filtern wir die Satzzeichen heraus, da die Wörter für die Analyse wichtiger (oder zumindest einfacher) sind.

```{r}
tidytext_df %>% 
  filter(str_detect(wort, "[a-z]")) -> tidytext_df_lowercase
```



In Pseudo-Code heißt dieser Abschnitt:


```
Nehme den Datensatz "tidytext_df" UND DANN  
filtere die Spalte "wort", so dass nur noch Kleinbuchstaben übrig bleiben (keine Ziffern etc.). FERTIG.  
```


### Text-Daten einlesen



#### Als CSV-Datei einlesen

Nun lesen wir Text-Daten ein; das können beliebige Daten sein^[Ggf. benötigen Sie Administrator-Rechte, um Dateien auf Ihre Festplatte zu speichern.]. Eine gewisse Reichhaltigkeit im Text ist von Vorteil. Nehmen wir das Parteiprogramm der Partei AfD^[ <geladen am 1. März 2017 von https://www.alternativefuer.de/wp-content/uploads/sites/7/2016/05/2016-06-27_afd-grundsatzprogramm_web-version.pdf>]. Vor dem Hintergrund des Erstarkens des Populismus weltweit und der großen Gefahr, die davon ausgeht - man blicke auf die Geschichte Europas in der ersten Hälfte des 20. Jahrhunderts - ~~verdient~~erfordert der politische Prozess und speziell Neuentwicklungen darin unsere besondere Beachtung. Das Parteiprogramm kann hier als CSV-Datei von <https://osf.io/b35r7/> eingelesen werden:

```{r}
osf_link <- paste0("https://osf.io/b35r7//?action=download")
afd <- read_csv(osf_link)
```


#### Als PDF oder unstrukturierte Text-Datei einlesen

Häufig sind Textdateien in Textdateien gespeichert, dann kann man mit `readr::read_lines` Daten, die keine Tabellenstruktur haben einlesen. Ein anderes häufiges Format sind PDF-Dateien; dafür bietet das Paket `pdftools` die Funktion `pdf_text`,  die den Inhalt einer PDF-Datei einliest. Jede Seite wird dabei als ein Element eines Vektors abgespeichert. Hätten wir die PDF-Datei vorliegen, so könnten wir sie mit `pdf_text` einlesen. Der resultierende Vektor `afd_raw` hat 96 Elemente (entsprechend der Seitenzahl des Dokuments). Wandeln wir als nächstes den Vektor in einen Dataframe um.

```{r eval = FALSE}
afd_pfad <- "data/afd_programm.pdf"
afd_raw <- pdf_text(afd_pfad)
afd <- data_frame(Zeile = 1:96, 
                  afd_raw)
```


#### Daten sind da - jetzt aufhübschen

Im Ergebnis hätten wir einen Dataframe, ein Prachtstück. Mit `head(afd)` können Sie sich den Beginn dieses Textvektor anzeigen lassen. Auch die Stopwörter entfernen wir wieder wie gehabt.

```{r}
afd %>% 
  unnest_tokens(output = token, input = content) %>% 
  dplyr::filter(str_detect(token, "[a-z]")) -> afd_long
```



Insgesamt `r nrow(afd)` Wörter^[wie kann man sich die Anzahl der Wörter ausgeben lassen? Z.B. so: `count(afd)`]; eine substanzielle Menge von Text. Was wohl die häufigsten Wörter sind?


### Worthäufigkeiten auszählen

```{r}
afd_long %>% 
  na.omit() %>%  # fehlende Werte löschen
  count(token, sort = TRUE)
```

Die häufigsten Wörter sind inhaltsleere Partikel, Präpositionen, Artikel... Solche sogenannten "Stopwörter" sollten wir besser herausfischen, um zu den inhaltlich tragenden Wörtern zu kommen. Praktischerweise gibt es frei verfügbare Listen von Stopwörtern, z.B. im Paket `lsa`.

```{r}
data(stopwords_de, package = "lsa")

stopwords_de <- data_frame(word = stopwords_de)

# Für das Joinen werden gleiche Spaltennamen benötigt
stopwords_de <- stopwords_de %>% 
  rename(token = word)  

afd_long %>% 
  anti_join(stopwords_de) -> afd_no_stop
```


Unser Datensatz hat jetzt viel weniger Zeilen; wir haben also durch `anti_join` Zeilen gelöscht (herausgefiltert). Das ist die Funktion von `anti_join`: Die Zeilen, die in beiden Dataframes vorkommen, werden herausgefiltert. Es verbleiben also nicht "Nicht-Stopwörter" in unserem Dataframe. Damit wird es schon interessanter, welche Wörter häufig sind.

```{r}
afd_no_stop %>% 
  count(token, sort = TRUE) -> afd_count
```


```{r echo = FALSE}
afd_count %>% 
  top_n(10) %>% 
  knitr::kable(caption = "Die häufigsten Wörter")
```

Ganz interessant; aber es gibt mehrere Varianten des Themas "deutsch". Es ist wohl sinnvoller, diese auf den gemeinsamen Wortstamm zurückzuführen und diesen nur einmal zu zählen. Dieses Verfahren nennt man "stemming" oder "trunkieren".

```{r}
afd_no_stop %>% 
  mutate(token_stem = wordStem(.$token, language = "de")) %>% 
  count(token_stem, sort = TRUE) -> afd_count_stemmed

afd_count_stemmed %>% 
  top_n(10) %>% 
  knitr::kable(caption = "Die häufigsten Wörter - mit 'stemming'")
```

Das ist schon informativer. Dem Befehl `SnowballC::wordStem` füttert man einen Vektor an Wörtern ein und gibt die Sprache an (Default ist Englisch). Denken Sie daran, dass `.` bei `dplyr` nur den Datensatz meint, wie er im letzten Schritt definiert war. Mit `.$token` wählen wir also die Variable `token` aus `afd_raw` aus.


### Visualisierung


Zum Abschluss noch eine Visualisierung mit einer "Wordcloud" dazu (Abbildung \@ref(fig:show-wordcloud)).

```{r show-wordcloud-FALSE, fig.cap = "Eine Wordwolke zum AfD-Parteiprogramm", eval = FALSE}
wordcloud(words = afd_count_stemmed$token_stem, 
          freq = afd_count_stemmed$n, 
          max.words = 100, 
          scale = c(2,.5), 
          colors=brewer.pal(6, "Dark2"))
```

```{r show-wordcloud, fig.cap = "Eine Wordwolke zum AfD-Parteiprogramm", echo = FALSE}
knitr::include_graphics("images/textmining/wordcloud1.png")
```



Man kann die Anzahl der Wörter, Farben und einige weitere Formatierungen der Wortwolke beeinflussen^[https://cran.r-project.org/web/packages/wordcloud/index.html].

 
 
Weniger verspielt ist eine schlichte visualisierte Häufigkeitsauszählung dieser Art, z.B. mit Balkendiagrammen (gedreht), s. Abbildung \@ref(fig:p-word-freq).

```{r}
afd_count_stemmed %>% 
  top_n(30) %>% 
  ggplot() +
  aes(x = reorder(token_stem, n), y = n) +
  geom_col() + 
  labs(title = "mit Trunkierung") +
  coord_flip() -> p1

afd_count %>% 
  top_n(30) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  labs(title = "ohne Trunkierung") +
  coord_flip() -> p2
```

```{r p-word-freq, echo = FALSE, fig.cap = "Worthäufigkeiten im AfD-Parteiprogramm"}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```



Die beiden Diagramme vergleichen die trunkierten Wörter mit den nicht trunkierten Wörtern. Mit `reorder` ordnen wir die Spalte `token` nach der Spalte `n`. `coord_flip` dreht die Abbildung um 90°, d.h. die Achsen sind vertauscht.


## Aufgaben^[F, R, F, F, R, R, F, F]



```{block2, exercises-text, type='rmdexercises', echo = TRUE} 
Richtig oder Falsch!?

1. Unter einem Token versteht man die größte Analyseeinheit in einem Text.
1. In einem tidytext Dataframe steht jedes Wort in einer (eigenen) Zeile.
1. Eine hinreichende Bedingung für einen tidytext Dataframe ist es, dass in jeder Zeile ein Wort steht (beziehen Sie sich auf den tidytext Dataframe wie in diesem Kapitel erörtert).
1. Gibt es 'Stop-Wörter' in einem Dataframe, dessen Text analysiert wird, so kommt es - per definitionem - zu einem Stop.
1. Mit dem Befehl `unnest_tokens` kann man einen tidytext Dataframe erstellen.
1. Balkendiagramme sind sinnvolle und auch häufige Diagrammtypen, um die häufigsten Wörter (oder auch Tokens) in einem Corpus darzustellen.
1. In einem 'tidy text Dataframe' steht in jeder Zeile ein Wort (token) *aber nicht* die Häufigkeit des Worts im Dokument.
1. Unter 'Stemming' versteht man (bei der Textanalyse), die Etymologie eines Wort (Herkunft) zu erkunden.

```



## Regulärausdrücke {#regex}

Das `"[a-z]"` in der Syntax oben steht für "alle Buchstaben von a-z". Diese flexible Art von "String-Verarbeitung mit Jokern" nennt man *Regulärausdrücke*\index{Regulärausdrücke} (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regulärausdrücken, die die Verarbeitung von Texten erleichert. Mit dem Paket `stringr` geht das - mit etwas Übung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:

```{r}
string <-"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\n\nhttps://t.co/YHyqTguVWx"  
```

Möchte man Ziffern identifizieren, so hilft der Reulärausdruck `[:digit:]`:

"Gibt es mindestens eine Ziffer in dem String?"

```{r}
str_detect(string, "[:digit:]")
```

"Finde die Position der ersten Ziffer! Welche Ziffer ist es?"

```{r}
str_locate(string, "[:digit:]")
str_extract(string, "[:digit:]")
```

"Finde alle Ziffern!"

```{r}
str_extract_all(string, "[:digit:]")
```


"Finde alle Stellen an denen genau 2 Ziffern hintereinander folgen!"
```{r}
str_extract_all(string, "[:digit:]{2}")
```

Der Quantitätsoperator `{n}` findet alle Stellen, in der der der gesuchte Ausdruck genau $n$ mal auftaucht.


"Gebe die Hashtags zurück!"

```{r}
str_extract_all(string, "#[:alnum:]+")
```

Der Operator `[:alnum:]` steht für "alphanumerischer Charakter" - also eine Ziffer oder ein Buchstabe; synonym hätte man auch `\\w` schreiben können (w wie word). Warum werden zwei Backslashes gebraucht? Mit `\\w` wird signalisiert, dass nicht der Buchstabe *w*, sondern etwas Besonderes, eben der Regex-Operator `\w` gesucht wird. 

"Gebe URLs zurück!"

```{r}
str_extract_all(string, "https?://[:graph:]+")
```

Das Fragezeichen `?` ist eine Quantitätsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier *s*) null oder einmal gefunden wird. `[:graph:]` ist die Summe von `[:alpha:]` (Buchstaben, groß und klein), `[:digit:]` (Ziffern) und `[:punct:]` (Satzzeichen u.ä.).

"Zähle die Wörter im String!"

```{r}
str_count(string, boundary("word"))
```


"Liefere nur Buchstaben*folgen* zurück, lösche alles übrige"

```{r}
str_extract_all(string, "[:alpha:]+")
```

Der Quantitätsoperator `+` liefert alle Stellen zurück, in denen der gesuchte Ausdruck *einmal oder häufiger*  vorkommt. Die Ergebnisse werden als Vektor von Wörtern zurückgegeben. Ein anderer Quantitätsoperator ist `*`, der für 0 oder mehr Treffer steht. Möchte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenfüngen, hilft `paste(string)` oder `str_c(string, collapse = " ")`.

```{r}
str_replace_all(string, "[^[:alpha:]+]", "")
```

Mit dem Negationsoperator `[^x]` wird der Regulärausrck `x` negiert; die Syntax oben heißt also "ersetze in `string` alles außer Buchstaben durch Nichts". Mit "Nichts" sind hier Strings der Länge Null gemeint; ersetzt man einen belieibgen String durch einen String der Länge Null, so hat man den String gelöscht.

Das Cheatsheet zur Strings bzw zu `stringr` von RStudio gibt einen guten Überblick über Regex; im Internet finden sich viele Beispiele.



## Sentiment-Analyse
Eine weitere interessante Analyse ist, die "Stimmung" oder "Emotionen" (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so: 



```{block2, sentiment-pseudocode, type='rmdpseudocode', echo = TRUE}
Schau dir jeden Token aus dem Text an.  
Prüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.  
Wenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.  
Wenn nein, dann gehe weiter zum nächsten Wort.  
Liefere zum Schluss die Summenwerte pro Sentiment zurück.  

```


     
Es gibt Sentiment-Lexika, die lediglich einen Punkt für "positive Konnotation" bzw. "negative Konnotation" geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier das Sentimentlexikon von @remquahey2010. 

```{r parse-sentiment-dics}
data(sentiws, package = "pradadata")
```

Tabelle \@ref(tab:afd_count) zeigt einen Ausschnitt aus dem Sentiment-Lexikon *SentiWS* [@remquahey2010].

```{r sentiws-head, echo = FALSE}
knitr::kable(head(sentiws), caption = "Auszug aus SentiwS")
```

### Ungewichtete Sentiment-Analyse
Nun können wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zählen wir die Treffer für positive bzw. negative Terme. Zuvor müssen wir aber noch die Daten (`afd_long`) mit dem Sentimentlexikon zusammenführen (joinen). Das geht nach bewährter Manier mit `inner_join`; "inner" sorgt dabei dafür, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle \@ref(tab:afd_senti_tab) zeigt Summe, Anzahl und Anteil der Emotionswerte.

```{r}
afd_long %>% 
  inner_join(sentiws, by = c("token" = "word")) %>% 
  select(-inflections) -> afd_senti  # die Spalte brauchen wir nicht

afd_senti %>% 
  group_by(neg_pos) %>% 
  summarise(polarity_sum = sum(value),
            polarity_count = n()) %>% 
  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) -> afd_senti_tab
```

```{r afd_senti_tab, echo = FALSE}
knitr::kable(afd_senti_tab, caption = "Zusammenfassung von SentiWS")
```


Die Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv getönte Wörter als negativ getönte. Allerdings sind die negativen Wörter offenbar deutlich stärker emotional aufgeladen, dennn die Summe an Emotionswert der negativen Wörter ist überraschenderweise deutlich größer als die der positiven.

Betrachten wir also die intensivsten negativ und positive konnotierten Wörter näher.

```{r}
afd_senti %>% 
  distinct(token, .keep_all = TRUE) %>% 
  mutate(value_abs = abs(value)) %>% 
  top_n(20, value_abs) %>% 
  pull(token)
```

Diese "Hitliste" wird zumeist (19/20) von negativ polarisierten Begriffen aufgefüllt, wobei "besonders" ein Intensivierwort ist, welches das Bezugswort verstärt ("besonders gefährlich"). Das Argument `keep_all = TRUE` sorgt dafür, dass alle Spalten zurückgegeben werden, nicht nur die durchsuchte Spalte `token`. Mit `pull` haben wir aus dem Dataframe, der von den dplyr-Verben überwegeben wird, die Spalte `pull` "herausgezogen"; hier nur um Platz zu sparen bzw. der Übersichtlichkeit halber.


     
Nun könnte man noch den erzielten "Netto-Sentimentswert" des Corpus ins Verhältnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wäre ein negativer Sentimentwer in einem beliebigen Corpus nicht überraschend. `skimr::skim()` gibt uns einen Überblick der üblichen deskriptiven Statistiken.
     

```{r eval = FALSE}
sentiws %>% 
  select(value, neg_pos) %>% 
  #group_by(neg_pos) %>% 
  skim()
```

Insgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der Überzahl im Lexikon. Unser Corpus hat eine ähnliche mittlere emotionale Konnotation wie das Lexikon:

```{r}
afd_senti %>% 
  summarise(senti_sum = mean(value) %>% round(2))
```






## Verweise

- Das Buch *Tidy Text Minig* [@tidytextminig] ist eine hervorragende Quelle vertieftem Wissens zum Textmining mit R.












