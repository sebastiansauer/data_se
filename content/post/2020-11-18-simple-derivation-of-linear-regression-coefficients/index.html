---
title: Simple derivation of linear regression coefficients
author: Sebastian Sauer
date: '2020-11-18'
slug: simple-derivation-of-linear-regression-coefficients
categories:
  - rstats
tags:
  - tutorial
---

<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<div id="load-packages" class="section level1">
<h1>Load packages</h1>
<pre class="r"><code>library(tidyverse)</code></pre>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<p>The (simple) linear regression is a standard tool in data analysis and statistics. Its properties are well-known but sometimes not known in details to the applied analyst; which is ok. However, if one wishes to understand deeper the internals of the system, the question may arise how to derive the coefficients of the linear regression. Here’s one way.</p>
<p>This approach focues on simple calculus and derivatives; no matrix algebra, and only the simple case for one predictor.</p>
<p>There are many sources and tutorials similar to this around for example <a href="https://towardsdatascience.com/linear-regression-derivation-d362ea3884c2">this</a> or <a href="https://math.stackexchange.com/questions/131590/derivation-of-the-formula-for-ordinary-least-squares-linear-regression">here</a>.</p>
</div>
<div id="heres-the-workhorse" class="section level1">
<h1>Here’s the workhorse</h1>
<p>Simple linear regression is defines such that</p>
<p><span class="math display">\[\hat{y}_i = b_0 + b_1 x_i\]</span></p>
<p>That’s the regression line.</p>
<p>The regression line is optimal in the sense that it minimizes the squared distances from the line; for example from this <a href="https://www.r-bloggers.com/2016/08/visualising-residuals/">source</a>:</p>
<pre class="r"><code>library(ggplot2)

d &lt;- mtcars
fit &lt;- lm(mpg ~ hp, data = d)

d$predicted &lt;- predict(fit)   # Save the predicted values
d$residuals &lt;- residuals(fit) # Save the residual values

ggplot(d, aes(x = hp, y = mpg)) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) +  # Plot regression slope
  geom_segment(aes(xend = hp, yend = predicted), alpha = .2) +  # alpha to fade lines
  geom_point() +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()  # Add theme for cleaner look</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In other words, the residuals minimize this cost function, <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[S = \sum(y_i - \hat{y})^2\]</span></p>
<p>Substituting the <span class="math inline">\(\hat{y}\)</span> values:</p>
<p><span class="math display">\[S = \sum(y_i  - b_0 - b_1 x_i)^2\]</span></p>
<p>To minimize a function, we can take the first derivative. For functions with more than one variable, we take the partial derivative of the respective parameter. Let’s begin with <span class="math inline">\(b_0\)</span>:</p>
<p><span class="math display">\[\frac{\partial S}{\partial b_0} \left[ \sum (y_i - b_0 - b_1 x_i)^2 \right]\]</span></p>
<p>Arg, now what? <a href="https://en.wikipedia.org/wiki/Chain_rule">Chain rule</a> to the rescue!</p>
<ul>
<li>Differentiate outer function first.</li>
<li>Multiply by inner function</li>
<li>Differentiate inner function and multiply with what we already have.</li>
</ul>
<p><span class="math display">\[\frac{\partial S}{\partial b_0} \sum -2(y_i -b_0 - b_1 x_i)\]</span></p>
<p>Note that the inner function here simplifies to <span class="math inline">\(-1\)</span>.</p>
<p>Set to zero to get the minimum.</p>
<p><span class="math display">\[\sum -2(y_i -b_0 - b_1 x_i) = 0\]</span></p>
<p>Now pull out the -2 from the summation and divide both equations by -2:</p>
<p><span class="math display">\[\sum (y_i -b_0 - b_1 x_i) = 0\]</span></p>
<p>Multiply out the summation:</p>
<p><span class="math display">\[\sum y_i - \sum b_0 - b_1\sum x_i=0\]</span></p>
<p>Note that the sum of a constant is <span class="math inline">\(n\)</span> times the constant: <span class="math inline">\(\sum k = nk\)</span>.</p>
<p>Using the above term we get:</p>
<p><span class="math display">\[\sum y_i - nb_0 - b_1\sum x_i=0\]</span></p>
<p>Now solve for <span class="math inline">\(b_0\)</span>:</p>
<p><span class="math display">\[ b_0 = \frac{ \sum y_i - b_1 \sum x_i}{n}\]</span></p>
<p>And that’s simply the mean of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>!</p>
<p><span class="math display">\[b_0 = \bar{y} - b_1 \bar{x}\]</span></p>
</div>
<div id="finding-b_1" class="section level1">
<h1>Finding <span class="math inline">\(b_1\)</span></h1>
<p>Get the partial derivative w.r.t. <span class="math inline">\(b_1\)</span>:</p>
<p><span class="math display">\[\frac{\partial S}{\partial b_1} \sum -2(y_i -b_0 - b_1 x_i)\]</span></p>
<p>That gives us, set to zero:</p>
<p><span class="math display">\[ \sum -2x_i (y_i - b_0 - b_1x_i) = 0\]</span></p>
<p>Divide by -2:</p>
<p><span class="math display">\[\sum x_i (y_i - b_0 - b_1 x_i)=0\]</span></p>
<p>Redistribute <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[\sum (x_i y_i - b_0 x_i - b_1 x_i^2) = 0\]</span></p>
<p>As we already now <span class="math inline">\(b_0\)</span>, let’s substitute that back in:</p>
<p><span class="math display">\[\sum(x_i y_i - (\bar{y} - b_1\bar{x})x_i - b_1x^2_i) = 0\]</span></p>
<p>Split in two sums:</p>
<p><span class="math display">\[\sum(x_iy_i - \bar{y}x_i) + \sum(b_1\bar{x}x_i - b_1x_i^2)=0\]</span></p>
<p>We would like to isolate <span class="math inline">\(b_1\)</span>, so let’s get this guy more isolated:</p>
<p><span class="math display">\[\sum(x_iy_i - \bar{y}x_i) + b_1 \sum(\bar{x}x_i - x_i^2)=0\]</span></p>
<p>To finally isolate, substract the first sum and divide by the second:</p>
<p><span class="math display">\[b_1 = \frac{ \sum (x_i y_i - \bar{y}x_i)} { \sum (x^2_i - \bar{x}x_i)}\]</span></p>
<p>Yeah! Here we go, there we have <span class="math inline">\(b_1\)</span>.</p>
</div>
